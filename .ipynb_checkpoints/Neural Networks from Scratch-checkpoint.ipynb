{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook imports, do not care about them!\n",
    "import jdc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "\n",
    "Redes Neurais de Múltiplas Camadas (RNM), ou Multilayer Perceptron (MLP), são modelos computacionais inspirados no sistema nervoso humano, compostas por estruturas matemáticas que simulam os neurônios e suas conexões. Os usos mais comuns desses modelos consistem em tarefas de classificação e regressão, \n",
    "o que favorece a aplicação em diversas áreas de pesquisa, sendo uma delas\n",
    "a de Visão Computacional.\n",
    "\n",
    "Para compreender as RNM, é importante primeiramente conhecer dois de seus aspectos fundamentais: os **neurônios** e a **arquitetura em camadas**.\n",
    "\n",
    "## Neurônios\n",
    "Um neurônio é uma estrutura que aceita como argumento um vetor $\\mathbf{x} = \\langle x_1, x_2, \\ldots, x_n \\rangle \\in \\mathbb{R}^n$\n",
    "de entrada e **responde com um valor real** de saída. Cada neurônio está relacionado a um vetor de pesos $\\mathbf{w} = \\langle w_1, w_2, \\ldots, w_n \\rangle \\in \\mathbb{R}^n$, de forma que cada componente $x_i$ da entrada é associada ao peso $w_i$. Ao receber a entrada,\n",
    "o neurônio responde com o valor $f(\\mathbf{x}\\cdot\\mathbf{w} + b)$, onde $f$ é chamada de \n",
    "**função de ativação**, $\\mathbf{x}\\cdot\\mathbf{w} = \\sum_i x_i \\cdot w_i$, ou seja, \n",
    "o produto interno entre $\\mathbf{x}$ e $\\mathbf{w}$, também chamado de $net$ do neurônio, e $b$ é \n",
    "uma constante real chamada *bias*, cuja principal função é permitir um ajuste fino \n",
    "do valor de saída por meio de um deslocamento horizontal da função de ativação.\n",
    "\n",
    "Uma função comum de ativação é a **sigmoide**, expressa pela equação:\n",
    "$$f(x) = \\frac{1}{1 + e^{-x}}.$$\n",
    "\n",
    "Ela possui a importante propriedade de ser contínua e derivável em $(-\\infty, +\\infty)$,\n",
    "o que favorece o algoritmo de treinamento a ser em breve explicado, apesar de existirem algumas desvantagens em sua utilização.\n",
    "\n",
    "### O truque do *bias*\n",
    "\n",
    "\n",
    "\n",
    "## Arquitetura em camadas\n",
    "\n",
    "Uma RNM é organizada em uma sequência de $k > 2$ camadas de neurônios, denotadas aqui por $L_0, L_1, \\ldots, L_{k-1}$. O tamanho de uma camada $L_i$, denotado aqui por $tam\\;L_i$, é a quantidade de neurônios que ela possui. A camada $L_0$ é dita de entrada, na qual existe um **neurônio de entrada** para cada componente do vetor de entrada da rede. Esses neurônios se diferenciam dos demais porque,\n",
    "ao invés de aceitarem todo o vetor, recebem apenas uma componente e simplesmente a **transmitem** para o interior da rede, sem \n",
    "a aplicação da função de ativação. Já a camada $L_{k-1}$ é dita de saída,\n",
    "e a sua resposta é a resposta da rede.\n",
    "\n",
    "O que viabiliza o funcionamento do modelo são as conexões entre essas camadas, e elas podem ocorrer de diversas maneiras na rede. Aqui,  \n",
    "o objeto de estudo,\n",
    "aqui, são as **redes densas *feedforward***. Nelas, cada neurônio da camada $L_i$ se conecta com\n",
    "todos os neurônios da camada $L_{i+1}$, $i = 0,\\ldots, k-2$, de maneira que o vetor de entrada dos neurônios da camada $L_{i+1}$ é o vetor composto pelas\n",
    "respostas dos neurônios da camada $L_{i}$. Assim, um neurônio na camada $L_{i+1}$ recebe, como entrada, um vetor\n",
    "de dimensão $tam\\;L_i$, $i=0,1, \\ldots, k-2$.\n",
    "\n",
    "Cada conexão entre neurônios pode ser vista como associada ao peso que o neurônio\n",
    "atribui ao valor por ela provido. Essa visão possibilita a utilização de uma\n",
    "notação muito útil para se trabalhar com pesos: denota-se por $w_{ij}^l$ o peso \n",
    "da conexão entre o neurônio $j$ da camada $l-1$ com o neurônio $i$ da camada $l$,\n",
    "$l = 1, \\ldots, k-1$. Mais ainda, é possível representar todas as conexões entre\n",
    "duas camadas por uma matriz de pesos $\\mathbf{W}^l = (w_{ij}^l)$ de dimensão $tam\\;L_{l} \\times tam\\;L_{l-1}$.\n",
    "\n",
    "Sabendo disso, já é possível iniciar a implementação em Python da RNM. Como o objetivo é\n",
    "produzir um módulo reutilizável, os recursos de orientação a objetos da linguagem\n",
    "serão aplicados. Além disso, o pacote `numpy` será utilizado para a manipulação de matrizes. \n",
    "\n",
    "A programação tem início com a importação do `numpy` e a declaração da classe `MultilayerNeuralNetwork`, a qual encapsulará todos os dados e métodos necessários ao\n",
    "funcionamento e à utilização da rede, como a matriz de pesos, a arquitetura e as rotinas de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultilayerNeuralNetwork:\n",
    "    '''A Multilayer Neural Network implementation.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa é apenas uma classe, e nada tem a ver com redes neurais além do nome simplesmente. O próximo passo é criar um **construtor**, método que sempre será chamado quando alguém\n",
    "criar (ou, mais tecnicamente, instanciar) a rede neural. Nele, é importante garantir\n",
    "a criação e inicialização da arquitetura, o que implica na configuração das camadas (quantas e quantos neurônios devem possuir)\n",
    "e nas matrizes de pesos.  Estas serão inicializadas com valores aleatórios, seguindo\n",
    "a distribuição normal, e, em seguida, normalizadas pela raiz quadrada da quantidade de neurônios da camada correspondente. A inicialização dos pesos é um tópico importante, para o qual existem diversas propostas, mas que será abstraído neste momento por economia de tempo.\n",
    "Além disso, um parâmetro `alpha` será acrescentado, \n",
    "cuja existência será justificada mais adiante. Portanto, o construtor receberá do\n",
    "usuário a **arquitetura da rede**, no formato $[tam\\;L_0, tam\\;L_1, \\ldots, tam\\;L_{k-1}]$, e o tal `alpha`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def __init__(self, arch = [1,2,1], alpha = 0.1):\n",
    "        '''Initialize the network.'''\n",
    "        \n",
    "        self.W = []\n",
    "        self.alpha = alpha\n",
    "        self.arch = arch\n",
    "        \n",
    "        # Set the internal architecture, considering the bias trick\n",
    "        self._arch = [i + 1 for i in self.arch[:-1]] + [self.arch[-1]]\n",
    "        \n",
    "        # Initialize the weight matrix with normalized random values\n",
    "        for i in np.arange(0,len(self._arch)-1):\n",
    "            w = np.random.randn(self._arch[i], self._arch[i+1])\n",
    "            self.W.append(w/np.sqrt(self._arch[i]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como será utilizada a função sigmoide para a ativação dos neurônios, é importante\n",
    "que ela e sua derivada estejam implementadas na classe:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def sigmoid(self, x):\n",
    "        '''Sigmoid function.'''\n",
    "        return 1.0/(1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_deriv(self, x):\n",
    "        '''Derivative of the sigmoid function, considering that x is the result\n",
    "        of applying the sigmoid function to the net.\n",
    "        '''\n",
    "        return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso, é possível instanciar uma rede, mesmo que não se possa fazer nada com ela ainda, apenas para checar a estrutura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.34508513, -0.6310941 , -0.34818161,  0.03718818],\n",
      "       [-0.93810365, -0.24729796, -0.54971193, -0.57938982],\n",
      "       [ 0.35531797,  0.12465738, -0.48428091,  0.38676956]]), array([[0.534274  ],\n",
      "       [0.02001503],\n",
      "       [0.40653573],\n",
      "       [0.34895457]])]\n"
     ]
    }
   ],
   "source": [
    "neuralnet = MultilayerNeuralNetwork(arch = [2,3,1], alpha = 0.5)\n",
    "print(neuralnet.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respostas das camadas\n",
    "\n",
    "Note que cada camada produz um vetor de resposta composto pelas respostas individuais\n",
    "de cada um de seus neurônios. Denote por $\\mathbf{a^l} = \\langle a^l_1, a^l_2, \\ldots, a^l_{tam\\; L_l} \\rangle$ esse vetor para a camada $L_l$. Como visto, ele deverá ser a entrada para a camada seguinte, mas os seus valores serão combinados de forma particular\n",
    "pela matriz de pesos $W^{l+1}$. O produto matricial\n",
    "$$\\mathbf{z}^l = \\mathbf{W}^{l+1}\\cdot\\mathbf{a^l}$$\n",
    "produz um vetor cujos componentes são os valores $net$ de cada neurônio da camada\n",
    "$L_{l+1}$. Sabendo disso, qual seria a expressão para o vetor de saída da camada $L_{l+1}$, ou seja, para $\\mathbf{a}^{l+1}$?\n",
    "Basta aplicar a função de ativação a cada componente do vetor obtido! Considerando-se a aplicação da função ponto a ponto, ou seja, $f([x_1, x_2, \\ldots, x_n]) = [f(x_1), f(x_2), \\ldots, f(x_n)]$, tem-se que:\n",
    "\n",
    "$$\\mathbf{a}^{l+1} = f(\\mathbf{W}^{l+1}\\cdot\\mathbf{a^l}).$$\n",
    "\n",
    "O conhecimento adquirido até este ponto é suficiente para se entender a arquitetura de uma rede neural e como computar as saídas das suas camadas de neurônios a partir \n",
    "de um vetor de entrada. Ocorre que as matrizes de pesos aleatórios \n",
    "tornam o modelo inútil. Ele necessita se ajustar (lê-se \"aprender\")\n",
    "para solucionar os problemas com os quais é confrontado, e é disso que \n",
    "trataremos a partir de agora!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado em uma RNM\n",
    "\n",
    "O modelo de aprendizagem de redes neurais é geralmente o **supervisionado**, pois faz\n",
    "uso de exemplos representantes da verdade (*ground truth*) para fazer\n",
    "com que a rede gere as respostas desejadas após uma **etapa de treinamento**. É como se um professor\n",
    "apresentasse a entrada, a rede respondesse e ele informasse qual foi o \n",
    "erro cometido. A rede, com base nisso, modifica sua estrutura (suas matrizes\n",
    " de pesos) para que o erro, da próxima vez em que o professor mostre\n",
    " o exemplo, seja garantidamente menor. Esse processo termina quando algum critério de parada é atingido. Os mais comuns são o erro máximo e o número de ciclos ou *epochs*.\n",
    " \n",
    " O erro cometido pela rede é modelado matematicamente por uma **função de custo**\n",
    " $C:\\mathbb{R}^n \\to \\mathbb{R}$. As funções que podem desempenhar esse papel devem cumprir alguns requisitos\n",
    " básicos. Uma das mais comuns é a do **erro quadrático**, de equação:\n",
    " \n",
    " $$C_x = \\frac 1 2 ||\\mathbf{y}(\\mathbf{x})- a^n(\\mathbf{x})||^2 = \\frac 1 2 \\sum_i (y_i(\\mathbf{x}) - a^n_i(\\mathbf{x}))^2,$$\n",
    " onde $\\mathbf{y}(\\mathbf{x})$ é a saída esperada para o exemplo $\\mathbf{x}$ e $a^n(\\mathbf{x})$ é a saída da última camada da rede para o exemplo $\\mathbf{x}$. Note que\n",
    " quanto maior a diferença entre a verdade e a saída da rede, maior o valor dessa\n",
    " função. \n",
    " \n",
    " Assim, o objetivo do treinamento é fazer com que o valor de $C$, para cada exemplo,\n",
    " seja menor a cada ciclo. Como $C$ é função dos pesos, nada melhor que buscar ajustá-los\n",
    " a fim de alcançar esse objetivo.\n",
    " \n",
    " Esse ajuste, entretanto, demanda o conhecimento sobre como variações nas matrizes\n",
    " de pesos - de camadas diferentes da última, inclusive - afetam o resultado da função de custo. Em outras palavras, \n",
    " é interessante conhecer \n",
    " \n",
    " $$\\frac{\\partial C}{\\partial w_{ij}^l}.$$\n",
    " \n",
    " Para tanto, existe um caminho implícito. Imagine que o neurônio $j$ da camada $L_l$\n",
    " deve produzir o valor $net$ denotado por $z_j^l$. Porém, suponha que esse valor\n",
    " seja alterado por uma quantidade pequena $\\Delta z_j^l$, de tal forma que a resposta dele\n",
    " seja $f(z_j^l + \\Delta z_j^l)$. Como a função de custo é afetada por essa mudança?\n",
    " Do polinômio de Taylor para o Cálculo Multivariável, chega-se que \n",
    " \n",
    " $$\\Delta C = \\frac{\\partial C}{\\partial z_{j}^l} \\Delta z_{j}^l.$$\n",
    " \n",
    " Note que, se $\\frac{\\partial C}{\\partial z_{j}^l}$ \n",
    " possuir um valor alto, é necessário um $\\Delta z_{j}^l$ de sinal oposto\n",
    " para reduzir o custo. Se possuir um valor próximo de zero, \n",
    " como $\\Delta z_j^l$ é pequeno, não se pode fazer muito para reduzir o custo, e o neurônio é dito estar\n",
    " em estado próximo do ótimo. Assim, faz sentido definir o erro nesse neurônio, \n",
    " denotado por $\\delta_j^l$, como\n",
    " \n",
    " \\begin{equation}\n",
    " \\delta_j^l = \\frac{\\partial C}{\\partial z_{j}^l}.\n",
    " \\end{equation}\n",
    " \n",
    " Além disso, define-se o vetor de erros dos neurônios da camada $L_l$ por\n",
    " $\\delta^l = \\langle \\delta^l_1, \\delta^l_2, \\ldots, \\delta^l_{tam\\;L_l}  \\rangle$.\n",
    " O algoritmo explicado na seção seguinte, denominado *Backpropagation*, provê meios de calcular os erros como definidos para se chegar às derivadas parciais $\\frac{\\partial C}{\\partial w^l_j}$ em todas as camadas. Depois, uma regra de otimização (neste estudo, a regra do **gradiente descendente**) as utilizará para atualizar os pesos de acordo, permitindo o aprendizado da rede."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O algoritmo Backpropagation\n",
    "\n",
    "Pode-se dizer que este é o grande viabilizador das redes neurais modernas, incluindo\n",
    "as utilizadas para *Deep Learning*. Este algoritmo utiliza as definições anteriores e está fundamentado em três equações, explicadas a seguir. Para simplificar a notação, considere a camada $L_{k-1}$ (de saída) denotada pelo índice $L$.\n",
    "\n",
    "### Uma equação para o $\\delta^L$\n",
    "\n",
    "Esta equação fornece meios para se calcular o erro na camada de saída. Sabemos que a função de custo tem \n",
    "a forma $C=C(X_1, X_2, \\ldots, X_{tam\\,L})$, tal que $X_1 = a_1^L(z^L_1, z^L_2 \\ldots, z^L_{tam\\,L}), X_2 = a_2^L(z^L_1, z^L_2 \\ldots, z^L_{tam\\,L}), \\ldots, X_{tam\\,L} = a_{tam\\, L}^L(z^L_1, z^L_2 \\ldots, z^L_{tam\\,L})$. Pela definição anterior, sabemos que:\n",
    "\n",
    "$$\\delta_j^L = \\frac{\\partial C}{\\partial z_{j}^L}.$$\n",
    "\n",
    "Utilizando a **regra da cadeia** do Cálculo Multivariável, tem-se que\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z_{j}^L} = \\sum_k \\frac{\\partial C}{\\partial a^L_j}\\frac{\\partial a^L_k}{\\partial z^L_j}.$$\n",
    "\n",
    "Como apenas $a^L_j$ depende de $z^L_j$, todas as $\\frac{\\partial a^L_k}{\\partial z^L_j}$, com $k \\neq j$, serão anuladas, restando que:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z_{j}^L} =\\frac{\\partial C}{\\partial a^L_j}\\frac{\\partial a^L_j}{\\partial z^L_j}.$$\n",
    "\n",
    "Como $a^L_j = \\sigma(z^L_j)$, \n",
    "\n",
    "$$\\frac{\\partial a^L_j}{\\partial z^L_j} = \\sigma'(z^L_j).$$\n",
    "\n",
    "Finalmente, chega-se à equação\n",
    "\n",
    "$$\\delta^L_j = \\frac{\\partial C}{\\partial a^L_j}\\sigma'(z_j^L).$$\n",
    "\n",
    "Na forma vetorial, temos:\n",
    "\n",
    "$$\\delta^L = \\nabla C_a \\odot \\sigma'(\\mathbf{z}^L).$$\n",
    "\n",
    "### Uma equação para o $\\delta^l$ em função de $\\delta^{l+1}$\n",
    "\n",
    "Na equação passada, conseguimos uma forma de calcular os erros da última camada, entretanto, queremos também um modo\n",
    "de se calcularem os erros nas demais camadas. A estratégia é tentar escrever $\\delta^{l}_j = \\frac{\\partial C}{\\partial z^l_j}$ em termos de $\\delta^{l+1}_{j} = \\frac{\\partial C}{\\partial z^{l+1}_j}$. Como? Primeiro, é válido notar que $z_j^{l+1}$, por definição,\n",
    "depende $z_j^l$:\n",
    "\n",
    "$$z_j^{l+1} = \\mathbf{W}^{l+1}_ja^l = \\mathbf{W}^{l+1}_jf(z^l) = \\sum_i w^{l+1}_{ij}f(z^l_i).$$\n",
    "\n",
    "Além disso, $C$ depende de $z_j^{l+1}$, devido à definição recursiva de $a^n$. Com isso, basta aplicar a regra da cadeia novamente, e temos que:\n",
    "\n",
    "$$\\delta_j^l = \\sum_k \\frac{\\partial C}{\\partial z^{l+1}_k}\\frac{\\partial z^{l+1}_k}{\\partial z^l_j} = \\sum_k \\frac{\\partial z^{l+1}_k}{\\partial z^l_k}\\delta^{l+1}_k.$$\n",
    "\n",
    "Diferenciando a expressão para $z_j^{l+1}$, temos que:\n",
    "$$\\frac{\\partial z_k^{l+1}}{\\partial z_j^l} = w^{l+1}_{jk}f'(z^l_j).$$\n",
    "\n",
    "Assim, substituindo, chegamos a:\n",
    "$$\\delta_j^l = \\sum_k w^{l+1}_{jk}\\delta^{l+1}_kf'(z^l_j).$$\n",
    "\n",
    "### Uma equação para $\\frac{\\partial C}{\\partial w^l_{jk}}$\n",
    "\n",
    "Sabemos que $C$ está em função de $z^l$ e que $z^l$ está em função dos pesos,\n",
    "o que inclui $w_{jk}^l$. Por essa razão, podemos novamente aplicar a regra\n",
    "da cadeia, da seguinte forma:\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}}=\\sum_m \\frac{\\partial C}{\\partial z_m^l} \\frac{\\partial z_m^l}{\\partial w_{jk}^l}$$\n",
    "\n",
    "Note que $w^l_{jk}$ apenas influencia no cálculo de $z^l_j$, logo\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}}=\\frac{\\partial C}{\\partial z_j^l} \\frac{\\partial z_j^l}{\\partial w_{jk}^l}=\\delta^l_j\\frac{\\partial z_j^l}{\\partial w_{jk}^l}.$$\n",
    "\n",
    "Lembremo-nos de que:\n",
    "$$z_j^{l} = W^{l}_ja^{l-1} = \\sum_i w^{l}_{ij}a^{l-1}_i.$$\n",
    "\n",
    "Com isso, diferenciando, temos:\n",
    "$$\\frac{\\partial z_j^{l}}{\\partial w_{jk}^l} =  a^{l-1}_j.$$\n",
    "\n",
    "Finalmente, substituindo:\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}}=\\delta^l_j a^{l-1}_j.$$\n",
    "\n",
    "\n",
    "Temos, assim, todas as equações fundamentais que constituem o algoritmo\n",
    "Backpropagation. Agora, resta implementarmos. Primeiramente, \n",
    "precisamos de um método que represente todo o processo de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def fit(self, X, y, epochs = 1000, displayUpdate = 100):\n",
    "        '''Fit the model to the training data.'''\n",
    "        # First, insert the columns of 1's, to perform the bias trick\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "        \n",
    "        # Loop through the number of epochs\n",
    "        for ep in np.arange(0, epochs):\n",
    "            # Fit each training point\n",
    "            for (x, target) in zip(X, y):\n",
    "                self.fit_partial(x, target)\n",
    "            # Print an update\n",
    "            if ep == 0 or (ep+1) % displayUpdate == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print(\"[INFO] Epoch: {}, Loss: {:.7f}\".format(ep + 1, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que devemos definir duas funções: `fit_partial` e `calculate_loss`. A primeira\n",
    "é o coração do algoritmo Backpropagation e segue abaixo implementada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def fit_partial(self, x, y):\n",
    "        # We start creating the matrix which will store the activation values, a^l.\n",
    "        # For the first layer, it just takes the input:\n",
    "        A = [np.atleast_2d(x)]\n",
    "        # FEEDFORWARD: compute all the outputs from all the neurons, storing in A\n",
    "        for l in np.arange(0,len(self.W)):\n",
    "            net = A[l].dot(self.W[l])\n",
    "            out = self.sigmoid(net)\n",
    "            A.append(out)\n",
    "        # BACKPROPAGATION\n",
    "        # Using Equation 1 to compute delta^L:\n",
    "        error = (A[-1] - y)\n",
    "        D = [error * self.sigmoid_deriv(A[-1])]\n",
    "        \n",
    "        for layer in np.arange(len(A)-2, 0, -1):\n",
    "            # Using Equation 2 to compute all deltas\n",
    "            delta = D[-1].dot(self.W[layer].T) \n",
    "            delta = delta * self.sigmoid_deriv(A[layer])\n",
    "            D.append(delta)\n",
    "        # Reverse the deltas\n",
    "        D = D[::-1]\n",
    "                \n",
    "        # WEIGHT UPDATE: using Equation 3\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            self.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A segunda função é responsável por computar a função de custo. Nesta implementação, a `calculate_loss` tomará um conjunto de pontos e computará o custo para ele. Vale lembrar, porém, que isso envolve computar a saída da rede para uma dada entrada. Essa tarefa será executada pelo método `predict`, que recebe esse nome por ser utilizado nas predições realizadas no uso efetivo da rede, após o treinamento.\n",
    "\n",
    "Seguem as implementações dos métodos `predict` e `calculate_loss`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def predict(self, X, addBias = True):\n",
    "        p = np.atleast_2d(X)\n",
    "        if addBias:\n",
    "            p = np.c_[p, np.ones((p.shape[0]))]\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "        return p\n",
    "    \n",
    "    def calculate_loss(self, X, targets):\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, addBias = False)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando a rede neural\n",
    "\n",
    "Agora que temos nossa rede neural implementada, podemos resolver alguns problemas bastante interessantes. Aqui serão mostrados dois exemplos: o XOR e o reconhecimento de caracteres utilizando o *dataset* MNIST.\n",
    "\n",
    "## XOR\n",
    "\n",
    "XOR é uma operação bem conhecida da lógica proposicional que possui a seguinte tabela verdade:\n",
    "\n",
    "|$x_1$|$x_2$|$y$\n",
    "|---|---|---|\n",
    "|0|0|0|\n",
    "|1|0|1|\n",
    "|0|1|1|\n",
    "|1|1|0|\n",
    "\n",
    "A partir disso, tem-se o *dataset* XOR, composto pelos vetores:\n",
    "$\\langle 0,0 \\rangle, \\langle 1,0 \\rangle, \\langle 0,1 \\rangle, \\langle 1,1 \\rangle$,\n",
    "com os respectivos *labels* $0, 1 , 1, 0$, correspondendo a cada linha da tabela acima. O que acontece se plotarmos \n",
    "$x_1$ no eixo $x$, $x_2$ no eixo $y$ e atribuirmos uma cor para cada um dos dois valores possíveis para $y$? Note:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6f9c47b3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x1 = np.array([[0],[1],[0],[1]])\n",
    "x2 = np.array([[0],[0],[1],[1]])\n",
    "\n",
    "plt.scatter(x1,x2,c=['green','red','red','green'])\n",
    "plt.title('The XOR dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que há de importante nesse *dataset*? **Não é possível separá-lo utilizando uma reta**! Ou seja, não corresponde a um problema de classificação com classes linearmente separáveis. Um neurônio somente não conseguiria resolvê-lo, mas nossa rede pode! Vamos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 1, Loss: 0.5314312\n",
      "[INFO] Epoch: 100, Loss: 0.4999804\n",
      "[INFO] Epoch: 200, Loss: 0.4994919\n",
      "[INFO] Epoch: 300, Loss: 0.4990977\n",
      "[INFO] Epoch: 400, Loss: 0.4987964\n",
      "[INFO] Epoch: 500, Loss: 0.4982415\n",
      "[INFO] Epoch: 600, Loss: 0.4971462\n",
      "[INFO] Epoch: 700, Loss: 0.4949796\n",
      "[INFO] Epoch: 800, Loss: 0.4899777\n",
      "[INFO] Epoch: 900, Loss: 0.4751478\n",
      "[INFO] Epoch: 1000, Loss: 0.4208956\n",
      "[INFO] Epoch: 1100, Loss: 0.2683639\n",
      "[INFO] Epoch: 1200, Loss: 0.1245102\n",
      "[INFO] Epoch: 1300, Loss: 0.0636411\n",
      "[INFO] Epoch: 1400, Loss: 0.0387600\n",
      "[INFO] Epoch: 1500, Loss: 0.0266780\n",
      "[INFO] Epoch: 1600, Loss: 0.0198856\n",
      "[INFO] Epoch: 1700, Loss: 0.0156439\n",
      "[INFO] Epoch: 1800, Loss: 0.0127871\n",
      "[INFO] Epoch: 1900, Loss: 0.0107520\n",
      "[INFO] Epoch: 2000, Loss: 0.0092388\n",
      "[INFO] Epoch: 2100, Loss: 0.0080752\n",
      "[INFO] Epoch: 2200, Loss: 0.0071558\n",
      "[INFO] Epoch: 2300, Loss: 0.0064132\n",
      "[INFO] Epoch: 2400, Loss: 0.0058020\n",
      "[INFO] Epoch: 2500, Loss: 0.0052912\n",
      "[INFO] Epoch: 2600, Loss: 0.0048584\n",
      "[INFO] Epoch: 2700, Loss: 0.0044876\n",
      "[INFO] Epoch: 2800, Loss: 0.0041666\n",
      "[INFO] Epoch: 2900, Loss: 0.0038863\n",
      "[INFO] Epoch: 3000, Loss: 0.0036395\n",
      "[INFO] Epoch: 3100, Loss: 0.0034207\n",
      "[INFO] Epoch: 3200, Loss: 0.0032255\n",
      "[INFO] Epoch: 3300, Loss: 0.0030504\n",
      "[INFO] Epoch: 3400, Loss: 0.0028925\n",
      "[INFO] Epoch: 3500, Loss: 0.0027494\n",
      "[INFO] Epoch: 3600, Loss: 0.0026192\n",
      "[INFO] Epoch: 3700, Loss: 0.0025002\n",
      "[INFO] Epoch: 3800, Loss: 0.0023911\n",
      "[INFO] Epoch: 3900, Loss: 0.0022908\n",
      "[INFO] Epoch: 4000, Loss: 0.0021981\n",
      "[INFO] Epoch: 4100, Loss: 0.0021124\n",
      "[INFO] Epoch: 4200, Loss: 0.0020329\n",
      "[INFO] Epoch: 4300, Loss: 0.0019588\n",
      "[INFO] Epoch: 4400, Loss: 0.0018898\n",
      "[INFO] Epoch: 4500, Loss: 0.0018253\n",
      "[INFO] Epoch: 4600, Loss: 0.0017649\n",
      "[INFO] Epoch: 4700, Loss: 0.0017082\n",
      "[INFO] Epoch: 4800, Loss: 0.0016548\n",
      "[INFO] Epoch: 4900, Loss: 0.0016046\n",
      "[INFO] Epoch: 5000, Loss: 0.0015573\n",
      "[INFO] Epoch: 5100, Loss: 0.0015125\n",
      "[INFO] Epoch: 5200, Loss: 0.0014702\n",
      "[INFO] Epoch: 5300, Loss: 0.0014300\n",
      "[INFO] Epoch: 5400, Loss: 0.0013920\n",
      "[INFO] Epoch: 5500, Loss: 0.0013558\n",
      "[INFO] Epoch: 5600, Loss: 0.0013214\n",
      "[INFO] Epoch: 5700, Loss: 0.0012887\n",
      "[INFO] Epoch: 5800, Loss: 0.0012574\n",
      "[INFO] Epoch: 5900, Loss: 0.0012276\n",
      "[INFO] Epoch: 6000, Loss: 0.0011992\n",
      "[INFO] Epoch: 6100, Loss: 0.0011719\n",
      "[INFO] Epoch: 6200, Loss: 0.0011459\n",
      "[INFO] Epoch: 6300, Loss: 0.0011209\n",
      "[INFO] Epoch: 6400, Loss: 0.0010970\n",
      "[INFO] Epoch: 6500, Loss: 0.0010740\n",
      "[INFO] Epoch: 6600, Loss: 0.0010520\n",
      "[INFO] Epoch: 6700, Loss: 0.0010308\n",
      "[INFO] Epoch: 6800, Loss: 0.0010104\n",
      "[INFO] Epoch: 6900, Loss: 0.0009908\n",
      "[INFO] Epoch: 7000, Loss: 0.0009719\n",
      "[INFO] Epoch: 7100, Loss: 0.0009537\n",
      "[INFO] Epoch: 7200, Loss: 0.0009361\n",
      "[INFO] Epoch: 7300, Loss: 0.0009192\n",
      "[INFO] Epoch: 7400, Loss: 0.0009028\n",
      "[INFO] Epoch: 7500, Loss: 0.0008870\n",
      "[INFO] Epoch: 7600, Loss: 0.0008717\n",
      "[INFO] Epoch: 7700, Loss: 0.0008569\n",
      "[INFO] Epoch: 7800, Loss: 0.0008426\n",
      "[INFO] Epoch: 7900, Loss: 0.0008288\n",
      "[INFO] Epoch: 8000, Loss: 0.0008154\n",
      "[INFO] Epoch: 8100, Loss: 0.0008024\n",
      "[INFO] Epoch: 8200, Loss: 0.0007898\n",
      "[INFO] Epoch: 8300, Loss: 0.0007775\n",
      "[INFO] Epoch: 8400, Loss: 0.0007657\n",
      "[INFO] Epoch: 8500, Loss: 0.0007542\n",
      "[INFO] Epoch: 8600, Loss: 0.0007430\n",
      "[INFO] Epoch: 8700, Loss: 0.0007321\n",
      "[INFO] Epoch: 8800, Loss: 0.0007216\n",
      "[INFO] Epoch: 8900, Loss: 0.0007113\n",
      "[INFO] Epoch: 9000, Loss: 0.0007013\n",
      "[INFO] Epoch: 9100, Loss: 0.0006916\n",
      "[INFO] Epoch: 9200, Loss: 0.0006821\n",
      "[INFO] Epoch: 9300, Loss: 0.0006729\n",
      "[INFO] Epoch: 9400, Loss: 0.0006639\n",
      "[INFO] Epoch: 9500, Loss: 0.0006552\n",
      "[INFO] Epoch: 9600, Loss: 0.0006466\n",
      "[INFO] Epoch: 9700, Loss: 0.0006383\n",
      "[INFO] Epoch: 9800, Loss: 0.0006302\n",
      "[INFO] Epoch: 9900, Loss: 0.0006223\n",
      "[INFO] Epoch: 10000, Loss: 0.0006146\n",
      "[INFO] Epoch: 10100, Loss: 0.0006071\n",
      "[INFO] Epoch: 10200, Loss: 0.0005997\n",
      "[INFO] Epoch: 10300, Loss: 0.0005925\n",
      "[INFO] Epoch: 10400, Loss: 0.0005855\n",
      "[INFO] Epoch: 10500, Loss: 0.0005787\n",
      "[INFO] Epoch: 10600, Loss: 0.0005719\n",
      "[INFO] Epoch: 10700, Loss: 0.0005654\n",
      "[INFO] Epoch: 10800, Loss: 0.0005590\n",
      "[INFO] Epoch: 10900, Loss: 0.0005527\n",
      "[INFO] Epoch: 11000, Loss: 0.0005466\n",
      "[INFO] Epoch: 11100, Loss: 0.0005406\n",
      "[INFO] Epoch: 11200, Loss: 0.0005347\n",
      "[INFO] Epoch: 11300, Loss: 0.0005289\n",
      "[INFO] Epoch: 11400, Loss: 0.0005233\n",
      "[INFO] Epoch: 11500, Loss: 0.0005178\n",
      "[INFO] Epoch: 11600, Loss: 0.0005124\n",
      "[INFO] Epoch: 11700, Loss: 0.0005071\n",
      "[INFO] Epoch: 11800, Loss: 0.0005019\n",
      "[INFO] Epoch: 11900, Loss: 0.0004968\n",
      "[INFO] Epoch: 12000, Loss: 0.0004918\n",
      "[INFO] Epoch: 12100, Loss: 0.0004869\n",
      "[INFO] Epoch: 12200, Loss: 0.0004821\n",
      "[INFO] Epoch: 12300, Loss: 0.0004774\n",
      "[INFO] Epoch: 12400, Loss: 0.0004728\n",
      "[INFO] Epoch: 12500, Loss: 0.0004683\n",
      "[INFO] Epoch: 12600, Loss: 0.0004638\n",
      "[INFO] Epoch: 12700, Loss: 0.0004595\n",
      "[INFO] Epoch: 12800, Loss: 0.0004552\n",
      "[INFO] Epoch: 12900, Loss: 0.0004510\n",
      "[INFO] Epoch: 13000, Loss: 0.0004468\n",
      "[INFO] Epoch: 13100, Loss: 0.0004428\n",
      "[INFO] Epoch: 13200, Loss: 0.0004388\n",
      "[INFO] Epoch: 13300, Loss: 0.0004349\n",
      "[INFO] Epoch: 13400, Loss: 0.0004310\n",
      "[INFO] Epoch: 13500, Loss: 0.0004272\n",
      "[INFO] Epoch: 13600, Loss: 0.0004235\n",
      "[INFO] Epoch: 13700, Loss: 0.0004199\n",
      "[INFO] Epoch: 13800, Loss: 0.0004163\n",
      "[INFO] Epoch: 13900, Loss: 0.0004127\n",
      "[INFO] Epoch: 14000, Loss: 0.0004092\n",
      "[INFO] Epoch: 14100, Loss: 0.0004058\n",
      "[INFO] Epoch: 14200, Loss: 0.0004025\n",
      "[INFO] Epoch: 14300, Loss: 0.0003991\n",
      "[INFO] Epoch: 14400, Loss: 0.0003959\n",
      "[INFO] Epoch: 14500, Loss: 0.0003927\n",
      "[INFO] Epoch: 14600, Loss: 0.0003895\n",
      "[INFO] Epoch: 14700, Loss: 0.0003864\n",
      "[INFO] Epoch: 14800, Loss: 0.0003833\n",
      "[INFO] Epoch: 14900, Loss: 0.0003803\n",
      "[INFO] Epoch: 15000, Loss: 0.0003774\n",
      "[INFO] Epoch: 15100, Loss: 0.0003744\n",
      "[INFO] Epoch: 15200, Loss: 0.0003716\n",
      "[INFO] Epoch: 15300, Loss: 0.0003687\n",
      "[INFO] Epoch: 15400, Loss: 0.0003659\n",
      "[INFO] Epoch: 15500, Loss: 0.0003632\n",
      "[INFO] Epoch: 15600, Loss: 0.0003605\n",
      "[INFO] Epoch: 15700, Loss: 0.0003578\n",
      "[INFO] Epoch: 15800, Loss: 0.0003552\n",
      "[INFO] Epoch: 15900, Loss: 0.0003526\n",
      "[INFO] Epoch: 16000, Loss: 0.0003500\n",
      "[INFO] Epoch: 16100, Loss: 0.0003475\n",
      "[INFO] Epoch: 16200, Loss: 0.0003450\n",
      "[INFO] Epoch: 16300, Loss: 0.0003425\n",
      "[INFO] Epoch: 16400, Loss: 0.0003401\n",
      "[INFO] Epoch: 16500, Loss: 0.0003377\n",
      "[INFO] Epoch: 16600, Loss: 0.0003354\n",
      "[INFO] Epoch: 16700, Loss: 0.0003331\n",
      "[INFO] Epoch: 16800, Loss: 0.0003308\n",
      "[INFO] Epoch: 16900, Loss: 0.0003285\n",
      "[INFO] Epoch: 17000, Loss: 0.0003263\n",
      "[INFO] Epoch: 17100, Loss: 0.0003241\n",
      "[INFO] Epoch: 17200, Loss: 0.0003219\n",
      "[INFO] Epoch: 17300, Loss: 0.0003198\n",
      "[INFO] Epoch: 17400, Loss: 0.0003176\n",
      "[INFO] Epoch: 17500, Loss: 0.0003156\n",
      "[INFO] Epoch: 17600, Loss: 0.0003135\n",
      "[INFO] Epoch: 17700, Loss: 0.0003115\n",
      "[INFO] Epoch: 17800, Loss: 0.0003095\n",
      "[INFO] Epoch: 17900, Loss: 0.0003075\n",
      "[INFO] Epoch: 18000, Loss: 0.0003055\n",
      "[INFO] Epoch: 18100, Loss: 0.0003036\n",
      "[INFO] Epoch: 18200, Loss: 0.0003017\n",
      "[INFO] Epoch: 18300, Loss: 0.0002998\n",
      "[INFO] Epoch: 18400, Loss: 0.0002979\n",
      "[INFO] Epoch: 18500, Loss: 0.0002961\n",
      "[INFO] Epoch: 18600, Loss: 0.0002942\n",
      "[INFO] Epoch: 18700, Loss: 0.0002924\n",
      "[INFO] Epoch: 18800, Loss: 0.0002907\n",
      "[INFO] Epoch: 18900, Loss: 0.0002889\n",
      "[INFO] Epoch: 19000, Loss: 0.0002872\n",
      "[INFO] Epoch: 19100, Loss: 0.0002855\n",
      "[INFO] Epoch: 19200, Loss: 0.0002838\n",
      "[INFO] Epoch: 19300, Loss: 0.0002821\n",
      "[INFO] Epoch: 19400, Loss: 0.0002804\n",
      "[INFO] Epoch: 19500, Loss: 0.0002788\n",
      "[INFO] Epoch: 19600, Loss: 0.0002772\n",
      "[INFO] Epoch: 19700, Loss: 0.0002756\n",
      "[INFO] Epoch: 19800, Loss: 0.0002740\n",
      "[INFO] Epoch: 19900, Loss: 0.0002724\n",
      "[INFO] Epoch: 20000, Loss: 0.0002709\n"
     ]
    }
   ],
   "source": [
    "# Vamos compor nosso dataset X justapondo horizontalmente x1 e x2 declarados anteriormente\n",
    "X = np.concatenate((x1,x2), axis=1)\n",
    "# Temos também de declarar os targets\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Agora, instanciamos e treinamos nossa rede neural!\n",
    "xorNetwork = MultilayerNeuralNetwork(arch=[2,2,1], alpha=0.5)\n",
    "xorNetwork.fit(X, y, epochs = 20000, displayUpdate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "O MNIST é um *dataset* massivamente utilizado pelos estudiosos de Machine Learning composto\n",
    "por imagens de dígitos de 0 a 9. Ele possui 60000 exemplos de treino e 10000 de testes. \n",
    "Os vetores de características são 784-dimensionais ($28 \\times 28$ pixels por imagem),\n",
    "com componentes assumindo valores em $[0,255]$. O propósito é corretamente classificar\n",
    "os dígitos desse *dataset*!\n",
    "\n",
    "A biblioteca `sklearn` oferece o uma amostra do MNIST por meio de comandos simples. \n",
    "Vamos utilizá-los para obter esse *dataset* e, com ele,\n",
    "treinar uma rede neural especializada em classificar\n",
    "seus dígitos de 0 a 9!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST dataset\n",
      "[INFO] samples: 1797, dim: 64\n",
      "[INFO] training network...\n",
      "[INFO] Epoch: 1, Loss: 605.3871169\n",
      "[INFO] Epoch: 100, Loss: 7.6378433\n",
      "[INFO] Epoch: 200, Loss: 2.8820704\n",
      "[INFO] Epoch: 300, Loss: 2.1217411\n",
      "[INFO] Epoch: 400, Loss: 1.8994183\n",
      "[INFO] Epoch: 500, Loss: 1.7923004\n",
      "[INFO] Epoch: 600, Loss: 1.7289397\n",
      "[INFO] Epoch: 700, Loss: 1.6856620\n",
      "[INFO] Epoch: 800, Loss: 0.7474762\n",
      "[INFO] Epoch: 900, Loss: 0.6688806\n",
      "[INFO] Epoch: 1000, Loss: 0.6380501\n"
     ]
    }
   ],
   "source": [
    "# Import sklearn tools\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load a sample of MNIST through simple sklearn commands\n",
    "print(\"[INFO] loading MNIST dataset\")\n",
    "digits = datasets.load_digits()\n",
    "data = digits.data.astype(\"float\")\n",
    "data = (data - data.min()) / (data.max() - data.min())    # normalize data\n",
    "print(\"[INFO] samples: {}, dim: {}\".format(data.shape[0], data.shape[1]))\n",
    "\n",
    "# Split the dataset into train and test\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, digits.target, test_size=0.25)\n",
    "\n",
    "# Binarize labels\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)\n",
    "\n",
    "# Train the neural network\n",
    "print(\"[INFO] training network...\")\n",
    "mnistNN = MultilayerNeuralNetwork(arch=[trainX.shape[1], 32, 16, 10], alpha=0.1)\n",
    "mnistNN.fit(trainX, trainY, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez treinada a rede, podemos checar seu desempenho no conjunto de testes, por meio da função `classification_report`, que fornece índices de desempenho e uma **matriz de confusão**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99        42\n",
      "          1       0.98      1.00      0.99        61\n",
      "          2       1.00      0.98      0.99        47\n",
      "          3       0.98      1.00      0.99        41\n",
      "          4       0.93      1.00      0.96        50\n",
      "          5       0.98      1.00      0.99        48\n",
      "          6       1.00      0.94      0.97        35\n",
      "          7       1.00      0.97      0.99        38\n",
      "          8       0.96      0.94      0.95        49\n",
      "          9       1.00      0.97      0.99        39\n",
      "\n",
      "avg / total       0.98      0.98      0.98       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] evaluating network...\")\n",
    "# Test the model\n",
    "predictions = mnistNN.predict(testX)\n",
    "predictions = predictions.argmax(axis=1)\n",
    "print(classification_report(testY.argmax(axis=1), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais com Keras\n",
    "\n",
    "O que fizemos até então geralmente não é feito na prática. Existem bibliotecas mais robustas e otimizadas para se trabalhar com redes neurais e outros modelos. O Keras é uma biblioteca para Python muito utilizada para Machine Learning e permite o treino e avaliação de redes neurais com poucas linhas de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training...\n",
      "Train on 52500 samples, validate on 17500 samples\n",
      "Epoch 1/100\n",
      "52500/52500 [==============================] - 6s 120us/step - loss: 1.7916 - acc: 0.4994 - val_loss: 1.1007 - val_acc: 0.6950\n",
      "Epoch 2/100\n",
      "52500/52500 [==============================] - 6s 112us/step - loss: 0.7345 - acc: 0.8169 - val_loss: 0.5681 - val_acc: 0.8445\n",
      "Epoch 3/100\n",
      "52500/52500 [==============================] - 6s 114us/step - loss: 0.4832 - acc: 0.8697 - val_loss: 0.4416 - val_acc: 0.8835\n",
      "Epoch 4/100\n",
      "52500/52500 [==============================] - 6s 112us/step - loss: 0.4000 - acc: 0.8891 - val_loss: 0.3891 - val_acc: 0.8910\n",
      "Epoch 5/100\n",
      "52500/52500 [==============================] - 6s 113us/step - loss: 0.3591 - acc: 0.8990 - val_loss: 0.3547 - val_acc: 0.8967\n",
      "Epoch 6/100\n",
      "52500/52500 [==============================] - 6s 114us/step - loss: 0.3339 - acc: 0.9047 - val_loss: 0.3343 - val_acc: 0.9042\n",
      "Epoch 7/100\n",
      "52500/52500 [==============================] - 6s 115us/step - loss: 0.3157 - acc: 0.9091 - val_loss: 0.3208 - val_acc: 0.9076\n",
      "Epoch 8/100\n",
      "52500/52500 [==============================] - 6s 114us/step - loss: 0.3021 - acc: 0.9123 - val_loss: 0.3070 - val_acc: 0.9114\n",
      "Epoch 9/100\n",
      "52500/52500 [==============================] - 6s 116us/step - loss: 0.2902 - acc: 0.9155 - val_loss: 0.3044 - val_acc: 0.9141\n",
      "Epoch 10/100\n",
      "52500/52500 [==============================] - 6s 115us/step - loss: 0.2809 - acc: 0.9188 - val_loss: 0.3101 - val_acc: 0.9103\n",
      "Epoch 11/100\n",
      "52500/52500 [==============================] - 6s 115us/step - loss: 0.2722 - acc: 0.9206 - val_loss: 0.2880 - val_acc: 0.9171\n",
      "Epoch 12/100\n",
      "52500/52500 [==============================] - 6s 117us/step - loss: 0.2645 - acc: 0.9233 - val_loss: 0.2867 - val_acc: 0.9176\n",
      "Epoch 13/100\n",
      "52500/52500 [==============================] - 6s 114us/step - loss: 0.2575 - acc: 0.9247 - val_loss: 0.2737 - val_acc: 0.9219\n",
      "Epoch 14/100\n",
      "52500/52500 [==============================] - 6s 116us/step - loss: 0.2504 - acc: 0.9266 - val_loss: 0.2692 - val_acc: 0.9217\n",
      "Epoch 15/100\n",
      "52500/52500 [==============================] - 6s 117us/step - loss: 0.2441 - acc: 0.9284 - val_loss: 0.2709 - val_acc: 0.9203\n",
      "Epoch 16/100\n",
      "52500/52500 [==============================] - 6s 115us/step - loss: 0.2373 - acc: 0.9310 - val_loss: 0.2560 - val_acc: 0.9258\n",
      "Epoch 17/100\n",
      "52500/52500 [==============================] - 6s 117us/step - loss: 0.2310 - acc: 0.9324 - val_loss: 0.2514 - val_acc: 0.9255\n",
      "Epoch 18/100\n",
      "52500/52500 [==============================] - 6s 119us/step - loss: 0.2256 - acc: 0.9339 - val_loss: 0.2422 - val_acc: 0.9301\n",
      "Epoch 19/100\n",
      "52500/52500 [==============================] - 6s 121us/step - loss: 0.2196 - acc: 0.9362 - val_loss: 0.2402 - val_acc: 0.9298\n",
      "Epoch 20/100\n",
      "52500/52500 [==============================] - 6s 108us/step - loss: 0.2136 - acc: 0.9376 - val_loss: 0.2362 - val_acc: 0.9313\n",
      "Epoch 21/100\n",
      "52500/52500 [==============================] - 6s 109us/step - loss: 0.2083 - acc: 0.9391 - val_loss: 0.2364 - val_acc: 0.9301\n",
      "Epoch 22/100\n",
      "52500/52500 [==============================] - 6s 109us/step - loss: 0.2028 - acc: 0.9412 - val_loss: 0.2251 - val_acc: 0.9338\n",
      "Epoch 23/100\n",
      "52500/52500 [==============================] - 6s 109us/step - loss: 0.1977 - acc: 0.9427 - val_loss: 0.2162 - val_acc: 0.9374\n",
      "Epoch 24/100\n",
      "52500/52500 [==============================] - 6s 109us/step - loss: 0.1925 - acc: 0.9433 - val_loss: 0.2122 - val_acc: 0.9379\n",
      "Epoch 25/100\n",
      "52500/52500 [==============================] - 6s 114us/step - loss: 0.1878 - acc: 0.9456 - val_loss: 0.2157 - val_acc: 0.9357\n",
      "Epoch 26/100\n",
      "52500/52500 [==============================] - 6s 122us/step - loss: 0.1827 - acc: 0.9470 - val_loss: 0.2264 - val_acc: 0.9342\n",
      "Epoch 27/100\n",
      "52500/52500 [==============================] - 6s 119us/step - loss: 0.1782 - acc: 0.9486 - val_loss: 0.2005 - val_acc: 0.9413\n",
      "Epoch 28/100\n",
      "52500/52500 [==============================] - 6s 118us/step - loss: 0.1739 - acc: 0.9496 - val_loss: 0.1987 - val_acc: 0.9408\n",
      "Epoch 29/100\n",
      "52500/52500 [==============================] - 6s 119us/step - loss: 0.1696 - acc: 0.9504 - val_loss: 0.2003 - val_acc: 0.9410\n",
      "Epoch 30/100\n",
      "52500/52500 [==============================] - 6s 123us/step - loss: 0.1655 - acc: 0.9518 - val_loss: 0.1912 - val_acc: 0.9430\n",
      "Epoch 31/100\n",
      "52500/52500 [==============================] - 6s 119us/step - loss: 0.1615 - acc: 0.9531 - val_loss: 0.1827 - val_acc: 0.9467\n",
      "Epoch 32/100\n",
      "52500/52500 [==============================] - 6s 120us/step - loss: 0.1571 - acc: 0.9542 - val_loss: 0.1766 - val_acc: 0.9482\n",
      "Epoch 33/100\n",
      "52500/52500 [==============================] - 6s 124us/step - loss: 0.1535 - acc: 0.9554 - val_loss: 0.1739 - val_acc: 0.9489\n",
      "Epoch 34/100\n",
      "52500/52500 [==============================] - 6s 116us/step - loss: 0.1500 - acc: 0.9565 - val_loss: 0.1721 - val_acc: 0.9498\n",
      "Epoch 35/100\n",
      "52500/52500 [==============================] - 6s 113us/step - loss: 0.1465 - acc: 0.9573 - val_loss: 0.1676 - val_acc: 0.9515\n",
      "Epoch 36/100\n",
      "52500/52500 [==============================] - 6s 117us/step - loss: 0.1431 - acc: 0.9583 - val_loss: 0.1679 - val_acc: 0.9515\n",
      "Epoch 37/100\n",
      "52500/52500 [==============================] - 6s 120us/step - loss: 0.1402 - acc: 0.9593 - val_loss: 0.1627 - val_acc: 0.9532\n",
      "Epoch 38/100\n",
      "52500/52500 [==============================] - 6s 116us/step - loss: 0.1363 - acc: 0.9607 - val_loss: 0.1647 - val_acc: 0.9518\n",
      "Epoch 39/100\n",
      "52500/52500 [==============================] - 6s 120us/step - loss: 0.1337 - acc: 0.9611 - val_loss: 0.1584 - val_acc: 0.9531\n",
      "Epoch 40/100\n",
      "52500/52500 [==============================] - 6s 118us/step - loss: 0.1307 - acc: 0.9622 - val_loss: 0.1590 - val_acc: 0.9544\n",
      "Epoch 41/100\n",
      "52500/52500 [==============================] - 6s 120us/step - loss: 0.1277 - acc: 0.9632 - val_loss: 0.1510 - val_acc: 0.9561\n",
      "Epoch 42/100\n",
      "52500/52500 [==============================] - 6s 117us/step - loss: 0.1251 - acc: 0.9643 - val_loss: 0.1508 - val_acc: 0.9557\n",
      "Epoch 43/100\n",
      "52500/52500 [==============================] - 6s 118us/step - loss: 0.1224 - acc: 0.9651 - val_loss: 0.1486 - val_acc: 0.9570\n",
      "Epoch 44/100\n",
      "52500/52500 [==============================] - 6s 116us/step - loss: 0.1197 - acc: 0.9656 - val_loss: 0.1443 - val_acc: 0.9574\n",
      "Epoch 45/100\n",
      "52500/52500 [==============================] - 6s 115us/step - loss: 0.1170 - acc: 0.9663 - val_loss: 0.1571 - val_acc: 0.9528\n",
      "Epoch 46/100\n",
      "52500/52500 [==============================] - 6s 118us/step - loss: 0.1149 - acc: 0.9670 - val_loss: 0.1487 - val_acc: 0.9558\n",
      "Epoch 47/100\n",
      "52500/52500 [==============================] - 6s 119us/step - loss: 0.1123 - acc: 0.9674 - val_loss: 0.1402 - val_acc: 0.9586\n",
      "Epoch 48/100\n",
      "52500/52500 [==============================] - 6s 115us/step - loss: 0.1099 - acc: 0.9682 - val_loss: 0.1397 - val_acc: 0.9588\n",
      "Epoch 49/100\n",
      "52500/52500 [==============================] - 6s 119us/step - loss: 0.1077 - acc: 0.9690 - val_loss: 0.1456 - val_acc: 0.9571\n",
      "Epoch 50/100\n",
      "52500/52500 [==============================] - 6s 122us/step - loss: 0.1058 - acc: 0.9691 - val_loss: 0.1336 - val_acc: 0.9618\n",
      "Epoch 51/100\n",
      "52500/52500 [==============================] - 6s 118us/step - loss: 0.1035 - acc: 0.9705 - val_loss: 0.1308 - val_acc: 0.9625\n",
      "Epoch 52/100\n",
      "52500/52500 [==============================] - 6s 118us/step - loss: 0.1012 - acc: 0.9706 - val_loss: 0.1300 - val_acc: 0.9623\n",
      "Epoch 53/100\n",
      "52500/52500 [==============================] - 6s 117us/step - loss: 0.0995 - acc: 0.9714 - val_loss: 0.1270 - val_acc: 0.9629\n",
      "Epoch 54/100\n",
      "52500/52500 [==============================] - 6s 118us/step - loss: 0.0969 - acc: 0.9719 - val_loss: 0.1265 - val_acc: 0.9629\n",
      "Epoch 55/100\n",
      "52500/52500 [==============================] - 6s 120us/step - loss: 0.0951 - acc: 0.9728 - val_loss: 0.1333 - val_acc: 0.9609\n",
      "Epoch 56/100\n",
      "52500/52500 [==============================] - 6s 118us/step - loss: 0.0936 - acc: 0.9731 - val_loss: 0.1265 - val_acc: 0.9633\n",
      "Epoch 57/100\n",
      "52500/52500 [==============================] - 6s 117us/step - loss: 0.0918 - acc: 0.9739 - val_loss: 0.1234 - val_acc: 0.9644\n",
      "Epoch 58/100\n",
      "52500/52500 [==============================] - 6s 118us/step - loss: 0.0900 - acc: 0.9745 - val_loss: 0.1221 - val_acc: 0.9644\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52500/52500 [==============================] - 6s 119us/step - loss: 0.0884 - acc: 0.9745 - val_loss: 0.1207 - val_acc: 0.9646\n",
      "Epoch 60/100\n",
      "52500/52500 [==============================] - 6s 121us/step - loss: 0.0867 - acc: 0.9750 - val_loss: 0.1311 - val_acc: 0.9609\n",
      "Epoch 61/100\n",
      "52500/52500 [==============================] - 7s 133us/step - loss: 0.0852 - acc: 0.9755 - val_loss: 0.1188 - val_acc: 0.9649\n",
      "Epoch 62/100\n",
      "52500/52500 [==============================] - 6s 116us/step - loss: 0.0834 - acc: 0.9760 - val_loss: 0.1163 - val_acc: 0.9662\n",
      "Epoch 63/100\n",
      "52500/52500 [==============================] - 7s 125us/step - loss: 0.0820 - acc: 0.9767 - val_loss: 0.1161 - val_acc: 0.9661\n",
      "Epoch 64/100\n",
      "52500/52500 [==============================] - 6s 113us/step - loss: 0.0804 - acc: 0.9772 - val_loss: 0.1130 - val_acc: 0.9676\n",
      "Epoch 65/100\n",
      "52500/52500 [==============================] - 6s 110us/step - loss: 0.0790 - acc: 0.9774 - val_loss: 0.1122 - val_acc: 0.9669\n",
      "Epoch 66/100\n",
      "52500/52500 [==============================] - 6s 108us/step - loss: 0.0777 - acc: 0.9776 - val_loss: 0.1107 - val_acc: 0.9675\n",
      "Epoch 67/100\n",
      "52500/52500 [==============================] - 6s 107us/step - loss: 0.0761 - acc: 0.9782 - val_loss: 0.1103 - val_acc: 0.9674\n",
      "Epoch 68/100\n",
      "52500/52500 [==============================] - 6s 110us/step - loss: 0.0746 - acc: 0.9786 - val_loss: 0.1135 - val_acc: 0.9669\n",
      "Epoch 69/100\n",
      "52500/52500 [==============================] - 6s 108us/step - loss: 0.0734 - acc: 0.9796 - val_loss: 0.1099 - val_acc: 0.9673\n",
      "Epoch 70/100\n",
      "52500/52500 [==============================] - 6s 108us/step - loss: 0.0723 - acc: 0.9797 - val_loss: 0.1065 - val_acc: 0.9685\n",
      "Epoch 71/100\n",
      "52500/52500 [==============================] - 6s 108us/step - loss: 0.0709 - acc: 0.9800 - val_loss: 0.1113 - val_acc: 0.9668\n",
      "Epoch 72/100\n",
      "52500/52500 [==============================] - 6s 108us/step - loss: 0.0696 - acc: 0.9806 - val_loss: 0.1142 - val_acc: 0.9661\n",
      "Epoch 73/100\n",
      "52500/52500 [==============================] - 6s 109us/step - loss: 0.0683 - acc: 0.9806 - val_loss: 0.1077 - val_acc: 0.9682\n",
      "Epoch 74/100\n",
      "52500/52500 [==============================] - 6s 109us/step - loss: 0.0671 - acc: 0.9814 - val_loss: 0.1027 - val_acc: 0.9695\n",
      "Epoch 75/100\n",
      "52500/52500 [==============================] - 6s 108us/step - loss: 0.0661 - acc: 0.9819 - val_loss: 0.1018 - val_acc: 0.9689\n",
      "Epoch 76/100\n",
      "52500/52500 [==============================] - 6s 108us/step - loss: 0.0651 - acc: 0.9819 - val_loss: 0.1028 - val_acc: 0.9692\n",
      "Epoch 77/100\n",
      "52500/52500 [==============================] - 6s 106us/step - loss: 0.0639 - acc: 0.9820 - val_loss: 0.1030 - val_acc: 0.9687\n",
      "Epoch 78/100\n",
      "52500/52500 [==============================] - 6s 107us/step - loss: 0.0628 - acc: 0.9824 - val_loss: 0.1009 - val_acc: 0.9700\n",
      "Epoch 79/100\n",
      "52500/52500 [==============================] - 6s 109us/step - loss: 0.0617 - acc: 0.9827 - val_loss: 0.0991 - val_acc: 0.9700\n",
      "Epoch 80/100\n",
      "52500/52500 [==============================] - 6s 107us/step - loss: 0.0607 - acc: 0.9832 - val_loss: 0.1010 - val_acc: 0.9695\n",
      "Epoch 81/100\n",
      "52500/52500 [==============================] - 6s 107us/step - loss: 0.0597 - acc: 0.9836 - val_loss: 0.1006 - val_acc: 0.9695\n",
      "Epoch 82/100\n",
      "52500/52500 [==============================] - 6s 108us/step - loss: 0.0586 - acc: 0.9838 - val_loss: 0.0996 - val_acc: 0.9701\n",
      "Epoch 83/100\n",
      "52500/52500 [==============================] - 6s 108us/step - loss: 0.0579 - acc: 0.9840 - val_loss: 0.0974 - val_acc: 0.9710\n",
      "Epoch 84/100\n",
      "52500/52500 [==============================] - 6s 108us/step - loss: 0.0567 - acc: 0.9845 - val_loss: 0.0986 - val_acc: 0.9698\n",
      "Epoch 85/100\n",
      "52500/52500 [==============================] - 6s 109us/step - loss: 0.0559 - acc: 0.9846 - val_loss: 0.0976 - val_acc: 0.9708\n",
      "Epoch 86/100\n",
      "52500/52500 [==============================] - 6s 117us/step - loss: 0.0549 - acc: 0.9850 - val_loss: 0.0966 - val_acc: 0.9706\n",
      "Epoch 87/100\n",
      "52500/52500 [==============================] - 6s 119us/step - loss: 0.0539 - acc: 0.9854 - val_loss: 0.0973 - val_acc: 0.9706\n",
      "Epoch 88/100\n",
      "52500/52500 [==============================] - 6s 113us/step - loss: 0.0531 - acc: 0.9853 - val_loss: 0.0937 - val_acc: 0.9719\n",
      "Epoch 89/100\n",
      "52500/52500 [==============================] - 6s 115us/step - loss: 0.0521 - acc: 0.9858 - val_loss: 0.0943 - val_acc: 0.9718\n",
      "Epoch 90/100\n",
      "52500/52500 [==============================] - 6s 108us/step - loss: 0.0514 - acc: 0.9860 - val_loss: 0.0938 - val_acc: 0.9717\n",
      "Epoch 91/100\n",
      "52500/52500 [==============================] - 6s 109us/step - loss: 0.0506 - acc: 0.9864 - val_loss: 0.0941 - val_acc: 0.9718\n",
      "Epoch 92/100\n",
      "52500/52500 [==============================] - 6s 108us/step - loss: 0.0498 - acc: 0.9865 - val_loss: 0.0931 - val_acc: 0.9715\n",
      "Epoch 93/100\n",
      "52500/52500 [==============================] - 6s 108us/step - loss: 0.0492 - acc: 0.9868 - val_loss: 0.0973 - val_acc: 0.9706\n",
      "Epoch 94/100\n",
      "52500/52500 [==============================] - 6s 109us/step - loss: 0.0480 - acc: 0.9875 - val_loss: 0.0929 - val_acc: 0.9719\n",
      "Epoch 95/100\n",
      "52500/52500 [==============================] - 6s 108us/step - loss: 0.0475 - acc: 0.9873 - val_loss: 0.0925 - val_acc: 0.9716\n",
      "Epoch 96/100\n",
      "52500/52500 [==============================] - 6s 107us/step - loss: 0.0467 - acc: 0.9878 - val_loss: 0.0918 - val_acc: 0.9719\n",
      "Epoch 97/100\n",
      "52500/52500 [==============================] - 6s 109us/step - loss: 0.0459 - acc: 0.9880 - val_loss: 0.0906 - val_acc: 0.9726\n",
      "Epoch 98/100\n",
      "52500/52500 [==============================] - 6s 107us/step - loss: 0.0452 - acc: 0.9881 - val_loss: 0.0924 - val_acc: 0.9719\n",
      "Epoch 99/100\n",
      "52500/52500 [==============================] - 6s 109us/step - loss: 0.0444 - acc: 0.9883 - val_loss: 0.0910 - val_acc: 0.9729\n",
      "Epoch 100/100\n",
      "52500/52500 [==============================] - 6s 108us/step - loss: 0.0435 - acc: 0.9885 - val_loss: 0.0894 - val_acc: 0.9731\n"
     ]
    }
   ],
   "source": [
    "# Import sklearn and keras tools\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "\n",
    "# Download the full MNIST dataset\n",
    "dataset = datasets.fetch_mldata(\"MNIST Original\")\n",
    "\n",
    "# Normalize data\n",
    "data = dataset.data.astype(\"float\")/255.0\n",
    "\n",
    "# Split data into sets\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, dataset.target, test_size=0.25)\n",
    "\n",
    "# Binarize labels\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)\n",
    "\n",
    "# Prepare the feedforward neural network with keras\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(784,), activation=\"sigmoid\"))\n",
    "model.add(Dense(128, activation=\"sigmoid\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Train the neural network\n",
    "print(\"[INFO] training...\")\n",
    "sgd = SGD(0.1)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=100, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
