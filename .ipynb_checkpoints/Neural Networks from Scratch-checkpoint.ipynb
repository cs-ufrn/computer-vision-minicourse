{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook imports, do not care about them!\n",
    "import jdc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "\n",
    "Redes Neurais de Múltiplas Camadas (RNM), ou Multilayer Perceptron (MLP), são modelos computacionais inspirados no sistema nervoso humano, compostas por estruturas matemáticas que simulam os neurônios e suas conexões. Os usos mais comuns desses modelos consistem em tarefas de classificação e regressão, \n",
    "o que favorece a aplicação em diversas áreas de pesquisa, sendo uma delas\n",
    "a de Visão Computacional.\n",
    "\n",
    "Para compreender as RNM, é importante primeiramente conhecer dois de seus aspectos fundamentais: os **neurônios** e a **arquitetura em camadas**.\n",
    "\n",
    "## Neurônios\n",
    "Um neurônio é uma estrutura que aceita como argumento um vetor $\\mathbf{x} = \\langle x_1, x_2, \\ldots, x_n \\rangle \\in \\mathbb{R}^n$\n",
    "de entrada e **responde com um valor real** de saída. Cada neurônio está relacionado a um vetor de pesos $\\mathbf{w} = \\langle w_1, w_2, \\ldots, w_n \\rangle \\in \\mathbb{R}^n$, de forma que cada componente $x_i$ da entrada é associada ao peso $w_i$. Ao receber a entrada,\n",
    "o neurônio responde com o valor $f(\\mathbf{x}\\cdot\\mathbf{w} + b)$, onde $f$ é chamada de \n",
    "**função de ativação**; $\\mathbf{x}\\cdot\\mathbf{w} = \\sum_i x_i \\cdot w_i$, ou seja, \n",
    "o produto interno entre $\\mathbf{x}$ e $\\mathbf{w}$, também chamado de $net$ do neurônio; e $b$ é \n",
    "uma constante real chamada *bias*, cuja principal função é permitir um ajuste fino \n",
    "do valor de saída por meio de um deslocamento horizontal da função de ativação.\n",
    "\n",
    "Uma função comum de ativação é a **sigmoide**, expressa pela equação:\n",
    "$$f(x) = \\frac{1}{1 + e^{-x}}.$$\n",
    "\n",
    "Ela possui a importante propriedade de ser contínua e derivável em $(-\\infty, +\\infty)$,\n",
    "o que favorece o algoritmo de treinamento a ser em breve explicado, apesar de existirem algumas desvantagens em sua utilização.\n",
    "\n",
    "\n",
    "## Arquitetura em camadas\n",
    "\n",
    "Uma RNM é organizada em uma sequência de $k > 2$ camadas de neurônios, denotadas aqui por $L_0, L_1, \\ldots, L_{k-1}$. O tamanho de uma camada $L_i$, denotado aqui por $tam\\;L_i$, é a quantidade de neurônios que ela possui. A camada $L_0$ é dita de entrada, na qual existe um **neurônio de entrada** para cada componente do vetor de entrada da rede. Esses neurônios se diferenciam dos demais porque,\n",
    "ao invés de aceitarem todo o vetor, recebem apenas uma componente e simplesmente a **transmitem** para o interior da rede, sem \n",
    "a aplicação da função de ativação. Já a camada $L_{k-1}$ é dita de saída,\n",
    "e a sua resposta é a resposta da rede.\n",
    "\n",
    "O que viabiliza o funcionamento do modelo são as conexões entre essas camadas, e elas podem ocorrer de diversas maneiras na rede. Aqui,  \n",
    "o objeto de estudo são as **redes densas *feedforward***, em que cada neurônio da camada $L_i$ se conecta com\n",
    "todos os neurônios da camada $L_{i+1}$, $i = 0,\\ldots, k-2$, de maneira que o vetor de entrada dos neurônios da camada $L_{i+1}$ é o vetor composto pelas\n",
    "respostas dos neurônios da camada $L_{i}$. Assim, um neurônio na camada $L_{i+1}$ recebe, como entrada, um vetor\n",
    "de dimensão $tam\\;L_i$, $i=0,1, \\ldots, k-2$.\n",
    "\n",
    "Cada conexão entre neurônios pode ser vista como associada ao peso que o neurônio\n",
    "atribui ao valor por ela provido. Essa visão possibilita a utilização de uma\n",
    "notação muito útil para se trabalhar com pesos: denota-se por $w_{ij}^l$ o peso \n",
    "da conexão entre o neurônio $j$ da camada $l-1$ com o neurônio $i$ da camada $l$,\n",
    "$l = 1, \\ldots, k-1$. Mais ainda, é possível representar todas as conexões entre\n",
    "duas camadas por uma matriz de pesos $\\mathbf{W}^l = (w_{ij}^l)$ de dimensão $tam\\;L_{l} \\times tam\\;L_{l-1}$.\n",
    "Para facilitar a identificação dos *biases* de cada neurônio em uma camada, denota-se por \n",
    "$\\mathbf{b}^l = \\langle b^l_1, b^l_2, \\ldots, b^l_{tam\\;L_l} \\rangle$ os *biases* \n",
    "dos neurônios da camada $L_l$, sendo $b^l_i$ o *bias* do neurônio $i$ na camada $L_l$.\n",
    "\n",
    "Sabendo disso, já é possível iniciar a implementação em Python da RNM!\n",
    "\n",
    "### Implementando a classe `MultilayerNeuralNetwork`\n",
    "\n",
    "Como o objetivo é\n",
    "produzir um módulo reutilizável, os recursos de orientação a objetos da linguagem Python\n",
    "serão aplicados. Além disso, o pacote `numpy` será utilizado para a manipulação de matrizes. \n",
    "\n",
    "A programação tem início com a importação do `numpy` e a declaração da classe `MultilayerNeuralNetwork`, a qual encapsulará todos os dados e métodos necessários ao\n",
    "funcionamento e à utilização da rede, como a matriz de pesos, a arquitetura e as rotinas de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class MultilayerNeuralNetwork:\n",
    "    '''A Multilayer Neural Network implementation.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa é apenas uma classe, e nada tem a ver com redes neurais além do nome simplesmente. O próximo passo é criar um **construtor**, método que sempre será chamado quando alguém\n",
    "criar (ou, mais tecnicamente, instanciar) a rede neural. Nele, é importante garantir\n",
    "a criação e inicialização da arquitetura, o que implica na configuração das camadas (quantas e quantos neurônios devem possuir), das matrizes de pesos e dos vetores de *bias*.  Estas serão inicializadas com valores aleatórios, seguindo\n",
    "a distribuição normal, e, em seguida, normalizadas pela raiz quadrada da quantidade de neurônios da camada correspondente. A inicialização dos pesos é um tópico importante, para o qual existem diversas propostas, mas que será abstraído neste momento por economia de tempo.\n",
    "Além disso, um parâmetro `alpha` será acrescentado, \n",
    "cuja existência será justificada mais adiante. Portanto, o construtor receberá do\n",
    "usuário a **arquitetura da rede**, no formato $[tam\\;L_0, tam\\;L_1, \\ldots, tam\\;L_{k-1}]$, e o tal `alpha`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def __init__(self, arch = [1,2,1], alpha = 0.1):\n",
    "        '''Initialize the network.'''\n",
    "        \n",
    "        self.W = {}\n",
    "        self.B = {}\n",
    "        self.alpha = alpha\n",
    "        self.arch = arch\n",
    "        \n",
    "        # Initialize the weight matrix and biases with normalized random values\n",
    "        for i in np.arange(1,len(self.arch)):\n",
    "            # Weights\n",
    "            w = np.random.randn(self.arch[i], self.arch[i-1])\n",
    "            self.W[i] = w/np.sqrt(self.arch[i])\n",
    "            # Biases\n",
    "            b = np.random.randn(self.arch[i],1)\n",
    "            self.B[i] = b/np.sqrt(self.arch[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como será utilizada a função sigmoide para a ativação dos neurônios, é importante\n",
    "que ela e sua derivada estejam implementadas na classe:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def sigmoid(self, x):\n",
    "        '''Sigmoid function.'''\n",
    "        return 1.0/(1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_deriv(self, x):\n",
    "        '''Derivative of the sigmoid function, considering that x is the result\n",
    "        of applying the sigmoid function to the net.\n",
    "        '''\n",
    "        return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso, é possível instanciar uma rede, mesmo que não se possa fazer nada com ela ainda, apenas para checar a sua estrutura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: array([[ 0.44721243, -1.13009679],\n",
      "       [ 0.48586539, -0.16278593],\n",
      "       [-0.40227884, -0.94164842]]), 2: array([[-1.62313517, -1.68975618, -0.82730578]])}\n",
      "{1: array([[-0.05423398],\n",
      "       [-1.28780246],\n",
      "       [-0.722561  ]]), 2: array([[ 0.25346544]])}\n"
     ]
    }
   ],
   "source": [
    "# Criando uma arquitetura com 2 neurônios na camada de entrada, uma camada escondida com 3 neurônios e 1 neurônio\n",
    "# na camada de saída.\n",
    "neuralnet = MultilayerNeuralNetwork(arch = [2,3,1], alpha = 0.5)\n",
    "print(neuralnet.W)\n",
    "print(neuralnet.B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respostas das camadas\n",
    "\n",
    "Note que cada camada produz um vetor de resposta composto pelas respostas individuais\n",
    "de cada um de seus neurônios. Denote por $\\mathbf{a^l} = \\langle a^l_1, a^l_2, \\ldots, a^l_{tam\\; L_l} \\rangle$ esse vetor para a camada $L_l$. Como visto, ele deverá ser a entrada para a camada seguinte, mas os seus valores serão combinados de forma particular\n",
    "pela matriz de pesos $\\mathbf{W}^{l+1}$. A expressão\n",
    "\n",
    "$$\\mathbf{z}^l = \\mathbf{W}^{l+1}\\cdot\\mathbf{a^l} + \\mathbf{b}^l$$\n",
    "\n",
    "produz um vetor cujos componentes são os valores $net$ de cada neurônio da camada\n",
    "$L_{l+1}$. Sabendo disso, qual seria a expressão para o vetor de saída da camada $L_{l+1}$, ou seja, para $\\mathbf{a}^{l+1}$?\n",
    "Basta aplicar a função de ativação a cada componente do vetor obtido! Considerando-se a aplicação da função ponto a ponto, ou seja, $f(\\langle x_1, x_2, \\ldots, x_n \\rangle) = \\langle f(x_1), f(x_2), \\ldots, f(x_n) \\rangle$, tem-se que:\n",
    "\n",
    "$$\\mathbf{a}^{l+1} = f(\\mathbf{W}^{l+1}\\cdot\\mathbf{a^l} + \\mathbf{b}^l) = f(\\mathbf{z}^l).$$\n",
    "\n",
    "O conhecimento adquirido até este ponto é suficiente para se entender a arquitetura de uma rede neural e como computar as saídas das suas camadas de neurônios a partir \n",
    "de um vetor de entrada. Já podemos implementar uma função muito importante da nossa rede neural: dado um vetor de entrada, qual é o vetor de resposta da rede, ou seja, qual o vetor na camada de saída? Essa função é comumente chamada de `predict`, pois é utilizada para gerar a previsão (resposta) da rede a partir de uma entrada. Note como é simples e advém diretamente da definição:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def predict(self, X, outputs=False):\n",
    "        ''' Given an input column vector X, compute the output of the network. '''\n",
    "        # Set the result of the input layer\n",
    "        p = np.atleast_2d(X).T\n",
    "        # Store outputs\n",
    "        if outputs: \n",
    "            A = {}\n",
    "            A[0] = p\n",
    "        # Compute the output of each layer, following the definition\n",
    "        for layer in np.arange(1, len(self.arch)):\n",
    "            p = self.sigmoid(np.dot(self.W[layer], p) + self.B[layer])\n",
    "            if outputs: A[layer] = p\n",
    "        # Return accordingly to outputs option\n",
    "        return A if outputs else p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ocorre que apenas saber como obter a saída da rede não resolve problema algum: as matrizes de pesos aleatórios \n",
    "tornam o modelo inútil. Ele necessita se ajustar (lê-se \"aprender\")\n",
    "para solucionar os problemas com os quais é confrontado, e é disso que \n",
    "trataremos a partir de agora!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado em uma RNM\n",
    "\n",
    "O modelo de aprendizagem de redes neurais é geralmente o **supervisionado**, pois faz\n",
    "uso de exemplos representantes da verdade (*ground truth*) para fazer\n",
    "com que a rede gere as respostas desejadas após uma **etapa de treinamento**. É como se um professor\n",
    "apresentasse a entrada, a rede respondesse e ele informasse qual foi o \n",
    "erro cometido. A rede, com base nisso, modifica sua estrutura (suas matrizes\n",
    " de pesos) para que o erro, da próxima vez que o professor mostrar\n",
    " o exemplo, seja garantidamente menor. Esse processo termina quando algum critério de parada é atingido. Os mais comuns são o erro máximo e o número de ciclos ou *epochs*.\n",
    " \n",
    " O erro cometido pela rede, ao responder para dado exemplo $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$, é modelado matematicamente por uma **função de custo**\n",
    " $C_x:\\mathbb{R}^{k+1} \\to \\mathbb{R}$, onde $k$ é a quantidade de pesos. As funções que podem desempenhar esse papel devem cumprir alguns requisitos\n",
    " básicos e dependem dos valores dos pesos e dos *biases*. Uma das mais comuns é a do **erro quadrático**, de equação:\n",
    " \n",
    " $$C_x = \\frac 1 2 ||\\mathbf{y}- \\mathbf{a}^{k-1}(\\mathbf{x})||^2 = \\frac 1 2 \\sum_i (y_i - a^{k-1}_i(\\mathbf{x}))^2,$$\n",
    "  $a^{k-1}(\\mathbf{x})$ é a saída da última camada da rede para o exemplo $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$ (que nossa rede já sabe computar). Note que\n",
    " quanto maior a diferença entre a verdade e a saída da rede, maior o valor dessa\n",
    " função, o que mostra sua adequação para representar o erro.\n",
    " \n",
    " A fim de representar o erro com respeito a todos os exemplos, define-se também a função\n",
    " de custo $\\bar{C}:\\mathbb{R}^{k+1} \\to \\mathbb{R}$, dada pela média dos erros para cada exemplo, ou seja:\n",
    " \n",
    " $$\n",
    "     \\bar{C} = \\frac 1 n \\sum_{\\langle \\mathbf{x}, \\mathbf{y} \\rangle \\in \\mathcal{D}} C_x = \\frac{1}{2n} \\sum_{\\langle \\mathbf{x}, \\mathbf{y} \\rangle \\in \\mathcal{D}} ||\\mathbf{y}- \\mathbf{a}^{k-1}(\\mathbf{x})||^2.\n",
    " $$ \n",
    " \n",
    " Precisaremos, em nossa implementação, do cálculo da função de custo quadrática. Façamos, pois:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def quadratic_loss(self, X, targets):\n",
    "        ''' Compute the total quadratic loss, given a matrix of data. '''\n",
    "        targets = np.atleast_2d(targets).T\n",
    "        predictions = self.predict(X)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "        return loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Assim, o objetivo do treinamento é fazer com que o valor de $C_x$ (e $\\bar{C}$, consequentemente), para cada exemplo $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$,\n",
    " seja menor a cada ciclo. Como $C_x$ é função dos pesos e dos *biases*, nada melhor que buscar ajustá-los a fim de alcançar esse objetivo. Esse ajuste ocorre por meio de algum\n",
    " **método de otimização**, cujo objetivo é encontrar um conjunto de valores para pesos e *biases* tal que o valor de $\\bar{C}$ seja tão menor quanto se queira. Um método \n",
    " muito utilizado para tanto chama-se **gradiente descendente**, do qual a próxima seção tratará.\n",
    " \n",
    " ## Gradiente descendente\n",
    " \n",
    " Fujamos do escopo das redes neurais, e tratemos do problema geral de minimizar uma função\n",
    " de múltiplas variáveis $C:\\mathbb{R}^n \\to \\mathbb{R}$. Queremos encontrar o vetor $\\mathbf{v} = \\langle v_1, v_2, \\ldots, v_n \\rangle$ tal que $C(\\mathbf{v}) = C(v_1, v_2, \\ldots, v_n)$ seja um mínimo global de $C$.\n",
    " \n",
    " Para conseguir a intuição sobre o método, vale reduzir a dimensão de entrada de $C$ para\n",
    " duas variáveis, $v_1$ e $v_2$, e imaginar que seu gráfico assume a forma de um vale. Por exemplo:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fcfb1aba198>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcfd80afb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def quad(x,y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "x = np.linspace(-10,10,30)\n",
    "y = np.linspace(-10,10,30)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "Z = quad(X,Y)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X,Y,Z,rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "ax.set_title('Quadratic plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função acima é bem simples, devemos concordar, e, apenas observando seu gráfico, é possível encontrar o mínimo. Também, utilizando técnicas primárias de cálculo, é possível encontrar o mínimo global sem muito esforço nesse caso. Porém, quando lidamos com aprendizado, geralmente estamos interessados em minimizar funções de dezenas, centenas, milhares de variáveis! É por isso que as técnicas tradicionais de cálculo se tornam inviáveis, e outros métodos entram em cena.\n",
    "\n",
    "O gradiente descendente, sendo um deles, não busca encontrar um mínimo global, mas sim **algum mínimo para a função**. Isso pode ser um problema em alguns casos, mas não se mostra um empecilho grave para o seu uso em muitas aplicações de redes neurais. Imagine uma bola colocada em algum ponto arbitrário do vale mostrado no gráfico acima. Nossa experiência diz que ela começará a rolar até o ponto mais baixo do vale, e esse fato nos dá uma heurística para encontrar um mínimo da função: encontremos uma maneira de mover a bola nas direções\n",
    "$v_1$ e $v_2$ de forma que ela sempre esteja descendo o vale! \n",
    "\n",
    "Já que queremos mover a bola, vamos representar o deslocamento por um vetor $\\Delta v = \\langle \\Delta v_1, \\Delta v_2 \\rangle$. Assumindo a bola inicialmente na posição $v = \\langle v_1, v_2 \\rangle$, qual será a mudança no valor de $C$ causada por esse deslocamento? Ou seja, como podemos estimar o valor de $\\Delta C = C(v+\\Delta v)-C(v)$?\n",
    "Do polinômio de Taylor para o caso Multivariável, temos que\n",
    "\n",
    "$$\\Delta C \\approx \\nabla C(v) \\cdot \\Delta v = \\left \\langle \\frac{\\partial C}{\\partial v_1}, \\frac{\\partial C}{\\partial v_2} \\right \\rangle \\cdot \\langle \\Delta v_1, \\Delta v_2 \\rangle ^T = \\frac{\\partial C}{\\partial v_1}\\Delta v_1 + \\frac{\\partial C}{\\partial v_2} \\Delta v_2.$$\n",
    "\n",
    "Queremos reduzir o valor de $C$, correto? Para isso, devemos escolher um valor de $\\Delta v$ apropriado, tal que $\\Delta C < 0$. Pela equação acima, isso se torna muito fácil! Vamos tomar $\\Delta v$ como\n",
    "\n",
    "$$\\Delta v = -\\alpha \\nabla C^T,$$\n",
    "onde $\\alpha \\in \\mathbb{R}_+$. Ainda não convencido de que o $\\Delta C$ causado por essa variação $\\Delta v$ seja negativa? Vamos substituir:\n",
    "\n",
    "$$\\Delta C \\approx \\nabla C \\cdot \\Delta v = \\nabla C \\cdot (-\\alpha \\nabla C^T) = -\\alpha (\\nabla C \\cdot \\nabla C^T ) = -\\alpha ||\\nabla C||^2 \\leq 0,$$\n",
    "já que $||\\nabla C||^2 \\geq 0$ e $\\alpha > 0$. Isso nos garante que, sempre que escolhermos o $\\Delta v$ daquela forma, o valor do $C$ (o valor do erro!) sempre vai diminuir (claro, dentro dos limites da aproximação).\n",
    "\n",
    "Observe que utilizamos, aqui, uma função de duas variáveis, como forma de simplificação. Porém, note que, nas equações vetoriais acima, esse fato não se mostrou nada relevante! De fato, não importa a dimensão: a regra do gradiente descendente funcionará, e é por isso que podemos utilizá-la em nossa rede neural! Assim, para reduzirmos o valor da função de custo $C$, alteraremos os pesos $\\mathbf w$ e os biases $\\mathbf b$ (as variáveis de $C$) pela regra abaixo, considerando a aplicação em cada variável:\n",
    "\n",
    "$$w_k \\gets w_k + (-\\alpha \\frac{\\partial C}{\\partial w_k})$$\n",
    "$$b_l \\gets b_l + (-\\alpha \\frac{\\partial C}{\\partial b_l})$$\n",
    "\n",
    "No contexto das redes, o parâmetro $\\alpha$ se chama **taxa de aprendizado** (*learning rate*) e, geralmente assumindo um valor pequeno (0.05, por exemplo), almeja diminuir o risco de provocarmos um $\\Delta C$ muito elevado, o que poderia representar uma fuga de um ponto mínimo, ou um muito pequeno, o que atrasaria o processo de otimização.\n",
    "\n",
    "### O gradiente descendente estocástico\n",
    "\n",
    "Suponha que nosso conjunto de treino $\\mathcal{D}$ possua muitas instâncias. Como \n",
    "$C = \\sum_{x \\in \\mathcal{D}} C_x,$\n",
    "computar $\\nabla C$ seria muito custoso, pois envolveria computar todos os valores\n",
    "$\\nabla C_x$ e depois fazer $\\nabla C = \\frac{1}{n} \\sum_{x \\in \\mathcal{D}} \\nabla C_x$.\n",
    "Isso poderia atrasar muito o treinamento! \n",
    "\n",
    "O que comumente se faz é adaptar o método do gradiente descendente para sua versão\n",
    "estocástica. Isso significa particionar aleatoriamente o conjunto $\\mathcal D$ em uma família de conjuntos\n",
    "$\\{\\mathcal{D_i}\\}_i$ de mesma cardinalidade (tamanho) $m$. Seja $\\mathcal{D_j} = \\{x^j_1, x^j_2, \\ldots, x^j_m\\}$ algum desses conjuntos e suponha $m$ suficientemente grande.\n",
    "Consideramos que a média entre os $\\nabla C_{x^j_k}$ se aproxime da média dos $\\nabla C_{x}$, ou seja:\n",
    "\n",
    "$$\\frac{\\sum_{k = 1}^m \\nabla C_{x^j_k}}{m} \\approx \\frac{\\sum_{x \\in \\mathcal{D}} \\nabla C_x}{n} = \\nabla C.$$\n",
    "\n",
    "Fantástico! Agora não precisamos pegar sempre o conjunto inteiro e gigantesco $\\mathcal{D}$ para treinar a rede! Basta pegarmos porções menores (chamadas *mini-batches*), estimar $\\Delta C$ utilizando a aproximação acima, e atualizar os pesos e *biases*! Na prática, tomamos um conjunto com $m$  pontos arbitrários, $\\mathcal{D_j} = \\{x^j_1, x^j_2, \\ldots, x^j_m\\}$, e executamos as atualizações:\n",
    "\n",
    "$$w_k \\gets w_k + (-\\frac{\\alpha}{m} \\sum_{p = 1}^m \\frac{\\partial C_{x^j_p}}{\\partial w_k})$$\n",
    "$$b_l \\gets b_l + (-\\frac{\\alpha}{m} \\sum_{p = 1}^m \\frac{\\partial C_{x^j_p}}{\\partial b_l})$$\n",
    "\n",
    "Caso o treinamento ainda não seja o bastante, tomamos outro conjunto de tamanho $m$,\n",
    "e executamos as atualizações. Isso se repete até que o conjunto $\\mathcal{D}$\n",
    "se exaura, marcando o fim de uma *epoch* do treino. As próximas *epochs* repetirão esse processo.\n",
    "\n",
    "Que tal implementarmos nosso *stochastic gradient descent* em nossa rede neural?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def stochastic_gradient_descent(self, D, batch_size):\n",
    "        ''' Update the weights and bias through stochastic gradient \n",
    "        descent optimization. '''\n",
    "        # Get the amount of data points\n",
    "        train_size = len(D)\n",
    "        # Shuffle the data\n",
    "        random.shuffle(D)\n",
    "        # Obtain mini-batches\n",
    "        mini_batches = [D[k : k + batch_size] \n",
    "            for k in range(0, train_size, batch_size)]\n",
    "        # Update weights and biases\n",
    "        for mini_batch in mini_batches:\n",
    "            self.optimize_mini_batch(mini_batch)\n",
    "    \n",
    "    def optimize_mini_batch(self, mini_batch):\n",
    "        ''' Given a minibatch {<x1,y1>,<x2,y2>,...,<xm,ym>}, optimize the cost function. '''\n",
    "        # Get the mini-batch size\n",
    "        m = len(mini_batch)\n",
    "        # Gradient of C\n",
    "        GCw = {}\n",
    "        GCb = {}\n",
    "        # Initialize with zeroes\n",
    "        for i in np.arange(1,len(self.arch)):\n",
    "            GCw[i] = np.zeros(self.W[i].shape)\n",
    "            GCb[i] = np.zeros(self.B[i].shape)\n",
    "        # Use the SGD update rules for teaching the neural network for each batch point\n",
    "        for x, y in mini_batch:\n",
    "            # Compute gradients: see next section to understand how!\n",
    "            Gw, Gb = self.compute_gradient(x, y)\n",
    "            for i in np.arange(1,len(self.arch)): \n",
    "                GCw[i] += Gw[i]\n",
    "                GCb[i] += Gb[i]\n",
    "        # Apply rules for updating weights and biases\n",
    "        for l in np.arange(1,len(self.arch)):\n",
    "            self.W[l] = self.W[l] - (self.alpha/m)*(GCw[l])\n",
    "            self.B[l] = self.B[l] - (self.alpha/m)*(GCb[l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta seção nos deu um método muito poderoso para nossa rede neural aprender\n",
    "com a experiência! Existe, porém, um ponto ainda obscuro... **Como calcular o gradiente?** Ou melhor, como calcular de forma eficiente? O algoritmo da próxima seção tratará exatamente disso! Vamos aproveitar a estrutura em camadas da nossa rede para computar rapidamente gradientes de forma brilhante!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " ## O algoritmo Backpropagation\n",
    " \n",
    " Por questões de simplicidade, consideraremos um exemplo fixo $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$, e chamaremos $C_x$ de $C$.\n",
    " \n",
    "O ajuste dos pesos, pelo método do gradiente descendente, como visto, demanda o conhecimento sobre como variações nas matrizes\n",
    " de pesos - de camadas diferentes da última, inclusive - e dos *biases* afetam o resultado da função de custo. Em outras palavras, queremos conhecer as derivadas\n",
    " \n",
    " $$\\frac{\\partial C}{\\partial w_{ij}^l} \\qquad \\frac{\\partial C}{\\partial b_{i}^l},$$\n",
    " para todos os $w_{ij}^l$ e $b_{i}^l$, o que nos permitirá utilizar as regras de atualização do gradiente descendente e fazer a rede neural aprender.\n",
    " \n",
    " Computá-los de forma explícita seria dispendioso. Existe, porém, um caminho implícito. Imagine que o neurônio $j$ da camada $L_l$\n",
    " deva produzir o valor $net$ denotado por $z_j^l$. Porém, suponha que esse valor\n",
    " seja alterado por uma quantidade pequena $\\Delta z_j^l$, de tal forma que a resposta dele\n",
    " seja $f(z_j^l + \\Delta z_j^l)$. Como a função de custo é afetada por essa mudança?\n",
    " Do polinômio de Taylor para o Cálculo Multivariável, chega-se que \n",
    " \n",
    " $$\\Delta C = \\frac{\\partial C}{\\partial z_{j}^l} \\Delta z_{j}^l.$$\n",
    " \n",
    " Note que, se $\\frac{\\partial C}{\\partial z_{j}^l}$ \n",
    " possuir um valor alto, é necessário um $\\Delta z_{j}^l$ de sinal oposto\n",
    " para reduzir o custo. Se possuir um valor próximo de zero, \n",
    " como $\\Delta z_j^l$ é pequeno, não se pode fazer muito para reduzir o custo, e o neurônio é dito estar\n",
    " em estado próximo do ótimo. Assim, faz sentido definir o erro nesse neurônio, \n",
    " denotado por $\\delta_j^l$, como\n",
    " \n",
    " \\begin{equation}\n",
    " \\delta_j^l = \\frac{\\partial C}{\\partial z_{j}^l}.\n",
    " \\end{equation}\n",
    " \n",
    " Além disso, define-se o vetor de erros dos neurônios da camada $L_l$ por\n",
    " $\\delta^l = \\langle \\delta^l_1, \\delta^l_2, \\ldots, \\delta^l_{tam\\;L_l} \\rangle$. O algoritmo Backpropagation está fundamentado em quatro equações, das quais se pode obter\n",
    " o gradiente da função de custo, de interesse para as regras de atualização do gradiente descendente. Pode-se dizer que este é o grande viabilizador das redes neurais modernas, incluindo\n",
    "as utilizadas para *Deep Learning*. Para simplificar a notação, considere a camada $L_{k-1}$ (de saída) denotada pelo índice $L$.\n",
    "\n",
    "Resta-nos compreender cada equação.\n",
    "\n",
    "### Uma equação para o $\\delta^L$\n",
    "\n",
    "Esta equação fornece meios para se calcular o erro na camada de saída. A função de custo pode ser vista da forma \n",
    "\n",
    "$$C=C(X_1, X_2, \\ldots, X_{tam\\,L}),$$\n",
    "\n",
    "tal que $X_1 = a_1^L(z^L_1, z^L_2 \\ldots, z^L_{tam\\,L}), X_2 = a_2^L(z^L_1, z^L_2 \\ldots, z^L_{tam\\,L}), \\ldots, X_{tam\\,L} = a_{tam\\, L}^L(z^L_1, z^L_2 \\ldots, z^L_{tam\\,L})$. Pela definição anterior, sabemos que:\n",
    "\n",
    "$$\\delta_j^L = \\frac{\\partial C}{\\partial z_{j}^L}.$$\n",
    "\n",
    "Utilizando a **regra da cadeia** do Cálculo Multivariável, tem-se que\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z_{j}^L} = \\sum_k \\frac{\\partial C}{\\partial a^L_j}\\frac{\\partial a^L_k}{\\partial z^L_j}.$$\n",
    "\n",
    "Como apenas $a^L_j$ depende de $z^L_j$, todas as $\\frac{\\partial a^L_k}{\\partial z^L_j}$, com $k \\neq j$, serão anuladas, restando que:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z_{j}^L} =\\frac{\\partial C}{\\partial a^L_j}\\frac{\\partial a^L_j}{\\partial z^L_j}.$$\n",
    "\n",
    "Como $a^L_j = f(z^L_j)$, \n",
    "\n",
    "$$\\frac{\\partial a^L_j}{\\partial z^L_j} = f'(z^L_j).$$\n",
    "\n",
    "Finalmente, chega-se à equação\n",
    "\n",
    "$$\\delta^L_j = \\frac{\\partial C}{\\partial a^L_j}f'(z_j^L).$$\n",
    "\n",
    "Na forma vetorial, temos:\n",
    "\n",
    "$$\\delta^L = \\nabla C_a \\odot f'(\\mathbf{z}^L).$$\n",
    "\n",
    "### Uma equação para o $\\delta^l$ em função de $\\delta^{l+1}$\n",
    "\n",
    "Na equação passada, conseguimos uma forma de calcular os erros da última camada, entretanto, queremos também um modo\n",
    "de se calcularem os erros nas demais camadas. A estratégia é tentar escrever $\\delta^{l}_j = \\frac{\\partial C}{\\partial z^l_j}$ em termos de $\\delta^{l+1}_{j} = \\frac{\\partial C}{\\partial z^{l+1}_j}$. Como? Primeiro, é válido notar que $z_j^{l+1}$, por definição,\n",
    "depende $z_j^l$:\n",
    "\n",
    "$$z_j^{l+1} = \\mathbf{W}^{l+1}_ja^l = \\mathbf{W}^{l+1}_jf(z^l) = \\sum_i w^{l+1}_{ij}f(z^l_i).$$\n",
    "\n",
    "Além disso, $C$ depende de $z_j^{l+1}$, devido à definição recursiva de $a^n$. Com isso, basta aplicar a regra da cadeia novamente, e temos que:\n",
    "\n",
    "$$\\delta_j^l = \\sum_k \\frac{\\partial C}{\\partial z^{l+1}_k}\\frac{\\partial z^{l+1}_k}{\\partial z^l_j} = \\sum_k \\frac{\\partial z^{l+1}_k}{\\partial z^l_k}\\delta^{l+1}_k.$$\n",
    "\n",
    "Diferenciando a expressão para $z_j^{l+1}$, temos que:\n",
    "$$\\frac{\\partial z_k^{l+1}}{\\partial z_j^l} = w^{l+1}_{jk}f'(z^l_j).$$\n",
    "\n",
    "Assim, substituindo, chegamos a:\n",
    "$$\\delta_j^l = \\sum_k w^{l+1}_{jk}\\delta^{l+1}_kf'(z^l_j).$$\n",
    "\n",
    "Podemos obter a versão vetorial da seguinte forma:\n",
    "\n",
    "\\begin{align}\n",
    "\\delta_j^l = & (\\sum_k w^{l+1}_{kj}\\delta^{l+1}_k)f'(z^l_j) \\\\\n",
    "           = & ((w^{l+1}_{j})^T \\cdot \\delta^{l+1})f'(z^l_j)\n",
    "\\end{align}\n",
    "\n",
    "Essa é a forma componente a componente; para obtermos o vetor $\\delta_j$, basta\n",
    "utilizarmos o produto de Hadamard:\n",
    "\n",
    "$$\\delta^l = ((w^{l+1})^T \\cdot \\delta^{l+1}) \\odot f'(z^l)$$\n",
    "\n",
    "### Uma equação para $\\frac{\\partial C}{\\partial b^l_{j}}$\n",
    "\n",
    "Chegou a hora de obtermos as nossas derivadas de interesse! Começaremos por aquelas\n",
    "com respeito aos *biases*. Da regra da cadeira para o cálculo de muitas variáveis, temos que:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^l_{j}}=\\sum_m \\frac{\\partial C}{\\partial z_m^l} \\frac{\\partial z_m^l}{\\partial b_{j}^l}$$\n",
    "\n",
    "Note agora que $\\frac{\\partial z_m^l}{\\partial b_{j}^l}$, quando $m \\neq j$,\n",
    "é zero, pois $z_m^l$ não depende de $b_{j}^l$ nessas condições. Assim, a equação acima\n",
    "se reduz a:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^l_{j}}=\\frac{\\partial C}{\\partial z_j^l} \\frac{\\partial z_j^l}{\\partial b_{j}^l}$$\n",
    "\n",
    "Vamos lembrar a forma de $z_j^l$:\n",
    "\n",
    "$$z_j^l = \\sum_k a^{l-1}_k w^l_{kj} + b^l_j$$\n",
    "\n",
    "Daí, derivando $z_j^l$ com respeito a $b^l_j$, temos:\n",
    "\n",
    "$$\\frac{\\partial z_j^l}{\\partial b_{j}^l} = 1.$$\n",
    "\n",
    "Substituindo na equação mais acima, chegamos que, simplesmente:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^l_{j}}=\\frac{\\partial C}{\\partial z_j^l} = \\delta^l_j.$$\n",
    "\n",
    "### Uma equação para $\\frac{\\partial C}{\\partial w^l_{jk}}$\n",
    "\n",
    "Sabemos que $C$ está em função de $z^l$ e que $z^l$ está em função dos pesos,\n",
    "o que inclui $w_{jk}^l$. Por essa razão, podemos novamente aplicar a regra\n",
    "da cadeia, da seguinte forma:\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}}=\\sum_m \\frac{\\partial C}{\\partial z_m^l} \\frac{\\partial z_m^l}{\\partial w_{jk}^l}$$\n",
    "\n",
    "Note que $w^l_{jk}$ apenas influencia no cálculo de $z^l_j$, logo\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}}=\\frac{\\partial C}{\\partial z_j^l} \\frac{\\partial z_j^l}{\\partial w_{jk}^l}=\\delta^l_j\\frac{\\partial z_j^l}{\\partial w_{jk}^l}.$$\n",
    "\n",
    "Lembremo-nos de que:\n",
    "$$z_j^{l} = W^{l}_ja^{l-1} = \\sum_i w^{l}_{ij}a^{l-1}_i.$$\n",
    "\n",
    "Com isso, diferenciando, temos:\n",
    "$$\\frac{\\partial z_j^{l}}{\\partial w_{jk}^l} =  a^{l-1}_k.$$\n",
    "\n",
    "Finalmente, substituindo:\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}}= a^{l-1}_k \\delta^l_j.$$\n",
    "\n",
    "\n",
    "Temos, assim, todas as equações fundamentais que constituem o algoritmo\n",
    "Backpropagation, e elas nos dão todas as derivadas parciais de que precisamos para treinar nossa rede com o gradiente descendente! Note que o algoritmo Backpropagation é um método geral de computar derivadas parciais nesse contexto, e pode ser utilizado em outros métodos de otimização!\n",
    "\n",
    "Agora, resta implementarmos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def compute_gradient(self, x, y):\n",
    "        ''' Use backpropagation equations to compute the gradient of the cost function. '''\n",
    "        # The index of the output layer, L\n",
    "        L = len(self.arch)-1\n",
    "        # Dicts for gradients\n",
    "        Gw = {}\n",
    "        Gb = {}\n",
    "        # Compute the network output for x input sample\n",
    "        A = self.predict(x, outputs=True)\n",
    "        # Declare what will be set of delta vectors \n",
    "        D = {}\n",
    "        # Compute the error for the last neurons, using Equation 1\n",
    "        y = np.atleast_2d(y).T\n",
    "        deltaL = (A[L] - y) * self.sigmoid_deriv(A[L])\n",
    "        # Include in the D matrix\n",
    "        D[L] = deltaL\n",
    "        # Compute each delta{l} = g(delta{l+1}), using Equation 2\n",
    "        for l in np.arange(L-1, 0, -1):\n",
    "            D[l] = (self.W[l+1].T.dot(D[l+1])) * self.sigmoid_deriv(A[l])\n",
    "        for l in np.arange(L, 0, -1):\n",
    "            # Compute derivatives with respect to biases, using Equation 3\n",
    "            Gb[l] = D[l]\n",
    "            # Compute derivatives with respect to biases, using Equation 4\n",
    "            Gw[l] = D[l].dot(A[l-1].T)\n",
    "        # Return all derivatives\n",
    "        return (Gw, Gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós temos agora todos os componentes para construir uma função de treino, que chamaremos de `fit`. Ela, basicamente, executará todos os métodos criado acima, informando ao usuário dados importantes sobre o processo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def fit(self, X, Y, batch_size = 20, epochs = 1000, max_loss = 0.001, displayUpdate = 100):\n",
    "        ''' Train the neural network '''\n",
    "        # Assemble D dataset\n",
    "        D = list(zip(X,Y))\n",
    "        # Store losses for returning\n",
    "        losses = [self.quadratic_loss(X,Y)]\n",
    "        # Loop over epochs\n",
    "        current_epoch = 0\n",
    "        while current_epoch < epochs and losses[current_epoch] > max_loss:\n",
    "            # Optimize using gradient descent\n",
    "            self.stochastic_gradient_descent(D, batch_size)\n",
    "            # Compute and store loss\n",
    "            losses.append(self.quadratic_loss(X,Y))\n",
    "            # Print information\n",
    "            if current_epoch % displayUpdate == 0:\n",
    "                print(\"Epoch: {}, Loss: {:.7f}\".format(current_epoch + 1, losses[current_epoch]))\n",
    "            current_epoch += 1\n",
    "        # Print about stop reason\n",
    "        if (losses[current_epoch] < max_loss):\n",
    "            print(\"Stopped by max loss criterion, loss: {:.7f}.\", losses[current_epoch])\n",
    "        elif (current_epoch == epochs):\n",
    "            print(\"Stopped by epochs criterion.\")\n",
    "        # Return training information\n",
    "        return (losses, current_epoch)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando a rede neural\n",
    "\n",
    "Agora que temos nossa rede neural implementada, podemos resolver alguns problemas bastante interessantes. Aqui serão mostrados dois exemplos: o XOR e o reconhecimento de caracteres utilizando o *dataset* MNIST.\n",
    "\n",
    "## XOR\n",
    "\n",
    "XOR é uma operação bem conhecida da lógica proposicional que possui a seguinte tabela verdade:\n",
    "\n",
    "|$x_1$|$x_2$|$y$\n",
    "|---|---|---|\n",
    "|0|0|0|\n",
    "|1|0|1|\n",
    "|0|1|1|\n",
    "|1|1|0|\n",
    "\n",
    "A partir disso, tem-se o *dataset* XOR, composto pelos vetores:\n",
    "$\\langle 0,0 \\rangle, \\langle 1,0 \\rangle, \\langle 0,1 \\rangle, \\langle 1,1 \\rangle$,\n",
    "com os respectivos *labels* $0, 1 , 1, 0$, correspondendo a cada linha da tabela acima. O que acontece se plotarmos \n",
    "$x_1$ no eixo $x$, $x_2$ no eixo $y$ e atribuirmos uma cor para cada um dos dois valores possíveis para $y$? Note:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf9d48e160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x1 = np.array([[0],[1],[0],[1]])\n",
    "x2 = np.array([[0],[0],[1],[1]])\n",
    "\n",
    "plt.scatter(x1,x2,c=['green','red','red','green'])\n",
    "plt.title('The XOR dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que há de importante nesse *dataset*? **Não é possível separá-lo utilizando uma reta**! Ou seja, não corresponde a um problema de classificação com classes linearmente separáveis. Um neurônio somente não conseguiria resolvê-lo, mas nossa rede pode! Vamos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([1, 1]), array([0])), (array([1, 0]), array([1])), (array([0, 1]), array([1])), (array([0, 0]), array([0]))]\n",
      "[[1]\n",
      " [0]]\n",
      "Epoch: 1, Loss: 0.7039884\n",
      "Epoch: 101, Loss: 0.4963054\n",
      "Epoch: 201, Loss: 0.4854876\n",
      "Epoch: 301, Loss: 0.4555097\n",
      "Epoch: 401, Loss: 0.4059865\n",
      "Epoch: 501, Loss: 0.3637040\n",
      "Epoch: 601, Loss: 0.3316619\n",
      "Epoch: 701, Loss: 0.3037615\n",
      "Epoch: 801, Loss: 0.2589973\n",
      "Epoch: 901, Loss: 0.1225725\n",
      "Epoch: 1001, Loss: 0.0580436\n",
      "Epoch: 1101, Loss: 0.0342951\n",
      "Epoch: 1201, Loss: 0.0234794\n",
      "Epoch: 1301, Loss: 0.0175608\n",
      "Epoch: 1401, Loss: 0.0139003\n",
      "Epoch: 1501, Loss: 0.0114396\n",
      "Epoch: 1601, Loss: 0.0096838\n",
      "Epoch: 1701, Loss: 0.0083735\n",
      "Epoch: 1801, Loss: 0.0073616\n",
      "Epoch: 1901, Loss: 0.0065585\n",
      "Epoch: 2001, Loss: 0.0059066\n",
      "Epoch: 2101, Loss: 0.0053679\n",
      "Epoch: 2201, Loss: 0.0049155\n",
      "Epoch: 2301, Loss: 0.0045308\n",
      "Epoch: 2401, Loss: 0.0041997\n",
      "Epoch: 2501, Loss: 0.0039121\n",
      "Epoch: 2601, Loss: 0.0036600\n",
      "Epoch: 2701, Loss: 0.0034373\n",
      "Epoch: 2801, Loss: 0.0032392\n",
      "Epoch: 2901, Loss: 0.0030620\n",
      "Epoch: 3001, Loss: 0.0029026\n",
      "Epoch: 3101, Loss: 0.0027584\n",
      "Epoch: 3201, Loss: 0.0026274\n",
      "Epoch: 3301, Loss: 0.0025079\n",
      "Epoch: 3401, Loss: 0.0023984\n",
      "Epoch: 3501, Loss: 0.0022978\n",
      "Epoch: 3601, Loss: 0.0022051\n",
      "Epoch: 3701, Loss: 0.0021193\n",
      "Epoch: 3801, Loss: 0.0020398\n",
      "Epoch: 3901, Loss: 0.0019659\n",
      "Epoch: 4001, Loss: 0.0018969\n",
      "Epoch: 4101, Loss: 0.0018325\n",
      "Epoch: 4201, Loss: 0.0017722\n",
      "Epoch: 4301, Loss: 0.0017157\n",
      "Epoch: 4401, Loss: 0.0016625\n",
      "Epoch: 4501, Loss: 0.0016125\n",
      "Epoch: 4601, Loss: 0.0015653\n",
      "Epoch: 4701, Loss: 0.0015207\n",
      "Epoch: 4801, Loss: 0.0014785\n",
      "Epoch: 4901, Loss: 0.0014385\n",
      "Epoch: 5001, Loss: 0.0014006\n",
      "Epoch: 5101, Loss: 0.0013645\n",
      "Epoch: 5201, Loss: 0.0013303\n",
      "Epoch: 5301, Loss: 0.0012976\n",
      "Epoch: 5401, Loss: 0.0012665\n",
      "Epoch: 5501, Loss: 0.0012368\n",
      "Epoch: 5601, Loss: 0.0012085\n",
      "Epoch: 5701, Loss: 0.0011813\n",
      "Epoch: 5801, Loss: 0.0011554\n",
      "Epoch: 5901, Loss: 0.0011305\n",
      "Epoch: 6001, Loss: 0.0011066\n",
      "Epoch: 6101, Loss: 0.0010837\n",
      "Epoch: 6201, Loss: 0.0010618\n",
      "Epoch: 6301, Loss: 0.0010406\n",
      "Epoch: 6401, Loss: 0.0010203\n",
      "Epoch: 6501, Loss: 0.0010007\n",
      "Epoch: 6601, Loss: 0.0009819\n",
      "Epoch: 6701, Loss: 0.0009637\n",
      "Epoch: 6801, Loss: 0.0009462\n",
      "Epoch: 6901, Loss: 0.0009293\n",
      "Epoch: 7001, Loss: 0.0009130\n",
      "Epoch: 7101, Loss: 0.0008972\n",
      "Epoch: 7201, Loss: 0.0008819\n",
      "Epoch: 7301, Loss: 0.0008672\n",
      "Epoch: 7401, Loss: 0.0008529\n",
      "Epoch: 7501, Loss: 0.0008391\n",
      "Epoch: 7601, Loss: 0.0008257\n",
      "Epoch: 7701, Loss: 0.0008127\n",
      "Epoch: 7801, Loss: 0.0008001\n",
      "Epoch: 7901, Loss: 0.0007879\n",
      "Epoch: 8001, Loss: 0.0007760\n",
      "Epoch: 8101, Loss: 0.0007645\n",
      "Epoch: 8201, Loss: 0.0007533\n",
      "Epoch: 8301, Loss: 0.0007425\n",
      "Epoch: 8401, Loss: 0.0007319\n",
      "Epoch: 8501, Loss: 0.0007216\n",
      "Epoch: 8601, Loss: 0.0007116\n",
      "Epoch: 8701, Loss: 0.0007019\n",
      "Epoch: 8801, Loss: 0.0006924\n",
      "Epoch: 8901, Loss: 0.0006832\n",
      "Epoch: 9001, Loss: 0.0006742\n",
      "Epoch: 9101, Loss: 0.0006654\n",
      "Epoch: 9201, Loss: 0.0006569\n",
      "Epoch: 9301, Loss: 0.0006486\n",
      "Epoch: 9401, Loss: 0.0006405\n",
      "Epoch: 9501, Loss: 0.0006325\n",
      "Epoch: 9601, Loss: 0.0006248\n",
      "Epoch: 9701, Loss: 0.0006172\n",
      "Epoch: 9801, Loss: 0.0006099\n",
      "Epoch: 9901, Loss: 0.0006027\n",
      "Epoch: 10001, Loss: 0.0005956\n",
      "Epoch: 10101, Loss: 0.0005887\n",
      "Epoch: 10201, Loss: 0.0005820\n",
      "Epoch: 10301, Loss: 0.0005754\n",
      "Epoch: 10401, Loss: 0.0005690\n",
      "Epoch: 10501, Loss: 0.0005627\n",
      "Epoch: 10601, Loss: 0.0005566\n",
      "Epoch: 10701, Loss: 0.0005505\n",
      "Epoch: 10801, Loss: 0.0005446\n",
      "Epoch: 10901, Loss: 0.0005388\n",
      "Epoch: 11001, Loss: 0.0005332\n",
      "Epoch: 11101, Loss: 0.0005276\n",
      "Epoch: 11201, Loss: 0.0005222\n",
      "Epoch: 11301, Loss: 0.0005169\n",
      "Epoch: 11401, Loss: 0.0005117\n",
      "Epoch: 11501, Loss: 0.0005066\n",
      "Epoch: 11601, Loss: 0.0005015\n",
      "Epoch: 11701, Loss: 0.0004966\n",
      "Epoch: 11801, Loss: 0.0004918\n",
      "Epoch: 11901, Loss: 0.0004871\n",
      "Epoch: 12001, Loss: 0.0004824\n",
      "Epoch: 12101, Loss: 0.0004779\n",
      "Epoch: 12201, Loss: 0.0004734\n",
      "Epoch: 12301, Loss: 0.0004690\n",
      "Epoch: 12401, Loss: 0.0004647\n",
      "Epoch: 12501, Loss: 0.0004604\n",
      "Epoch: 12601, Loss: 0.0004563\n",
      "Epoch: 12701, Loss: 0.0004522\n",
      "Epoch: 12801, Loss: 0.0004482\n",
      "Epoch: 12901, Loss: 0.0004442\n",
      "Epoch: 13001, Loss: 0.0004403\n",
      "Epoch: 13101, Loss: 0.0004365\n",
      "Epoch: 13201, Loss: 0.0004328\n",
      "Epoch: 13301, Loss: 0.0004291\n",
      "Epoch: 13401, Loss: 0.0004255\n",
      "Epoch: 13501, Loss: 0.0004219\n",
      "Epoch: 13601, Loss: 0.0004184\n",
      "Epoch: 13701, Loss: 0.0004149\n",
      "Epoch: 13801, Loss: 0.0004115\n",
      "Epoch: 13901, Loss: 0.0004082\n",
      "Epoch: 14001, Loss: 0.0004049\n",
      "Epoch: 14101, Loss: 0.0004017\n",
      "Epoch: 14201, Loss: 0.0003985\n",
      "Epoch: 14301, Loss: 0.0003954\n",
      "Epoch: 14401, Loss: 0.0003923\n",
      "Epoch: 14501, Loss: 0.0003892\n",
      "Epoch: 14601, Loss: 0.0003862\n",
      "Epoch: 14701, Loss: 0.0003833\n",
      "Epoch: 14801, Loss: 0.0003804\n",
      "Epoch: 14901, Loss: 0.0003775\n",
      "Epoch: 15001, Loss: 0.0003747\n",
      "Epoch: 15101, Loss: 0.0003719\n",
      "Epoch: 15201, Loss: 0.0003692\n",
      "Epoch: 15301, Loss: 0.0003665\n",
      "Epoch: 15401, Loss: 0.0003638\n",
      "Epoch: 15501, Loss: 0.0003612\n",
      "Epoch: 15601, Loss: 0.0003586\n",
      "Epoch: 15701, Loss: 0.0003560\n",
      "Epoch: 15801, Loss: 0.0003535\n",
      "Epoch: 15901, Loss: 0.0003510\n",
      "Epoch: 16001, Loss: 0.0003486\n",
      "Epoch: 16101, Loss: 0.0003462\n",
      "Epoch: 16201, Loss: 0.0003438\n",
      "Epoch: 16301, Loss: 0.0003415\n",
      "Epoch: 16401, Loss: 0.0003391\n",
      "Epoch: 16501, Loss: 0.0003368\n",
      "Epoch: 16601, Loss: 0.0003346\n",
      "Epoch: 16701, Loss: 0.0003324\n",
      "Epoch: 16801, Loss: 0.0003302\n",
      "Epoch: 16901, Loss: 0.0003280\n",
      "Epoch: 17001, Loss: 0.0003259\n",
      "Epoch: 17101, Loss: 0.0003237\n",
      "Epoch: 17201, Loss: 0.0003217\n",
      "Epoch: 17301, Loss: 0.0003196\n",
      "Epoch: 17401, Loss: 0.0003176\n",
      "Epoch: 17501, Loss: 0.0003155\n",
      "Epoch: 17601, Loss: 0.0003136\n",
      "Epoch: 17701, Loss: 0.0003116\n",
      "Epoch: 17801, Loss: 0.0003097\n",
      "Epoch: 17901, Loss: 0.0003077\n",
      "Epoch: 18001, Loss: 0.0003059\n",
      "Epoch: 18101, Loss: 0.0003040\n",
      "Epoch: 18201, Loss: 0.0003021\n",
      "Epoch: 18301, Loss: 0.0003003\n",
      "Epoch: 18401, Loss: 0.0002985\n",
      "Epoch: 18501, Loss: 0.0002967\n",
      "Epoch: 18601, Loss: 0.0002950\n",
      "Epoch: 18701, Loss: 0.0002932\n",
      "Epoch: 18801, Loss: 0.0002915\n",
      "Epoch: 18901, Loss: 0.0002898\n",
      "Epoch: 19001, Loss: 0.0002881\n",
      "Epoch: 19101, Loss: 0.0002865\n",
      "Epoch: 19201, Loss: 0.0002848\n",
      "Epoch: 19301, Loss: 0.0002832\n",
      "Epoch: 19401, Loss: 0.0002816\n",
      "Epoch: 19501, Loss: 0.0002800\n",
      "Epoch: 19601, Loss: 0.0002785\n",
      "Epoch: 19701, Loss: 0.0002769\n",
      "Epoch: 19801, Loss: 0.0002754\n",
      "Epoch: 19901, Loss: 0.0002738\n",
      "Stopped by epochs criterion.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.70398844566777163,\n",
       "  0.66733688000262892,\n",
       "  0.62637842059611593,\n",
       "  0.58850029856233732,\n",
       "  0.55852341590298471,\n",
       "  0.54195876107312446,\n",
       "  0.52524982444310264,\n",
       "  0.51457904914494534,\n",
       "  0.50907415288666491,\n",
       "  0.50530873596132175,\n",
       "  0.50305045090155021,\n",
       "  0.50189499941130833,\n",
       "  0.50150541673943938,\n",
       "  0.50127217148139191,\n",
       "  0.50126483133033728,\n",
       "  0.50113933926837639,\n",
       "  0.5011867810344478,\n",
       "  0.50150842003901053,\n",
       "  0.50142998860463761,\n",
       "  0.50114728100596184,\n",
       "  0.50109877927058211,\n",
       "  0.50094485774494468,\n",
       "  0.50091523320870679,\n",
       "  0.50087991506807583,\n",
       "  0.50051677889852031,\n",
       "  0.5003953719437968,\n",
       "  0.50033414081183625,\n",
       "  0.50038811176596765,\n",
       "  0.50040631860133655,\n",
       "  0.50042024526358309,\n",
       "  0.50042567012743122,\n",
       "  0.5004215408258843,\n",
       "  0.50071841387917537,\n",
       "  0.50000787301807603,\n",
       "  0.49992759528493913,\n",
       "  0.49984757420636994,\n",
       "  0.49986891132766864,\n",
       "  0.4999207345703614,\n",
       "  0.50019396114068004,\n",
       "  0.4997749350275183,\n",
       "  0.49967365475634479,\n",
       "  0.499530047035182,\n",
       "  0.49947597318584003,\n",
       "  0.4994072366022535,\n",
       "  0.49943847309153144,\n",
       "  0.49935105626507981,\n",
       "  0.49954405166307614,\n",
       "  0.49920095365276906,\n",
       "  0.49920785103076426,\n",
       "  0.4994093312341773,\n",
       "  0.49912125634130639,\n",
       "  0.49904887377285673,\n",
       "  0.49899496649356478,\n",
       "  0.49904236590947892,\n",
       "  0.49884730438820901,\n",
       "  0.49880632397581481,\n",
       "  0.49874449716252039,\n",
       "  0.49869817538823896,\n",
       "  0.49871383310721851,\n",
       "  0.49896732951115069,\n",
       "  0.49893378428810892,\n",
       "  0.49868863630114124,\n",
       "  0.49853075764240845,\n",
       "  0.4984395230127151,\n",
       "  0.49837405804985657,\n",
       "  0.49855799044664312,\n",
       "  0.49821779453674331,\n",
       "  0.49818510971483765,\n",
       "  0.49818274487395059,\n",
       "  0.49840790231573956,\n",
       "  0.49837306058706732,\n",
       "  0.4986573488321655,\n",
       "  0.49840300314326136,\n",
       "  0.49814442216819599,\n",
       "  0.49819109274987267,\n",
       "  0.49816605653589818,\n",
       "  0.49779568713361444,\n",
       "  0.49766801232613211,\n",
       "  0.49762404881014816,\n",
       "  0.49755463770142611,\n",
       "  0.49747683594074943,\n",
       "  0.49763187632054723,\n",
       "  0.49787276146456744,\n",
       "  0.49761326468386313,\n",
       "  0.49725008732912263,\n",
       "  0.49732906977594082,\n",
       "  0.49738221938005422,\n",
       "  0.49770485845965756,\n",
       "  0.49719727759046056,\n",
       "  0.4975019729554393,\n",
       "  0.49704295234714563,\n",
       "  0.49683853907915898,\n",
       "  0.49677338357975542,\n",
       "  0.49671815490601118,\n",
       "  0.4968185727324147,\n",
       "  0.49710984362006755,\n",
       "  0.49690080555684107,\n",
       "  0.49649090752520297,\n",
       "  0.49662504825948994,\n",
       "  0.49638114797534133,\n",
       "  0.49630538994309936,\n",
       "  0.49630500489204515,\n",
       "  0.49654831547555711,\n",
       "  0.4961017860010154,\n",
       "  0.49605018762480391,\n",
       "  0.49599907574940527,\n",
       "  0.49599477444704332,\n",
       "  0.49604391303172923,\n",
       "  0.49580622630936982,\n",
       "  0.49572911402104874,\n",
       "  0.49566702292455911,\n",
       "  0.49583787928202655,\n",
       "  0.49561956987520134,\n",
       "  0.4954894486927004,\n",
       "  0.49542036340179962,\n",
       "  0.49535049813481913,\n",
       "  0.49524840067855896,\n",
       "  0.49538469291147325,\n",
       "  0.49569408201004583,\n",
       "  0.49568478492033002,\n",
       "  0.49540157395734752,\n",
       "  0.49507380247968025,\n",
       "  0.49480069079790662,\n",
       "  0.49473123386344914,\n",
       "  0.49464564833378738,\n",
       "  0.49456104971117065,\n",
       "  0.49447264489690201,\n",
       "  0.49438338518395358,\n",
       "  0.49452467784984089,\n",
       "  0.49420758171176365,\n",
       "  0.49426809998738902,\n",
       "  0.49405435056322378,\n",
       "  0.49401200454932004,\n",
       "  0.49389017959473192,\n",
       "  0.49385792116452748,\n",
       "  0.49383553277440356,\n",
       "  0.49370990267034159,\n",
       "  0.49355675918161424,\n",
       "  0.49350839040301098,\n",
       "  0.49339792322561632,\n",
       "  0.49325459665869625,\n",
       "  0.49319771294637271,\n",
       "  0.49306927775754683,\n",
       "  0.49298394136200496,\n",
       "  0.49294980928189319,\n",
       "  0.49283360262298459,\n",
       "  0.49300132211430708,\n",
       "  0.49293461355737339,\n",
       "  0.49267753204156373,\n",
       "  0.49240272151739622,\n",
       "  0.49227546370467395,\n",
       "  0.49216513064688677,\n",
       "  0.49206182305191115,\n",
       "  0.49215586980118664,\n",
       "  0.49215929804069064,\n",
       "  0.49179936011331427,\n",
       "  0.49195550315386638,\n",
       "  0.49174073068931196,\n",
       "  0.49195049933242202,\n",
       "  0.49132998457379967,\n",
       "  0.49148725604906762,\n",
       "  0.49127425539529002,\n",
       "  0.49097267133180367,\n",
       "  0.49083555603101908,\n",
       "  0.49072092210433194,\n",
       "  0.49078807985119993,\n",
       "  0.4905557012178533,\n",
       "  0.49033477899176969,\n",
       "  0.4902731716477623,\n",
       "  0.49007669036436469,\n",
       "  0.48995317732664712,\n",
       "  0.48981584592001154,\n",
       "  0.48970623635597871,\n",
       "  0.48965883666502286,\n",
       "  0.489413213921713,\n",
       "  0.48942998262783566,\n",
       "  0.48917077543204213,\n",
       "  0.48903685975050604,\n",
       "  0.48887043358290405,\n",
       "  0.4887652085714374,\n",
       "  0.48858089507756802,\n",
       "  0.48844267947772624,\n",
       "  0.48828358250661008,\n",
       "  0.48829074869889538,\n",
       "  0.48806658477328047,\n",
       "  0.48787330128533629,\n",
       "  0.48783139317726026,\n",
       "  0.48796928075027396,\n",
       "  0.48782759486851018,\n",
       "  0.48803855383717121,\n",
       "  0.48791562736410465,\n",
       "  0.48778242930983506,\n",
       "  0.48680678099488345,\n",
       "  0.48654966723826559,\n",
       "  0.48643185015905349,\n",
       "  0.4863094942054158,\n",
       "  0.48647803638321735,\n",
       "  0.48658927324186829,\n",
       "  0.48571862024443113,\n",
       "  0.48584395255397317,\n",
       "  0.48548763371718806,\n",
       "  0.48521625338734964,\n",
       "  0.48501929192022847,\n",
       "  0.48488082618044126,\n",
       "  0.48458308113158055,\n",
       "  0.4845439615234815,\n",
       "  0.48427117190192093,\n",
       "  0.48414710478793588,\n",
       "  0.48403144509201057,\n",
       "  0.48425093120252449,\n",
       "  0.48376660387010306,\n",
       "  0.48362205851436912,\n",
       "  0.48330786113513835,\n",
       "  0.48324582373316893,\n",
       "  0.4828972216450414,\n",
       "  0.48256022562539569,\n",
       "  0.48230633052194455,\n",
       "  0.4824655821760413,\n",
       "  0.48257953729263892,\n",
       "  0.48169640158419175,\n",
       "  0.48155626562203807,\n",
       "  0.48172469547442676,\n",
       "  0.48193029397897535,\n",
       "  0.48136345005686326,\n",
       "  0.48082628892867263,\n",
       "  0.48049470830974933,\n",
       "  0.48002845646734738,\n",
       "  0.47978463880867184,\n",
       "  0.47979659553352227,\n",
       "  0.47962530885081001,\n",
       "  0.47921800571315087,\n",
       "  0.47883393916939954,\n",
       "  0.47867390688190581,\n",
       "  0.47872173381881566,\n",
       "  0.47883260017443796,\n",
       "  0.47793236370446329,\n",
       "  0.47760440237430246,\n",
       "  0.4773243139453745,\n",
       "  0.47705305675856713,\n",
       "  0.4770391947151843,\n",
       "  0.4770952607967156,\n",
       "  0.47730879192772768,\n",
       "  0.47656772640481282,\n",
       "  0.47639138477250431,\n",
       "  0.47583378185469644,\n",
       "  0.47568002649084584,\n",
       "  0.47497744397154285,\n",
       "  0.47457342959757587,\n",
       "  0.47426064040217752,\n",
       "  0.47426952931775479,\n",
       "  0.47362537907519764,\n",
       "  0.47327357341938803,\n",
       "  0.47293525799165381,\n",
       "  0.4726131159194441,\n",
       "  0.4723303030395159,\n",
       "  0.47201407007920115,\n",
       "  0.47170469665483689,\n",
       "  0.47142361437929409,\n",
       "  0.47134005650176164,\n",
       "  0.47086791249513088,\n",
       "  0.47045194782814281,\n",
       "  0.47008381181015446,\n",
       "  0.46972011083613663,\n",
       "  0.46960432688262405,\n",
       "  0.46933950169852312,\n",
       "  0.46892023644460135,\n",
       "  0.46847062289267183,\n",
       "  0.46828915433559648,\n",
       "  0.46768494670404404,\n",
       "  0.46733657351777402,\n",
       "  0.46705084630119531,\n",
       "  0.46665247120705489,\n",
       "  0.46662445918670792,\n",
       "  0.46610988700107164,\n",
       "  0.46558261245490551,\n",
       "  0.46551415727560019,\n",
       "  0.4648992371553301,\n",
       "  0.46452709890772781,\n",
       "  0.46419385019889292,\n",
       "  0.46387785272640014,\n",
       "  0.46357242661083792,\n",
       "  0.46318981749775656,\n",
       "  0.46269840664101147,\n",
       "  0.46230098021604882,\n",
       "  0.46197870774007821,\n",
       "  0.4615055450231797,\n",
       "  0.4611103206125719,\n",
       "  0.46106016953151352,\n",
       "  0.46071274723684841,\n",
       "  0.46010320532227811,\n",
       "  0.45977187410410297,\n",
       "  0.45929532671023759,\n",
       "  0.45933450431301737,\n",
       "  0.45860231389690259,\n",
       "  0.45789785278760231,\n",
       "  0.45755860278833227,\n",
       "  0.45705230898359506,\n",
       "  0.45660125616001207,\n",
       "  0.4562087889841866,\n",
       "  0.45585710137772639,\n",
       "  0.45550974456753968,\n",
       "  0.45504081136856617,\n",
       "  0.45453599620264457,\n",
       "  0.45402703211572909,\n",
       "  0.45358769736035115,\n",
       "  0.45316761259695504,\n",
       "  0.45273595959029916,\n",
       "  0.45230934527991296,\n",
       "  0.45193680438654538,\n",
       "  0.4514141027008628,\n",
       "  0.4509711194210792,\n",
       "  0.45046749463634395,\n",
       "  0.45003802399890164,\n",
       "  0.44953864596378396,\n",
       "  0.44908679433165077,\n",
       "  0.44864464727382314,\n",
       "  0.44824690856732635,\n",
       "  0.4476779920205104,\n",
       "  0.44728068475262855,\n",
       "  0.44708210910705998,\n",
       "  0.4462083998555868,\n",
       "  0.44576130794700713,\n",
       "  0.44538824051905684,\n",
       "  0.44531770618279909,\n",
       "  0.44458565513026072,\n",
       "  0.44380089440635484,\n",
       "  0.44340203440457843,\n",
       "  0.44285357378601575,\n",
       "  0.44242471980552223,\n",
       "  0.44188782839097179,\n",
       "  0.44139522866962211,\n",
       "  0.44086362913582644,\n",
       "  0.44053755438317299,\n",
       "  0.43988379101708969,\n",
       "  0.43951638919677039,\n",
       "  0.43900948416471336,\n",
       "  0.4387601107137174,\n",
       "  0.43787266101672051,\n",
       "  0.43753484988062619,\n",
       "  0.43686800160308636,\n",
       "  0.43637937017422795,\n",
       "  0.43600351487502964,\n",
       "  0.43594583813910509,\n",
       "  0.43539322118543788,\n",
       "  0.43518054551211993,\n",
       "  0.43449268452731932,\n",
       "  0.43448731812740804,\n",
       "  0.4344543575562303,\n",
       "  0.43268687514550641,\n",
       "  0.43200233534759419,\n",
       "  0.43176523636618291,\n",
       "  0.43088447412449321,\n",
       "  0.43033169623699752,\n",
       "  0.42988188447549502,\n",
       "  0.42933123296061437,\n",
       "  0.42878872521698697,\n",
       "  0.42829140947875211,\n",
       "  0.42776484986285179,\n",
       "  0.42726326472166409,\n",
       "  0.42673066920085956,\n",
       "  0.42623127073646849,\n",
       "  0.42574989837605326,\n",
       "  0.42540615603691206,\n",
       "  0.42470135532988051,\n",
       "  0.42417692771001003,\n",
       "  0.42360682973284142,\n",
       "  0.42308386152306282,\n",
       "  0.42285980768745679,\n",
       "  0.42239682047750798,\n",
       "  0.42164704224611321,\n",
       "  0.42111357780122405,\n",
       "  0.42058122087606353,\n",
       "  0.42014394060435656,\n",
       "  0.42000665468420717,\n",
       "  0.41918543349194037,\n",
       "  0.41865123451179587,\n",
       "  0.41830153536087644,\n",
       "  0.41806384841853217,\n",
       "  0.41783326184698205,\n",
       "  0.41660715188890413,\n",
       "  0.41604140636895476,\n",
       "  0.41549572843579791,\n",
       "  0.41502191679390005,\n",
       "  0.41453767008480397,\n",
       "  0.41407957791438316,\n",
       "  0.41356974857862022,\n",
       "  0.41307009134518502,\n",
       "  0.4126809269568622,\n",
       "  0.41204081734097564,\n",
       "  0.41149989166282502,\n",
       "  0.41112481893380848,\n",
       "  0.41103050382584605,\n",
       "  0.41002805033015982,\n",
       "  0.40949740207221685,\n",
       "  0.40899232260239421,\n",
       "  0.4085252061130451,\n",
       "  0.40812304116063242,\n",
       "  0.40750353663374977,\n",
       "  0.40713004073219444,\n",
       "  0.40653604947669064,\n",
       "  0.40598652055663637,\n",
       "  0.40549155649471258,\n",
       "  0.40504081736366576,\n",
       "  0.40452798188457634,\n",
       "  0.40413323938142093,\n",
       "  0.40361072493745431,\n",
       "  0.40346519433230127,\n",
       "  0.40270467943650917,\n",
       "  0.40229935684470614,\n",
       "  0.40161996057598837,\n",
       "  0.40121314964865762,\n",
       "  0.40082775199684229,\n",
       "  0.40026467069085503,\n",
       "  0.39975790195349603,\n",
       "  0.3993540962529174,\n",
       "  0.39886202799916282,\n",
       "  0.39871832584258304,\n",
       "  0.39828893293186723,\n",
       "  0.39743006875478593,\n",
       "  0.3970001170748238,\n",
       "  0.39642509471912202,\n",
       "  0.39599876619246388,\n",
       "  0.39551702462784666,\n",
       "  0.39515456515400671,\n",
       "  0.3945854174578633,\n",
       "  0.39409461903878107,\n",
       "  0.39372986470339333,\n",
       "  0.39333570546999452,\n",
       "  0.3927379086376156,\n",
       "  0.39257729920983458,\n",
       "  0.39216130103207864,\n",
       "  0.39197810551052931,\n",
       "  0.39124395282028257,\n",
       "  0.3907814123261959,\n",
       "  0.39075732872887048,\n",
       "  0.39024510822633524,\n",
       "  0.39004403978421159,\n",
       "  0.38901272992777702,\n",
       "  0.38882994036368063,\n",
       "  0.38798965555178,\n",
       "  0.38751279472585709,\n",
       "  0.38688498456996279,\n",
       "  0.38647798264493077,\n",
       "  0.38622846198146904,\n",
       "  0.38570740243059409,\n",
       "  0.38536740170943812,\n",
       "  0.38497339294933486,\n",
       "  0.38451449086730027,\n",
       "  0.38402375673102851,\n",
       "  0.38349846775879892,\n",
       "  0.38316176644586775,\n",
       "  0.38307145228415834,\n",
       "  0.38291388507505508,\n",
       "  0.38247716448946573,\n",
       "  0.38204721795313662,\n",
       "  0.38110409460544342,\n",
       "  0.38054159787516501,\n",
       "  0.38024587657024123,\n",
       "  0.38006426008838656,\n",
       "  0.37941561389162914,\n",
       "  0.37895328161422015,\n",
       "  0.37849114842104237,\n",
       "  0.37809610696062368,\n",
       "  0.37774378654369722,\n",
       "  0.37729267802779654,\n",
       "  0.37690030127468555,\n",
       "  0.37647664270687098,\n",
       "  0.37611327684265289,\n",
       "  0.37569867755931791,\n",
       "  0.37531775851944621,\n",
       "  0.37494584555353971,\n",
       "  0.3745883933910012,\n",
       "  0.37413711880305417,\n",
       "  0.37388291898110759,\n",
       "  0.37337178585346126,\n",
       "  0.3729617806135071,\n",
       "  0.3725950745568346,\n",
       "  0.37239894529034828,\n",
       "  0.37236736774793611,\n",
       "  0.37190314370311106,\n",
       "  0.37118026429993095,\n",
       "  0.37089377650921052,\n",
       "  0.37086328174295058,\n",
       "  0.37052866296246373,\n",
       "  0.36985068332266768,\n",
       "  0.36916774744557018,\n",
       "  0.368857329350328,\n",
       "  0.36841575152439304,\n",
       "  0.36809911598761974,\n",
       "  0.36802370075492291,\n",
       "  0.36773644747278739,\n",
       "  0.36761354877263774,\n",
       "  0.36759110802339551,\n",
       "  0.36708158308338479,\n",
       "  0.36661395340185388,\n",
       "  0.36565388896742229,\n",
       "  0.36519381077542662,\n",
       "  0.36468752322689635,\n",
       "  0.36451155054524098,\n",
       "  0.3640425476501879,\n",
       "  0.36370404591901007,\n",
       "  0.3633569470005541,\n",
       "  0.36287338530962787,\n",
       "  0.36252876734959494,\n",
       "  0.36212499680950472,\n",
       "  0.36176989236163737,\n",
       "  0.36162820946718216,\n",
       "  0.3611733337006976,\n",
       "  0.36069112521038932,\n",
       "  0.36041048455673746,\n",
       "  0.36016000293855877,\n",
       "  0.35970989733066988,\n",
       "  0.35929061059286349,\n",
       "  0.35913857433881291,\n",
       "  0.35886293439689299,\n",
       "  0.35832751737382312,\n",
       "  0.35808431279934799,\n",
       "  0.35760160624950971,\n",
       "  0.35727261035803276,\n",
       "  0.35694237372558246,\n",
       "  0.35656343504040333,\n",
       "  0.35637361356362851,\n",
       "  0.35626490960836449,\n",
       "  0.35563398180142192,\n",
       "  0.35519557980447219,\n",
       "  0.35505381630489052,\n",
       "  0.35469104419596048,\n",
       "  0.35422724984206527,\n",
       "  0.35405769707095647,\n",
       "  0.3539397405940814,\n",
       "  0.35345273235264141,\n",
       "  0.3530902744860076,\n",
       "  0.35266396743001038,\n",
       "  0.35254334782403807,\n",
       "  0.35214458909045132,\n",
       "  0.35200319187316326,\n",
       "  0.35129648268640418,\n",
       "  0.35092689089447537,\n",
       "  0.35079344863117284,\n",
       "  0.35020037358139644,\n",
       "  0.34989960011949517,\n",
       "  0.34958700445526203,\n",
       "  0.34926153927034215,\n",
       "  0.34901323885325253,\n",
       "  0.34853035370563684,\n",
       "  0.34825260930278329,\n",
       "  0.34811444900520644,\n",
       "  0.34760714374108859,\n",
       "  0.34721152859381188,\n",
       "  0.34686358014055207,\n",
       "  0.34658709252700765,\n",
       "  0.34633998097690871,\n",
       "  0.34601824274894788,\n",
       "  0.34581853441391952,\n",
       "  0.34532201428710357,\n",
       "  0.34516025265301048,\n",
       "  0.34507883747576179,\n",
       "  0.34446413730682823,\n",
       "  0.34437593192392085,\n",
       "  0.34378015867309225,\n",
       "  0.34332210137391012,\n",
       "  0.34302701140151048,\n",
       "  0.34271676102713022,\n",
       "  0.34246110125412021,\n",
       "  0.34206180348811721,\n",
       "  0.34193644368337656,\n",
       "  0.34189740870564428,\n",
       "  0.34128445339992952,\n",
       "  0.34083813692455261,\n",
       "  0.34060213965085423,\n",
       "  0.3401985028392433,\n",
       "  0.33993148097188314,\n",
       "  0.33978158789855767,\n",
       "  0.33930362729982855,\n",
       "  0.33906564601307432,\n",
       "  0.33882017214036719,\n",
       "  0.33857215542977565,\n",
       "  0.3383990514884751,\n",
       "  0.33811813403844193,\n",
       "  0.33751440791770271,\n",
       "  0.33736323312579514,\n",
       "  0.33723490291479136,\n",
       "  0.33689905861039549,\n",
       "  0.33639380744049896,\n",
       "  0.33616297366676856,\n",
       "  0.33587301774765149,\n",
       "  0.33541079987704803,\n",
       "  0.33513379200758586,\n",
       "  0.33477737652432832,\n",
       "  0.33446767261696342,\n",
       "  0.33422534236428547,\n",
       "  0.33384520556596009,\n",
       "  0.33354307936543037,\n",
       "  0.33321423629577662,\n",
       "  0.33298473492569569,\n",
       "  0.33279630629293117,\n",
       "  0.33274427987037741,\n",
       "  0.33228853707969941,\n",
       "  0.33184593418496949,\n",
       "  0.33172355082322669,\n",
       "  0.3316618659689175,\n",
       "  0.33130837262332447,\n",
       "  0.33076546063818013,\n",
       "  0.3304196017078288,\n",
       "  0.32997260575995607,\n",
       "  0.32967489913886544,\n",
       "  0.32941960297557865,\n",
       "  0.32907249378948161,\n",
       "  0.32891644208275983,\n",
       "  0.32884832919504359,\n",
       "  0.32839039270145126,\n",
       "  0.32791122578324838,\n",
       "  0.32757563504560999,\n",
       "  0.32727637511059199,\n",
       "  0.32703351268917558,\n",
       "  0.32670577002171008,\n",
       "  0.32639626787264903,\n",
       "  0.32625118129915243,\n",
       "  0.32610625947490735,\n",
       "  0.32561294442372513,\n",
       "  0.3254620685784006,\n",
       "  0.32498076187110114,\n",
       "  0.32475824371861878,\n",
       "  0.32438612878999595,\n",
       "  0.32413176584344472,\n",
       "  0.32382187066993273,\n",
       "  0.32354583125598901,\n",
       "  0.32324095671235037,\n",
       "  0.32298576392812534,\n",
       "  0.3227430633363918,\n",
       "  0.32256526523694801,\n",
       "  0.32218456084103342,\n",
       "  0.32193308217363198,\n",
       "  0.32161383785373759,\n",
       "  0.32140304166013306,\n",
       "  0.32108197410169109,\n",
       "  0.32081963102873118,\n",
       "  0.3205318135428567,\n",
       "  0.32023155794445546,\n",
       "  0.32000573712060743,\n",
       "  0.31976694596137029,\n",
       "  0.319439370843912,\n",
       "  0.31923132934143172,\n",
       "  0.31902361237099353,\n",
       "  0.31878079414967997,\n",
       "  0.31861415454700648,\n",
       "  0.31814597294944713,\n",
       "  0.31798358920151709,\n",
       "  0.31747053686080084,\n",
       "  0.31718607470734045,\n",
       "  0.31700810117271055,\n",
       "  0.31662971676538954,\n",
       "  0.31637178653633335,\n",
       "  0.31610698894955669,\n",
       "  0.31588793594416215,\n",
       "  0.31566095769218089,\n",
       "  0.31532203586367485,\n",
       "  0.31504143246565519,\n",
       "  0.31487631331977517,\n",
       "  0.31456903466895242,\n",
       "  0.31440762675021622,\n",
       "  0.31407769141482933,\n",
       "  0.31376467538371533,\n",
       "  0.31341514555605621,\n",
       "  0.31318871271517562,\n",
       "  0.31288483916486315,\n",
       "  0.31258640984977965,\n",
       "  0.31239533117660684,\n",
       "  0.31210066315803514,\n",
       "  0.31179496282074148,\n",
       "  0.31163917038931682,\n",
       "  0.31143232108733376,\n",
       "  0.31138381194377796,\n",
       "  0.31081717336097719,\n",
       "  0.31048670290012387,\n",
       "  0.31019527996460422,\n",
       "  0.30991540647812388,\n",
       "  0.30967414665839349,\n",
       "  0.3093574931762178,\n",
       "  0.30911604965608563,\n",
       "  0.3089018438421729,\n",
       "  0.30860118311013307,\n",
       "  0.30835065285421165,\n",
       "  0.30809359858686169,\n",
       "  0.30778046233377493,\n",
       "  0.30755148317746805,\n",
       "  0.30739381700916746,\n",
       "  0.30734196148294868,\n",
       "  0.30677205707123173,\n",
       "  0.30672847743548592,\n",
       "  0.30624600040176642,\n",
       "  0.30617430669821466,\n",
       "  0.30594951709383778,\n",
       "  0.30551041479044372,\n",
       "  0.30527781993308573,\n",
       "  0.30511494917236925,\n",
       "  0.30462401542021078,\n",
       "  0.30446697310427828,\n",
       "  0.30441005493361561,\n",
       "  0.30389737465524347,\n",
       "  0.30376149552725723,\n",
       "  0.30327691901266141,\n",
       "  0.30300204116421159,\n",
       "  0.30274019203686831,\n",
       "  0.30249788578096309,\n",
       "  0.30234775007775805,\n",
       "  0.3019890483963657,\n",
       "  0.30167858028117095,\n",
       "  0.30142912540833383,\n",
       "  0.3011482752303753,\n",
       "  0.30100279185347756,\n",
       "  0.30056814175326535,\n",
       "  0.30032755613466816,\n",
       "  0.30006053409551448,\n",
       "  0.29978865342937533,\n",
       "  0.29950203367076789,\n",
       "  0.29929345371441241,\n",
       "  0.29895122500655547,\n",
       "  0.29868181190953735,\n",
       "  0.29848486875877456,\n",
       "  0.29811521782979217,\n",
       "  0.29786439214389882,\n",
       "  0.29756535336723122,\n",
       "  0.29728800775878478,\n",
       "  0.29698332472754307,\n",
       "  0.29670578211784304,\n",
       "  0.29653452532769442,\n",
       "  0.29633066356023741,\n",
       "  0.29585959515777482,\n",
       "  0.29560366623112494,\n",
       "  0.29526454375773309,\n",
       "  0.29492845301746784,\n",
       "  0.29467253625540119,\n",
       "  0.29432474729731373,\n",
       "  0.29401234423272549,\n",
       "  0.2937045749997414,\n",
       "  0.29351815235986156,\n",
       "  0.29335115537485174,\n",
       "  0.29328466592606578,\n",
       "  0.29299492453489739,\n",
       "  0.29269706753137575,\n",
       "  0.29256430183920079,\n",
       "  0.29188431397819903,\n",
       "  0.29118981254207843,\n",
       "  0.29079847336886006,\n",
       "  0.29041704395049245,\n",
       "  0.2900577445630193,\n",
       "  0.28977601730776953,\n",
       "  0.28951636205818082,\n",
       "  0.28897692319079815,\n",
       "  0.28864799333784252,\n",
       "  0.28839008162034152,\n",
       "  0.28799753792956184,\n",
       "  0.28778861149113844,\n",
       "  0.28716978748126604,\n",
       "  0.28675910737749688,\n",
       "  0.28634129935420405,\n",
       "  0.2859344558726577,\n",
       "  0.28561192094117682,\n",
       "  0.28513331909490708,\n",
       "  0.28474437886681064,\n",
       "  0.28432024838076286,\n",
       "  0.28394242352874877,\n",
       "  0.28343765352696276,\n",
       "  0.28303158437517884,\n",
       "  0.28278492574147712,\n",
       "  0.28247004965349076,\n",
       "  0.28196755992860967,\n",
       "  0.28118252942531097,\n",
       "  0.2806399143105594,\n",
       "  0.28014116192985494,\n",
       "  0.27964229617630143,\n",
       "  0.27912881246892668,\n",
       "  0.27867502943834599,\n",
       "  0.27807564084130287,\n",
       "  0.27769120650457457,\n",
       "  0.27724940669521336,\n",
       "  0.27649032852324507,\n",
       "  0.27580044145855737,\n",
       "  0.27524918920227726,\n",
       "  0.27479363690439673,\n",
       "  0.27397730770818185,\n",
       "  0.27334878843796567,\n",
       "  0.27272755499738888,\n",
       "  0.27210974129542592,\n",
       "  0.27163539706841916,\n",
       "  0.27094491799814396,\n",
       "  0.26995546303659751,\n",
       "  0.26940214749396568,\n",
       "  0.26868303141225475,\n",
       "  0.26778628410246536,\n",
       "  0.26722117204456058,\n",
       "  0.26652037714690724,\n",
       "  0.26531481285890379,\n",
       "  0.2644282608776487,\n",
       "  0.26358591647420787,\n",
       "  0.26270474729418664,\n",
       "  0.26178614154788132,\n",
       "  0.26095194344701883,\n",
       "  0.25992294133884147,\n",
       "  0.25899730729759907,\n",
       "  0.25795137903649595,\n",
       "  0.25700844029960423,\n",
       "  0.25599219745938506,\n",
       "  0.25507874210521797,\n",
       "  0.2537849363795121,\n",
       "  0.25268131248336401,\n",
       "  0.25168487659520167,\n",
       "  0.25054934069332185,\n",
       "  0.24944391614372916,\n",
       "  0.24799796394866488,\n",
       "  0.24674929891983496,\n",
       "  0.24567213836820487,\n",
       "  0.24424143854403974,\n",
       "  0.2429547623996256,\n",
       "  0.24174781642858106,\n",
       "  0.24059748398892467,\n",
       "  0.23886961423910794,\n",
       "  0.23751185089304869,\n",
       "  0.23627455885038526,\n",
       "  0.23460469621972255,\n",
       "  0.23323380690800921,\n",
       "  0.23173028202949827,\n",
       "  0.23016780563578348,\n",
       "  0.22859263200841395,\n",
       "  0.2272779338952691,\n",
       "  0.2259146740460341,\n",
       "  0.22443593885478827,\n",
       "  0.22257697015192884,\n",
       "  0.22057129931298111,\n",
       "  0.2189643941760952,\n",
       "  0.21736253018222371,\n",
       "  0.21569251812568827,\n",
       "  0.21411485350822368,\n",
       "  0.21257491487210103,\n",
       "  0.21088509333122671,\n",
       "  0.20917665204349464,\n",
       "  0.20781017722301959,\n",
       "  0.20614960323139508,\n",
       "  0.20434954616549195,\n",
       "  0.20272288664008631,\n",
       "  0.20127025085237207,\n",
       "  0.19944354622094343,\n",
       "  0.19804219879277649,\n",
       "  0.19632368020796589,\n",
       "  0.19492854915717886,\n",
       "  0.19313677577123037,\n",
       "  0.19164810678844618,\n",
       "  0.1900197835231697,\n",
       "  0.18870491395028982,\n",
       "  0.18744273524198285,\n",
       "  0.18539233014198656,\n",
       "  0.18406465221019586,\n",
       "  0.18279025510745092,\n",
       "  0.18146578073816091,\n",
       "  0.17914136972644049,\n",
       "  0.17766772088410612,\n",
       "  0.17632016164121656,\n",
       "  0.17477717276575833,\n",
       "  0.17333876742896767,\n",
       "  0.17213442222079539,\n",
       "  0.17059845889508429,\n",
       "  0.1695001973108011,\n",
       "  0.16766531187993736,\n",
       "  0.16634239024188838,\n",
       "  0.16490172532958475,\n",
       "  0.16380036122309655,\n",
       "  0.16213996932140659,\n",
       "  0.16095350279271864,\n",
       "  0.15979636775584558,\n",
       "  0.15825310380434501,\n",
       "  0.15716135087375935,\n",
       "  0.1560463922810045,\n",
       "  0.15492147804707804,\n",
       "  0.15285510676491321,\n",
       "  0.15182209638501992,\n",
       "  0.15031130195278983,\n",
       "  0.14898973222358886,\n",
       "  0.14768054413463949,\n",
       "  0.1464732837746808,\n",
       "  0.14550132199476176,\n",
       "  0.14407093042615896,\n",
       "  0.14273796289836485,\n",
       "  0.14164716875512176,\n",
       "  0.14036037573682239,\n",
       "  0.13938788963117787,\n",
       "  0.13795476096123394,\n",
       "  0.13679414141907953,\n",
       "  0.13564516039515276,\n",
       "  0.13467197404051792,\n",
       "  0.13378147966773427,\n",
       "  0.13226047539656691,\n",
       "  0.13129433714611999,\n",
       "  0.13007201502091684,\n",
       "  0.12894607677227382,\n",
       "  0.12784980643958196,\n",
       "  0.1267552942618019,\n",
       "  0.12573560885934845,\n",
       "  0.12462627538874851,\n",
       "  0.12365265088069795,\n",
       "  0.12257248235165739,\n",
       "  0.12154471585161596,\n",
       "  0.12055426048087872,\n",
       "  0.11970429438910851,\n",
       "  0.11849512275241164,\n",
       "  0.11750950855132798,\n",
       "  0.11666398465493844,\n",
       "  0.11559955553862857,\n",
       "  0.11460835267711336,\n",
       "  0.11374995725123274,\n",
       "  0.1129060038664373,\n",
       "  0.11205550291695415,\n",
       "  0.11085346819743599,\n",
       "  0.10999270511103137,\n",
       "  0.10902849672109691,\n",
       "  0.1081837778368077,\n",
       "  0.10725524740090646,\n",
       "  0.10636961274677184,\n",
       "  0.10558205924729713,\n",
       "  0.10469188734799949,\n",
       "  0.10398672267781736,\n",
       "  0.10328605527127167,\n",
       "  0.10255489710055662,\n",
       "  0.10172855442244556,\n",
       "  0.10066692871009289,\n",
       "  0.099655249618735481,\n",
       "  0.098841159999046468,\n",
       "  0.098045845848734159,\n",
       "  0.097255895324013089,\n",
       "  0.09655459730659624,\n",
       "  0.095748814681176145,\n",
       "  0.094948971399637816,\n",
       "  0.094239648481266258,\n",
       "  0.093474307355284378,\n",
       "  0.092735374913834989,\n",
       "  0.091986676525833116,\n",
       "  0.091296772155176356,\n",
       "  0.090559553671776663,\n",
       "  0.089850742536705053,\n",
       "  0.089190709540136637,\n",
       "  0.088462719726690578,\n",
       "  0.087833778348357858,\n",
       "  0.087133088662040406,\n",
       "  0.086436695444436246,\n",
       "  0.085822116964088216,\n",
       "  0.085239751414903184,\n",
       "  0.084532911336240316,\n",
       "  0.083979451189498816,\n",
       "  0.083272380795811604,\n",
       "  0.082620415896093827,\n",
       "  0.08195104248667659,\n",
       "  0.081336095297454336,\n",
       "  0.08076612504593203,\n",
       "  0.080130649229434858,\n",
       "  0.079554573351796415,\n",
       "  0.079007217036304339,\n",
       "  0.078371163606294134,\n",
       "  0.077795195782746546,\n",
       "  0.077230766825709674,\n",
       "  0.076686641038087353,\n",
       "  0.076109519425228575,\n",
       "  0.075571395360335261,\n",
       "  0.075023888020530782,\n",
       "  0.07446956428661812,\n",
       "  0.073934967121074016,\n",
       "  0.073425891907615004,\n",
       "  0.07288885325854419,\n",
       "  0.072377356638603188,\n",
       "  0.071870047339810755,\n",
       "  0.071396326061954216,\n",
       "  0.070856791653442747,\n",
       "  0.070390510223533195,\n",
       "  0.069945286661135281,\n",
       "  0.069437034875572887,\n",
       "  0.068988438926168502,\n",
       "  0.068539425608237531,\n",
       "  0.068089042509030293,\n",
       "  0.067524155297734229,\n",
       "  0.067037804906284401,\n",
       "  0.066582405021048077,\n",
       "  0.06614069100009512,\n",
       "  0.065693434395269687,\n",
       "  0.065247140375926327,\n",
       "  0.064809744066878103,\n",
       "  0.064381775302911337,\n",
       "  0.063969325278826178,\n",
       "  0.063529095183763457,\n",
       "  0.063113734977799027,\n",
       "  0.062696624030660897,\n",
       "  0.062294008145412132,\n",
       "  0.061879566886739311,\n",
       "  0.061489160610268198,\n",
       "  0.06108130294494217,\n",
       "  0.06068684314247165,\n",
       "  0.060300658487833843,\n",
       "  0.059917115379377398,\n",
       "  0.059530184625783106,\n",
       "  0.059151313214669453,\n",
       "  0.058776754273210337,\n",
       "  0.058409380686611917,\n",
       "  ...],\n",
       " 20000)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos compor nosso dataset X justapondo horizontalmente x1 e x2 declarados anteriormente\n",
    "X = np.array([[1,1],[1,0],[0,1],[0,0]])\n",
    "# Temos também de declarar os targets\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "l = list(zip(X,y))\n",
    "\n",
    "print(l)\n",
    "\n",
    "a = np.atleast_2d(l[1][0]).T\n",
    "print(a)\n",
    "\n",
    "# Agora, instanciamos e treinamos nossa rede neural!\n",
    "xorNetwork = MultilayerNeuralNetwork(arch=[2,2,1], alpha=0.5)\n",
    "xorNetwork.fit(X, y, batch_size=1, epochs = 20000, displayUpdate=100, max_loss=0.0000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "O MNIST é um *dataset* massivamente utilizado pelos estudiosos de Machine Learning composto\n",
    "por imagens de dígitos de 0 a 9. Ele possui 60000 exemplos de treino e 10000 de testes. \n",
    "Os vetores de características são 784-dimensionais ($28 \\times 28$ pixels por imagem),\n",
    "com componentes assumindo valores em $[0,255]$. O propósito é corretamente classificar\n",
    "os dígitos desse *dataset*!\n",
    "\n",
    "A biblioteca `sklearn` oferece o uma amostra do MNIST por meio de comandos simples. \n",
    "Vamos utilizá-los para obter esse *dataset* e, com ele,\n",
    "treinar uma rede neural especializada em classificar\n",
    "seus dígitos de 0 a 9!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST dataset\n",
      "[INFO] samples: 1797, dim: 64\n",
      "[INFO] training network...\n",
      "Epoch: 1, Loss: 2256.0400703\n",
      "Epoch: 101, Loss: 6.1838777\n",
      "Epoch: 201, Loss: 2.0033121\n",
      "Epoch: 301, Loss: 1.1087339\n",
      "Epoch: 401, Loss: 0.8904364\n",
      "Epoch: 501, Loss: 0.7846794\n",
      "Epoch: 601, Loss: 0.7226654\n",
      "Epoch: 701, Loss: 0.6820705\n",
      "Epoch: 801, Loss: 0.6534447\n",
      "Epoch: 901, Loss: 0.6321970\n",
      "Stopped by epochs criterion.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2256.0400703338246,\n",
       "  603.91114806666315,\n",
       "  596.92706894581181,\n",
       "  585.45131531260881,\n",
       "  550.37097014007634,\n",
       "  480.20557874903596,\n",
       "  394.0418547907932,\n",
       "  324.60476593080347,\n",
       "  267.67906185362335,\n",
       "  217.05980561431932,\n",
       "  185.1058613057989,\n",
       "  159.91741937102671,\n",
       "  133.09267947293603,\n",
       "  119.22974969194887,\n",
       "  105.58554272054334,\n",
       "  95.346918022257199,\n",
       "  86.84640811535985,\n",
       "  78.921274158667615,\n",
       "  75.444760610205151,\n",
       "  67.322842439626072,\n",
       "  62.780111273460022,\n",
       "  62.201186029180128,\n",
       "  58.74542819301125,\n",
       "  57.426566944566602,\n",
       "  52.3874551506741,\n",
       "  50.204972758047511,\n",
       "  46.766176158956384,\n",
       "  46.057969073355125,\n",
       "  41.780707382903032,\n",
       "  41.939011101340348,\n",
       "  39.311937996924399,\n",
       "  38.269085903809589,\n",
       "  36.816868568442885,\n",
       "  36.583066598305024,\n",
       "  33.211444056748192,\n",
       "  31.572438194842434,\n",
       "  30.505649417386508,\n",
       "  29.751479726524025,\n",
       "  28.467793009824597,\n",
       "  27.64613351996362,\n",
       "  32.073504433895621,\n",
       "  25.841595002692738,\n",
       "  27.090915071719447,\n",
       "  24.357416222028039,\n",
       "  24.509854904772567,\n",
       "  23.318374116547965,\n",
       "  24.127823610991889,\n",
       "  22.526431969636974,\n",
       "  20.840552924143921,\n",
       "  21.192838860071603,\n",
       "  20.031660222973144,\n",
       "  20.398609156252476,\n",
       "  19.040972215766182,\n",
       "  18.849251778038678,\n",
       "  18.880589381571507,\n",
       "  18.238265952246451,\n",
       "  17.615528893299153,\n",
       "  17.920525954686319,\n",
       "  16.988934829078271,\n",
       "  16.624593805340645,\n",
       "  16.670846125458198,\n",
       "  15.564286498012491,\n",
       "  15.385002051817006,\n",
       "  15.0374006985653,\n",
       "  14.464378770803506,\n",
       "  14.313396214667588,\n",
       "  14.053408431008934,\n",
       "  12.841277919830425,\n",
       "  12.459038624808002,\n",
       "  13.262668699462312,\n",
       "  11.952357894322759,\n",
       "  12.501686284610621,\n",
       "  11.844792416737342,\n",
       "  12.123306303150407,\n",
       "  11.140565305312979,\n",
       "  10.906427021883459,\n",
       "  9.939304164587794,\n",
       "  10.273699048416344,\n",
       "  10.07686784058809,\n",
       "  9.3492865499906941,\n",
       "  9.3262396711905744,\n",
       "  8.8186673378506875,\n",
       "  8.7976699623113053,\n",
       "  8.5666108652557842,\n",
       "  8.44108300379966,\n",
       "  8.3148464049731032,\n",
       "  7.8875495104888493,\n",
       "  8.0017071460444456,\n",
       "  7.4805619607880658,\n",
       "  7.4152082966082373,\n",
       "  7.2806438347176261,\n",
       "  7.1088358565471212,\n",
       "  7.090599770531103,\n",
       "  7.0009255990307313,\n",
       "  6.8942989440668097,\n",
       "  6.772967913313976,\n",
       "  6.5561352556904193,\n",
       "  6.6563647733889235,\n",
       "  6.3004480231701532,\n",
       "  6.2289407323536592,\n",
       "  6.1838776839535274,\n",
       "  6.1614667154379177,\n",
       "  6.0959480972255324,\n",
       "  5.8912566789681806,\n",
       "  5.8110598607210502,\n",
       "  5.8047718991337369,\n",
       "  5.74607757048443,\n",
       "  5.6737571131548039,\n",
       "  5.6387900253159611,\n",
       "  5.5606425941425677,\n",
       "  5.4683895974210355,\n",
       "  5.4280704894010556,\n",
       "  5.1786602353155864,\n",
       "  5.1396487305946632,\n",
       "  5.0799350570271971,\n",
       "  5.028505669945126,\n",
       "  4.9567219764253183,\n",
       "  4.9549931924375192,\n",
       "  4.9066103898644577,\n",
       "  4.9257662518611234,\n",
       "  4.861749341060305,\n",
       "  4.6848286640731054,\n",
       "  4.5903770486366851,\n",
       "  4.7650720388286345,\n",
       "  4.6052630100182945,\n",
       "  4.4622739403593794,\n",
       "  4.5517781797638897,\n",
       "  4.33566266623997,\n",
       "  4.4136681989423074,\n",
       "  4.5274427263054342,\n",
       "  4.440675373136715,\n",
       "  4.2697907804260149,\n",
       "  4.2482565168007813,\n",
       "  4.1367731021744891,\n",
       "  4.3007404955426436,\n",
       "  4.0579904234426447,\n",
       "  4.0730473925829962,\n",
       "  4.0124359744530942,\n",
       "  3.9283532907585164,\n",
       "  3.8976776772506518,\n",
       "  3.8434791784882298,\n",
       "  3.8980598197024543,\n",
       "  3.8033628946987039,\n",
       "  3.8380518224891715,\n",
       "  3.7338713576704445,\n",
       "  3.6706846023317312,\n",
       "  3.631792614786387,\n",
       "  3.605217349079874,\n",
       "  3.4775411939103291,\n",
       "  3.3692679376891403,\n",
       "  3.2824794095248735,\n",
       "  3.1666596894266754,\n",
       "  3.1158487607110246,\n",
       "  3.0981268109228846,\n",
       "  2.9610358903494398,\n",
       "  2.9284000490532414,\n",
       "  2.8197591306375367,\n",
       "  2.9573632701777783,\n",
       "  2.7525610054445147,\n",
       "  2.8280398459191778,\n",
       "  2.717062850042641,\n",
       "  2.691487445641588,\n",
       "  2.6357279300069161,\n",
       "  2.6333382587359497,\n",
       "  2.6005461191235781,\n",
       "  2.6149188387411089,\n",
       "  2.5568419087157608,\n",
       "  2.5116807038740312,\n",
       "  2.4962621596096808,\n",
       "  2.5068921921340852,\n",
       "  2.4887692212026709,\n",
       "  2.530765283906184,\n",
       "  2.4128192110281477,\n",
       "  2.4168679095253052,\n",
       "  2.3993682654194899,\n",
       "  2.3756678185183073,\n",
       "  2.3983926666814011,\n",
       "  2.3585286629797872,\n",
       "  2.3233363723305573,\n",
       "  2.2994784288928356,\n",
       "  2.2812437410474171,\n",
       "  2.273146902088762,\n",
       "  2.2517384200831376,\n",
       "  2.249904972675238,\n",
       "  2.2279096291117582,\n",
       "  2.2251583361401543,\n",
       "  2.2168207677438918,\n",
       "  2.1964885416924131,\n",
       "  2.1720670924233167,\n",
       "  2.1788890522866122,\n",
       "  2.1381203016946535,\n",
       "  2.1292666573113133,\n",
       "  2.1541296804695391,\n",
       "  2.1003486191602607,\n",
       "  2.0897676646341505,\n",
       "  2.0822152954096413,\n",
       "  2.0601711463374808,\n",
       "  2.0379086368254495,\n",
       "  2.0258664340642292,\n",
       "  2.0301767156743851,\n",
       "  2.0033121341333402,\n",
       "  1.9681433358990619,\n",
       "  1.9593132654781278,\n",
       "  1.9326331105823686,\n",
       "  1.91838700888074,\n",
       "  1.8906002063086818,\n",
       "  1.8744955730665576,\n",
       "  1.8574394930331599,\n",
       "  1.8233596805346006,\n",
       "  1.805863823252682,\n",
       "  1.7822533335828719,\n",
       "  1.7529869851830686,\n",
       "  1.7344793244486194,\n",
       "  1.7156190575681887,\n",
       "  1.7345119726207296,\n",
       "  1.6849137368016536,\n",
       "  1.6653444824529897,\n",
       "  1.647388256949279,\n",
       "  1.6430483658959856,\n",
       "  1.6217984524650864,\n",
       "  1.6145224154851086,\n",
       "  1.6106474033669278,\n",
       "  1.5874325955570616,\n",
       "  1.5810737862294983,\n",
       "  1.569338978490699,\n",
       "  1.5487005762596802,\n",
       "  1.5383447152688994,\n",
       "  1.5312593390433953,\n",
       "  1.5182236032698182,\n",
       "  1.5130682103725794,\n",
       "  1.4949678479568429,\n",
       "  1.4953696151529008,\n",
       "  1.4783335864705047,\n",
       "  1.4811958811823644,\n",
       "  1.4604384037003375,\n",
       "  1.454266190690487,\n",
       "  1.4409423806929818,\n",
       "  1.4378497698460131,\n",
       "  1.4244899363853254,\n",
       "  1.4217168833152574,\n",
       "  1.4107319709065056,\n",
       "  1.4066769048213512,\n",
       "  1.396242834303679,\n",
       "  1.3884767003127392,\n",
       "  1.38140928847499,\n",
       "  1.375791972080304,\n",
       "  1.3668471476853965,\n",
       "  1.3654405673606771,\n",
       "  1.3625466517201721,\n",
       "  1.34620217432564,\n",
       "  1.3393318743425011,\n",
       "  1.3345156543576702,\n",
       "  1.3286679063231293,\n",
       "  1.321047303752767,\n",
       "  1.3223188690022845,\n",
       "  1.3175813062307944,\n",
       "  1.301723960204056,\n",
       "  1.3001238249520781,\n",
       "  1.2919925847605715,\n",
       "  1.2878743233010377,\n",
       "  1.2800181123507803,\n",
       "  1.2741412390461315,\n",
       "  1.2702256645780459,\n",
       "  1.2646599262027123,\n",
       "  1.2610484591504569,\n",
       "  1.254604232773525,\n",
       "  1.2476327294511473,\n",
       "  1.2447401604964035,\n",
       "  1.2414862492441538,\n",
       "  1.2355510799756737,\n",
       "  1.2291355675249369,\n",
       "  1.2294504235017945,\n",
       "  1.2200261262462933,\n",
       "  1.2142890969495972,\n",
       "  1.2103493200441457,\n",
       "  1.2069893293433649,\n",
       "  1.2013957159999489,\n",
       "  1.1963826519171437,\n",
       "  1.1947186596654054,\n",
       "  1.1891722099862765,\n",
       "  1.1833730024422124,\n",
       "  1.1798336928906323,\n",
       "  1.1755763061023208,\n",
       "  1.1709539560696154,\n",
       "  1.1683494005976223,\n",
       "  1.1643036968158564,\n",
       "  1.1598336656192667,\n",
       "  1.1554254688938514,\n",
       "  1.150832078570875,\n",
       "  1.1478887200616981,\n",
       "  1.1440616554906262,\n",
       "  1.1409329485352093,\n",
       "  1.1360973833145396,\n",
       "  1.1322229217683981,\n",
       "  1.1311645869986999,\n",
       "  1.1259005459224656,\n",
       "  1.1225633298509452,\n",
       "  1.1191303217846758,\n",
       "  1.1148258219846301,\n",
       "  1.112630047191574,\n",
       "  1.1087338862572742,\n",
       "  1.1049077109407528,\n",
       "  1.1028340269387495,\n",
       "  1.0992301308070143,\n",
       "  1.09667595663321,\n",
       "  1.092135208463386,\n",
       "  1.0887189457811015,\n",
       "  1.0879693718341716,\n",
       "  1.0825224959286457,\n",
       "  1.0794536135523067,\n",
       "  1.0774291963140674,\n",
       "  1.0754272405504763,\n",
       "  1.0713308271295521,\n",
       "  1.0682567623865551,\n",
       "  1.0647312217276648,\n",
       "  1.062397992246173,\n",
       "  1.0591586221553879,\n",
       "  1.0560097096914842,\n",
       "  1.0539435177444954,\n",
       "  1.0511971603156014,\n",
       "  1.0480352963588069,\n",
       "  1.0448086455577843,\n",
       "  1.0421124805231921,\n",
       "  1.0401071338905006,\n",
       "  1.0370602788924621,\n",
       "  1.0351563495442695,\n",
       "  1.0319924609876048,\n",
       "  1.0293873840329102,\n",
       "  1.0272713198490615,\n",
       "  1.0242812365824543,\n",
       "  1.0223695666607981,\n",
       "  1.020145086047153,\n",
       "  1.0175700159184018,\n",
       "  1.0149115691215989,\n",
       "  1.0120363364384841,\n",
       "  1.0094069825439065,\n",
       "  1.0073869396131436,\n",
       "  1.0056476690621303,\n",
       "  1.0038440190181912,\n",
       "  1.0004982689692901,\n",
       "  0.9980491074867246,\n",
       "  0.99598400602514126,\n",
       "  0.99352021570068161,\n",
       "  0.99156513256007661,\n",
       "  0.98927961252769225,\n",
       "  0.98680135646917355,\n",
       "  0.98489776261716599,\n",
       "  0.98267878801194541,\n",
       "  0.98073461991459421,\n",
       "  0.97856416206870289,\n",
       "  0.97630613610298944,\n",
       "  0.97419185833539546,\n",
       "  0.97190314062922956,\n",
       "  0.97053979814528646,\n",
       "  0.96849692655198127,\n",
       "  0.96668669660294038,\n",
       "  0.96395686872074637,\n",
       "  0.96209887063221677,\n",
       "  0.96021917136561752,\n",
       "  0.95828190572436545,\n",
       "  0.95610161120640957,\n",
       "  0.95423711426046953,\n",
       "  0.95305363881929495,\n",
       "  0.95143355766776883,\n",
       "  0.94874403897644655,\n",
       "  0.94771707102431113,\n",
       "  0.94526715254044791,\n",
       "  0.94315039930509492,\n",
       "  0.94127617217541049,\n",
       "  0.9396499624009963,\n",
       "  0.93803150266562807,\n",
       "  0.93600156788460942,\n",
       "  0.93430203571024562,\n",
       "  0.93338031658947562,\n",
       "  0.93066526090359236,\n",
       "  0.92914652264246045,\n",
       "  0.9273985665116562,\n",
       "  0.92592878979068272,\n",
       "  0.92427081988795501,\n",
       "  0.92236897037216981,\n",
       "  0.92071121861241889,\n",
       "  0.91902690481474825,\n",
       "  0.91788315518354069,\n",
       "  0.91600094608085825,\n",
       "  0.91458440432256949,\n",
       "  0.91291675099172942,\n",
       "  0.91147019853827105,\n",
       "  0.9096064679615623,\n",
       "  0.90826342804696814,\n",
       "  0.9065855165995329,\n",
       "  0.90492919580328912,\n",
       "  0.90338536146574089,\n",
       "  0.90189129123325507,\n",
       "  0.90033945174439611,\n",
       "  0.89886748811372785,\n",
       "  0.89766755624026373,\n",
       "  0.89599771290575037,\n",
       "  0.89464375633009618,\n",
       "  0.89320799274904716,\n",
       "  0.89153949008233258,\n",
       "  0.89043642506178267,\n",
       "  0.88876173623359389,\n",
       "  0.88730459507294024,\n",
       "  0.88588598799217833,\n",
       "  0.88460573670988873,\n",
       "  0.88334098554104301,\n",
       "  0.88177753195815112,\n",
       "  0.88058890903055198,\n",
       "  0.8792594255072953,\n",
       "  0.87822031638616249,\n",
       "  0.87664618274792827,\n",
       "  0.87542616507302606,\n",
       "  0.87381746114194703,\n",
       "  0.87254979415251543,\n",
       "  0.8711610582449193,\n",
       "  0.87017424307670088,\n",
       "  0.86857354069508985,\n",
       "  0.86743133705507081,\n",
       "  0.86598195706693004,\n",
       "  0.86502640998260871,\n",
       "  0.86357852659967771,\n",
       "  0.86225660043536845,\n",
       "  0.86123719805763077,\n",
       "  0.85989890733158725,\n",
       "  0.8587049971814722,\n",
       "  0.85779828111112011,\n",
       "  0.85629368420646501,\n",
       "  0.85540634655850845,\n",
       "  0.85380529796271998,\n",
       "  0.85263327612104489,\n",
       "  0.85159558065686092,\n",
       "  0.85038373751323548,\n",
       "  0.84925036927978248,\n",
       "  0.84838765826516194,\n",
       "  0.84697312157081261,\n",
       "  0.84577304212869175,\n",
       "  0.84470291153617294,\n",
       "  0.84368424795578933,\n",
       "  0.84237552850727404,\n",
       "  0.84129001638071621,\n",
       "  0.84026415032751389,\n",
       "  0.83907959642783125,\n",
       "  0.83796543665011547,\n",
       "  0.8368852047607741,\n",
       "  0.83610989318957496,\n",
       "  0.83513671169253678,\n",
       "  0.83375734255878997,\n",
       "  0.83271994802816962,\n",
       "  0.83173021845656858,\n",
       "  0.83071213158651447,\n",
       "  0.82956442307081213,\n",
       "  0.82845747547669057,\n",
       "  0.82744479631194923,\n",
       "  0.82644384624921918,\n",
       "  0.82539120886727946,\n",
       "  0.82444080079323068,\n",
       "  0.8235477209027422,\n",
       "  0.82248747034638703,\n",
       "  0.82148994708226963,\n",
       "  0.82054213329657699,\n",
       "  0.81956669406981097,\n",
       "  0.8186489556073635,\n",
       "  0.81763275508734101,\n",
       "  0.81664253407567733,\n",
       "  0.81563135380032104,\n",
       "  0.81472184346052345,\n",
       "  0.81374071452989938,\n",
       "  0.8128263010019845,\n",
       "  0.81195123322623197,\n",
       "  0.8109757134847404,\n",
       "  0.81004156834316809,\n",
       "  0.80930619770133982,\n",
       "  0.80828232373535092,\n",
       "  0.80737568538101689,\n",
       "  0.80651761163710267,\n",
       "  0.80549590128040305,\n",
       "  0.80459197884496381,\n",
       "  0.80391159471322782,\n",
       "  0.80287206654241905,\n",
       "  0.80195957289209807,\n",
       "  0.80121417428594732,\n",
       "  0.80036846490290359,\n",
       "  0.79949658102254517,\n",
       "  0.79851214734970177,\n",
       "  0.7977105823470455,\n",
       "  0.79686662092865657,\n",
       "  0.79603319939564909,\n",
       "  0.79521768784433622,\n",
       "  0.79432684913821161,\n",
       "  0.79347714674249015,\n",
       "  0.79264391656243682,\n",
       "  0.79186210131885915,\n",
       "  0.79124413616217293,\n",
       "  0.7902734024506155,\n",
       "  0.78940976600093204,\n",
       "  0.78856359410718513,\n",
       "  0.78780426902928169,\n",
       "  0.78715919361774334,\n",
       "  0.78633048260215366,\n",
       "  0.78546013784528901,\n",
       "  0.78467942634628107,\n",
       "  0.78383974903414688,\n",
       "  0.78304848499995439,\n",
       "  0.78231626199650273,\n",
       "  0.78154789704443683,\n",
       "  0.78075676487174595,\n",
       "  0.78009741522595299,\n",
       "  0.77929909140162046,\n",
       "  0.7785558826324801,\n",
       "  0.77774055891790483,\n",
       "  0.77703783363780721,\n",
       "  0.77633562034462811,\n",
       "  0.77554714764446597,\n",
       "  0.77488615869663358,\n",
       "  0.77415137825378211,\n",
       "  0.77338623435597509,\n",
       "  0.7726207136096328,\n",
       "  0.77203322914462369,\n",
       "  0.77127256346676809,\n",
       "  0.77050177212824233,\n",
       "  0.76987453952768004,\n",
       "  0.76908388432469543,\n",
       "  0.76848873743891433,\n",
       "  0.76776562740894994,\n",
       "  0.76702697696912603,\n",
       "  0.76628202519650623,\n",
       "  0.7656475896459467,\n",
       "  0.76493156669313467,\n",
       "  0.76427477897171869,\n",
       "  0.7635645046414502,\n",
       "  0.76287701715494993,\n",
       "  0.76228576343989785,\n",
       "  0.76168582086725389,\n",
       "  0.76091886511818263,\n",
       "  0.7602876159818992,\n",
       "  0.75956936760485338,\n",
       "  0.75890676753550157,\n",
       "  0.75829606607992917,\n",
       "  0.75763893894870993,\n",
       "  0.75697487405138464,\n",
       "  0.75634756667341285,\n",
       "  0.75581888395063768,\n",
       "  0.75508570473898584,\n",
       "  0.75448780384939629,\n",
       "  0.75377464014287177,\n",
       "  0.75320218021704155,\n",
       "  0.75258320334218476,\n",
       "  0.75204798158177655,\n",
       "  0.75132430809939266,\n",
       "  0.75076961820586197,\n",
       "  0.75006701636239526,\n",
       "  0.74943488469848973,\n",
       "  0.74883806911565387,\n",
       "  0.74826174553334601,\n",
       "  0.74764842753024119,\n",
       "  0.74709867882088332,\n",
       "  0.74643953739364044,\n",
       "  0.74584953287401667,\n",
       "  0.74524466855793103,\n",
       "  0.74470987410159517,\n",
       "  0.74406816473902548,\n",
       "  0.74349094704540175,\n",
       "  0.74293285664749364,\n",
       "  0.74234914206646085,\n",
       "  0.74177752837007449,\n",
       "  0.74121560971885181,\n",
       "  0.74065848045361449,\n",
       "  0.74011857021579885,\n",
       "  0.73954157491840355,\n",
       "  0.7389225004399238,\n",
       "  0.73835077240050673,\n",
       "  0.73786062729124091,\n",
       "  0.73726236053524885,\n",
       "  0.7367177194352299,\n",
       "  0.73621325658296122,\n",
       "  0.73562598776736654,\n",
       "  0.73511773471691255,\n",
       "  0.73452680748028665,\n",
       "  0.73399743053630861,\n",
       "  0.73344766247683979,\n",
       "  0.73296176572343996,\n",
       "  0.73243101243223852,\n",
       "  0.73188274099559658,\n",
       "  0.73134461101218107,\n",
       "  0.73088365900422914,\n",
       "  0.73029518841168706,\n",
       "  0.72984966641432081,\n",
       "  0.7292710320045136,\n",
       "  0.72873420856245907,\n",
       "  0.72821183714771953,\n",
       "  0.72768403704447171,\n",
       "  0.72717792366162937,\n",
       "  0.72667377857107773,\n",
       "  0.72619018345663144,\n",
       "  0.72566414773491439,\n",
       "  0.7252098529421338,\n",
       "  0.72478849787699895,\n",
       "  0.72420504841603917,\n",
       "  0.72372469406763018,\n",
       "  0.72316330183431099,\n",
       "  0.72266538674201408,\n",
       "  0.72224045457085173,\n",
       "  0.72167535977395381,\n",
       "  0.72118689909133049,\n",
       "  0.72072054892728521,\n",
       "  0.72024227537076213,\n",
       "  0.71976160049033433,\n",
       "  0.71928416492772107,\n",
       "  0.7188241349036677,\n",
       "  0.71835016551339947,\n",
       "  0.71784827559642905,\n",
       "  0.71738438161072182,\n",
       "  0.71690946995695515,\n",
       "  0.71643606373316515,\n",
       "  0.71597386241473071,\n",
       "  0.71553716100014009,\n",
       "  0.71512491000144918,\n",
       "  0.71462503155071122,\n",
       "  0.71425015384660639,\n",
       "  0.71373049212323758,\n",
       "  0.71326402540568623,\n",
       "  0.71282030136028363,\n",
       "  0.71233324352469896,\n",
       "  0.71188263348524072,\n",
       "  0.71144044771190018,\n",
       "  0.71098611245314969,\n",
       "  0.71054544107475093,\n",
       "  0.7101286562626441,\n",
       "  0.70969043286152433,\n",
       "  0.70924256561906507,\n",
       "  0.70879429359577173,\n",
       "  0.70839287134853368,\n",
       "  0.70795226546180789,\n",
       "  0.707558454225094,\n",
       "  0.70708687802360171,\n",
       "  0.70667052045321332,\n",
       "  0.70623455138274505,\n",
       "  0.70580700077923753,\n",
       "  0.70538000520059863,\n",
       "  0.70496989992470405,\n",
       "  0.70453475702836688,\n",
       "  0.70417078305127601,\n",
       "  0.70374707912850354,\n",
       "  0.70330448653334754,\n",
       "  0.7028717609970484,\n",
       "  0.70247835755084898,\n",
       "  0.70211346359648363,\n",
       "  0.70166220162646042,\n",
       "  0.7012577621029954,\n",
       "  0.70083575928359432,\n",
       "  0.70045144762197709,\n",
       "  0.70003819378624066,\n",
       "  0.69963397023124452,\n",
       "  0.69926648900366783,\n",
       "  0.69886241612673283,\n",
       "  0.69843500035808148,\n",
       "  0.69804377486845448,\n",
       "  0.69765224607935461,\n",
       "  0.69726321790868129,\n",
       "  0.6968690413025751,\n",
       "  0.69647724855556836,\n",
       "  0.69611711904569118,\n",
       "  0.69573013590705046,\n",
       "  0.69539969733298579,\n",
       "  0.69499429774530508,\n",
       "  0.69460585457351076,\n",
       "  0.69421932234802297,\n",
       "  0.69382407103982957,\n",
       "  0.69343829619870823,\n",
       "  0.6930646693485365,\n",
       "  0.6927186696044958,\n",
       "  0.69231914122093707,\n",
       "  0.6919468053321729,\n",
       "  0.69155570130252308,\n",
       "  0.69120796772006843,\n",
       "  0.69085571955974701,\n",
       "  0.69046825598258443,\n",
       "  0.69009242069178656,\n",
       "  0.68973607907972023,\n",
       "  0.68938034118960234,\n",
       "  0.68901476531242767,\n",
       "  0.68868966724308156,\n",
       "  0.68829860108794838,\n",
       "  0.68795445050561344,\n",
       "  0.68759782497311206,\n",
       "  0.68725512311593784,\n",
       "  0.68689986683748971,\n",
       "  0.68656122971743083,\n",
       "  0.68620483117715436,\n",
       "  0.68582665771022,\n",
       "  0.68548197846089387,\n",
       "  0.68513912449797909,\n",
       "  0.68478498894574746,\n",
       "  0.6844326254979104,\n",
       "  0.6841061566895692,\n",
       "  0.68374083773400396,\n",
       "  0.68340969273641317,\n",
       "  0.68307433299413278,\n",
       "  0.68273758502280291,\n",
       "  0.68238538977929608,\n",
       "  0.68207050735465991,\n",
       "  0.68171929192211089,\n",
       "  0.6813752532392523,\n",
       "  0.68104199997372428,\n",
       "  0.68075702596426468,\n",
       "  0.68042288846006371,\n",
       "  0.68005696515251213,\n",
       "  0.67972964133913483,\n",
       "  0.67942468055980942,\n",
       "  0.67908026877687178,\n",
       "  0.67876662906350149,\n",
       "  0.67843611698189876,\n",
       "  0.6781094924025225,\n",
       "  0.67778306914914044,\n",
       "  0.67748664365249378,\n",
       "  0.67713562409007677,\n",
       "  0.67682651181097586,\n",
       "  0.67649813992307206,\n",
       "  0.67618735009921815,\n",
       "  0.67587595783176624,\n",
       "  0.67556420843033371,\n",
       "  0.67524673886585906,\n",
       "  0.67492945959950823,\n",
       "  0.67462578641607107,\n",
       "  0.67430904059619923,\n",
       "  0.67401026396343622,\n",
       "  0.67369209472161284,\n",
       "  0.67338160449502049,\n",
       "  0.67308564395498771,\n",
       "  0.67277276168271749,\n",
       "  0.67247778042570128,\n",
       "  0.67216794713531269,\n",
       "  0.6718707452999616,\n",
       "  0.67157193711437624,\n",
       "  0.6712791668975373,\n",
       "  0.67096917661435351,\n",
       "  0.67066560375307416,\n",
       "  0.67037069660253934,\n",
       "  0.67007639024417665,\n",
       "  0.6697889005126747,\n",
       "  0.66948814517266175,\n",
       "  0.66921986253720134,\n",
       "  0.6689186571603607,\n",
       "  0.66862542538068914,\n",
       "  0.66832742769392905,\n",
       "  0.66805170284985116,\n",
       "  0.6677746189216347,\n",
       "  0.66746536364944542,\n",
       "  0.66717939267720927,\n",
       "  0.6668838576665912,\n",
       "  0.66659784135693378,\n",
       "  0.66630937888903419,\n",
       "  0.66602874422391911,\n",
       "  0.66576844512459299,\n",
       "  0.66547986391202574,\n",
       "  0.66518948940273059,\n",
       "  0.66491726024901499,\n",
       "  0.66462805118398938,\n",
       "  0.66435646843204077,\n",
       "  0.66409531648588238,\n",
       "  0.66379895287142943,\n",
       "  0.66353008963704152,\n",
       "  0.66325687157662427,\n",
       "  0.66299000422311394,\n",
       "  0.66271157819240134,\n",
       "  0.66244233285201226,\n",
       "  0.66217411772344581,\n",
       "  0.66191013453605052,\n",
       "  0.66164558626566428,\n",
       "  0.66135236583959101,\n",
       "  0.66110113953194183,\n",
       "  0.66082928263421159,\n",
       "  0.66055780185136892,\n",
       "  0.66029815013069226,\n",
       "  0.66002509316996461,\n",
       "  0.65976068720806935,\n",
       "  0.65950800564091483,\n",
       "  0.65923982846533247,\n",
       "  0.6589858037027021,\n",
       "  0.65872155083880424,\n",
       "  0.65845879080720038,\n",
       "  0.65820490028500667,\n",
       "  0.65794311258920946,\n",
       "  0.65768614612912635,\n",
       "  0.65743395088978973,\n",
       "  0.65720260015264886,\n",
       "  0.65692514777714173,\n",
       "  0.65667936490447598,\n",
       "  0.6564277478556324,\n",
       "  0.65617781457263813,\n",
       "  0.65591266138721427,\n",
       "  0.65567562489935616,\n",
       "  0.6554232151509698,\n",
       "  0.65516158273302749,\n",
       "  0.65492487734264326,\n",
       "  0.65466477097715781,\n",
       "  0.65442849397351521,\n",
       "  0.65418390143703198,\n",
       "  0.65392459793397806,\n",
       "  0.65368732117763395,\n",
       "  0.65344466605498674,\n",
       "  0.65320641153846981,\n",
       "  0.65295706791384323,\n",
       "  0.65271448476767691,\n",
       "  0.65246372396389818,\n",
       "  0.65222431953258009,\n",
       "  0.65198695241937765,\n",
       "  0.65174987604226775,\n",
       "  0.65150463299755501,\n",
       "  0.65127226064574262,\n",
       "  0.65103607593401536,\n",
       "  0.65080039908039189,\n",
       "  0.65056161678502866,\n",
       "  0.65032818448030016,\n",
       "  0.6500944529102719,\n",
       "  0.64985414827382393,\n",
       "  0.6496267423501223,\n",
       "  0.64940506745517923,\n",
       "  0.6491593487825017,\n",
       "  0.64893056776439417,\n",
       "  0.64869944143806901,\n",
       "  0.64847062474054695,\n",
       "  0.64824129910240413,\n",
       "  0.64800945647732422,\n",
       "  0.64778327296027971,\n",
       "  0.64755261453916124,\n",
       "  0.64733068639139701,\n",
       "  0.64710435971693181,\n",
       "  0.64688030296638477,\n",
       "  0.6466524317151473,\n",
       "  0.64642704718289357,\n",
       "  0.64620424042911462,\n",
       "  0.64602861375360288,\n",
       "  0.64577196074536536,\n",
       "  0.6455426891670214,\n",
       "  0.64531753882290432,\n",
       "  0.64510166510777522,\n",
       "  0.64488781743015811,\n",
       "  0.64466334101380895,\n",
       "  0.64444481748542448,\n",
       "  0.64423634204184088,\n",
       "  0.64401088346402302,\n",
       "  0.64379272585047476,\n",
       "  0.6435799398703822,\n",
       "  0.6433627103578744,\n",
       "  0.64314361624471483,\n",
       "  0.6429412705281542,\n",
       "  0.64272081040243523,\n",
       "  0.64250657250016552,\n",
       "  0.64229313912682673,\n",
       "  0.64208438692338121,\n",
       "  0.64187140956304645,\n",
       "  0.64165916700833403,\n",
       "  0.64146137651939505,\n",
       "  0.64124817481709095,\n",
       "  0.6410367440996283,\n",
       "  0.64083088001289956,\n",
       "  0.64061664693902443,\n",
       "  0.64041030764193951,\n",
       "  0.64019864623838063,\n",
       "  0.63999387947176278,\n",
       "  0.63979680088404145,\n",
       "  0.63958962991651647,\n",
       "  0.63937796393485546,\n",
       "  0.6391740454643543,\n",
       "  0.63897523005891965,\n",
       "  0.63876822306058778,\n",
       "  0.6385665194247957,\n",
       "  0.63837093247078525,\n",
       "  0.63817301592218523,\n",
       "  0.6379634115109647,\n",
       "  0.63776491752646447,\n",
       "  0.63757144361109197,\n",
       "  0.63737329354983174,\n",
       "  0.63718035823482322,\n",
       "  0.63697183504636623,\n",
       "  0.63676986442260852,\n",
       "  0.63657475529211482,\n",
       "  0.63638064581248055,\n",
       "  0.63618555871660432,\n",
       "  0.63598945784789418,\n",
       "  0.6357932497853851,\n",
       "  0.6355968818321116,\n",
       "  0.63540323845990643,\n",
       "  0.63521645731907372,\n",
       "  0.63502606191461419,\n",
       "  0.63482496872525207,\n",
       "  0.63463692614328504,\n",
       "  0.63444291666042874,\n",
       "  0.63425292169762293,\n",
       "  0.63405825524521908,\n",
       "  0.63386816544703228,\n",
       "  0.63368169836092869,\n",
       "  0.63349311598957925,\n",
       "  0.63330595797405465,\n",
       "  0.63312026413507883,\n",
       "  0.63292753729048212,\n",
       "  0.63274343697576607,\n",
       "  0.63255388478970431,\n",
       "  0.63236911497572224,\n",
       "  0.63219698628029897,\n",
       "  0.63199865907786223,\n",
       "  0.63181329366646843,\n",
       "  0.63164060122082544,\n",
       "  0.63145188516346618,\n",
       "  0.63127497578490188,\n",
       "  0.63108283085213723,\n",
       "  0.63089679076034211,\n",
       "  0.63071363856551166,\n",
       "  0.6305345849174866,\n",
       "  0.63035420294678401,\n",
       "  0.63017504951901504,\n",
       "  0.62999109811765897,\n",
       "  0.62981076472195496,\n",
       "  0.62963471315853758,\n",
       "  0.62945261203380309,\n",
       "  0.62927517303903913,\n",
       "  0.62909769430820328,\n",
       "  0.62892455622970522,\n",
       "  0.62874282486860089,\n",
       "  0.6285656983851049,\n",
       "  0.62838954071815456,\n",
       "  0.62821707942805971,\n",
       "  0.62806155840202871,\n",
       "  0.62787555398207162,\n",
       "  0.62769056659412625,\n",
       "  0.62751702810379006,\n",
       "  0.62735221016075138,\n",
       "  0.62717223177862136,\n",
       "  0.6269944835638791,\n",
       "  0.62682381768419793,\n",
       "  0.62665084322402953,\n",
       "  0.6264791155372651,\n",
       "  0.62631056080795466,\n",
       "  0.62613746639315704,\n",
       "  0.62596544627636752,\n",
       "  0.62579481849125973,\n",
       "  0.62563003909222525,\n",
       "  0.62545987990572338,\n",
       "  0.62529343477358046,\n",
       "  0.62512279848093932,\n",
       "  0.62495205577753632,\n",
       "  0.62479346308357131,\n",
       "  0.6246209495817544,\n",
       "  0.62445351494595414,\n",
       "  0.6242875011445479,\n",
       "  0.62412256809227651,\n",
       "  0.62395441357442061,\n",
       "  0.6237910031707441,\n",
       "  0.62362465558191416,\n",
       "  0.62346143294025014,\n",
       "  0.62330269174247244,\n",
       "  0.62313174194180321,\n",
       "  0.62296963360802027,\n",
       "  0.62280594537069922,\n",
       "  0.62264157681380228,\n",
       "  0.62248159582536922,\n",
       "  0.62231971275534548,\n",
       "  0.62215874916344838,\n",
       "  0.62199635138311371,\n",
       "  0.62184402382047466,\n",
       "  0.62168531261729132,\n",
       "  0.62151858256353143,\n",
       "  0.62135541350320267,\n",
       "  0.62119440612303745,\n",
       "  0.62103668724065564,\n",
       "  0.6208792619013499,\n",
       "  0.62072400054334065,\n",
       "  0.62056395418728949,\n",
       "  0.6204042062395001,\n",
       "  0.62024700499564434,\n",
       "  0.6200893609823237,\n",
       "  0.61993351268947627,\n",
       "  0.61977749375918267,\n",
       "  0.61962135332743395,\n",
       "  0.61946636708129588,\n",
       "  0.619311073787285,\n",
       "  0.6191582237458626,\n",
       "  0.61900339371390978,\n",
       "  0.6188560852617413,\n",
       "  0.6186946533688813,\n",
       "  0.61854165279091311,\n",
       "  0.61839015779765838,\n",
       "  0.61823800808060037,\n",
       "  0.61808295057291252,\n",
       "  0.61793744840374876,\n",
       "  0.61777882520451532,\n",
       "  0.61762663832955655,\n",
       "  0.61747779881639064,\n",
       "  0.61732758055484771,\n",
       "  0.61717554990498591,\n",
       "  0.61703435972120513,\n",
       "  0.61688318381606444,\n",
       "  0.61672972681817306,\n",
       "  0.61657502157468069,\n",
       "  0.61643074480539395,\n",
       "  0.61627784052793322,\n",
       "  0.61613168154233811,\n",
       "  0.61598440816035693,\n",
       "  0.61583724706104692,\n",
       "  ...],\n",
       " 1000)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import sklearn tools\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load a sample of MNIST through simple sklearn commands\n",
    "print(\"[INFO] loading MNIST dataset\")\n",
    "digits = datasets.load_digits()\n",
    "data = digits.data.astype(\"float\")\n",
    "data = (data - data.min()) / (data.max() - data.min())    # normalize data\n",
    "print(\"[INFO] samples: {}, dim: {}\".format(data.shape[0], data.shape[1]))\n",
    "\n",
    "# Split the dataset into train and test\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, digits.target, test_size=0.25)\n",
    "\n",
    "# Binarize labels\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)\n",
    "\n",
    "# Train the neural network\n",
    "print(\"[INFO] training network...\")\n",
    "mnistNN = MultilayerNeuralNetwork(arch=[trainX.shape[1], 32, 16, 10], alpha=0.1)\n",
    "mnistNN.fit(trainX, trainY, epochs=1000, batch_size=1, max_loss=0.00001, displayUpdate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez treinada a rede, podemos checar seu desempenho no conjunto de testes, por meio da função `classification_report`, que fornece índices de desempenho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "(450, 64)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        50\n",
      "          1       0.96      1.00      0.98        48\n",
      "          2       1.00      0.98      0.99        42\n",
      "          3       0.98      1.00      0.99        50\n",
      "          4       0.98      0.98      0.98        41\n",
      "          5       0.98      0.98      0.98        49\n",
      "          6       0.98      0.98      0.98        41\n",
      "          7       0.98      1.00      0.99        47\n",
      "          8       0.97      0.92      0.94        37\n",
      "          9       0.98      1.00      0.99        45\n",
      "\n",
      "avg / total       0.98      0.98      0.98       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] evaluating network...\")\n",
    "# Test the model\n",
    "print(testX.shape)\n",
    "predictions = mnistNN.predict(testX)\n",
    "predictions = predictions.T.argmax(axis=1)\n",
    "print(classification_report(testY.argmax(axis=1), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais com Keras\n",
    "\n",
    "O que fizemos até então geralmente não é feito na prática. Existem bibliotecas mais robustas e otimizadas para se trabalhar com redes neurais e outros modelos. O Keras é uma biblioteca para Python muito utilizada para Machine Learning e permite o treino e avaliação de redes neurais com poucas linhas de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sklearn and keras tools\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "\n",
    "# Download the full MNIST dataset\n",
    "dataset = datasets.fetch_mldata(\"MNIST Original\")\n",
    "\n",
    "# Normalize data\n",
    "data = dataset.data.astype(\"float\")/255.0\n",
    "\n",
    "# Split data into sets\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, dataset.target, test_size=0.25)\n",
    "\n",
    "# Binarize labels\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)\n",
    "\n",
    "# Prepare the feedforward neural network with keras\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(784,), activation=\"sigmoid\"))\n",
    "model.add(Dense(128, activation=\"sigmoid\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Train the neural network\n",
    "print(\"[INFO] training...\")\n",
    "sgd = SGD(0.1)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=100, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
