{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook imports, do not care about them!\n",
    "import jdc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "\n",
    "Redes Neurais de Múltiplas Camadas (RNM), ou Multilayer Perceptron (MLP), são modelos computacionais inspirados no sistema nervoso humano, compostas por estruturas matemáticas que simulam os neurônios e suas conexões. Os usos mais comuns desses modelos consistem em tarefas de classificação e regressão, \n",
    "o que favorece a aplicação em diversas áreas de pesquisa, sendo uma delas\n",
    "a de Visão Computacional.\n",
    "\n",
    "Para compreender as RNM, é importante primeiramente conhecer dois de seus aspectos fundamentais: os **neurônios** e a **arquitetura em camadas**.\n",
    "\n",
    "## Neurônios\n",
    "Um neurônio é uma estrutura que aceita como argumento um vetor $\\mathbf{x} = \\langle x_1, x_2, \\ldots, x_n \\rangle \\in \\mathbb{R}^n$\n",
    "de entrada e **responde com um valor real** de saída. Cada neurônio está relacionado a um vetor de pesos $\\mathbf{w} = \\langle w_1, w_2, \\ldots, w_n \\rangle \\in \\mathbb{R}^n$, de forma que cada componente $x_i$ da entrada é associada ao peso $w_i$. Ao receber a entrada,\n",
    "o neurônio responde com o valor $f(\\mathbf{x}\\cdot\\mathbf{w} + b)$, onde $f$ é chamada de \n",
    "**função de ativação**; $\\mathbf{x}\\cdot\\mathbf{w} = \\sum_i x_i \\cdot w_i$, ou seja, \n",
    "o produto interno entre $\\mathbf{x}$ e $\\mathbf{w}$, também chamado de $net$ do neurônio; e $b$ é \n",
    "uma constante real chamada *bias*, cuja principal função é permitir um ajuste fino \n",
    "do valor de saída por meio de um deslocamento horizontal da função de ativação.\n",
    "\n",
    "Uma função comum de ativação é a **sigmoide**, expressa pela equação:\n",
    "$$f(x) = \\frac{1}{1 + e^{-x}}.$$\n",
    "\n",
    "Ela possui a importante propriedade de ser contínua e derivável em $(-\\infty, +\\infty)$,\n",
    "o que favorece o algoritmo de treinamento a ser em breve explicado, apesar de existirem algumas desvantagens em sua utilização.\n",
    "\n",
    "\n",
    "## Arquitetura em camadas\n",
    "\n",
    "Uma RNM é organizada em uma sequência de $k > 2$ camadas de neurônios, denotadas aqui por $L_0, L_1, \\ldots, L_{k-1}$. O tamanho de uma camada $L_i$, denotado aqui por $tam\\;L_i$, é a quantidade de neurônios que ela possui. A camada $L_0$ é dita de entrada, na qual existe um **neurônio de entrada** para cada componente do vetor de entrada da rede. Esses neurônios se diferenciam dos demais porque,\n",
    "ao invés de aceitarem todo o vetor, recebem apenas uma componente e simplesmente a **transmitem** para o interior da rede, sem \n",
    "a aplicação da função de ativação. Já a camada $L_{k-1}$ é dita de saída,\n",
    "e a sua resposta é a resposta da rede.\n",
    "\n",
    "O que viabiliza o funcionamento do modelo são as conexões entre essas camadas, e elas podem ocorrer de diversas maneiras na rede. Aqui,  \n",
    "o objeto de estudo são as **redes densas *feedforward***, em que cada neurônio da camada $L_i$ se conecta com\n",
    "todos os neurônios da camada $L_{i+1}$, $i = 0,\\ldots, k-2$, de maneira que o vetor de entrada dos neurônios da camada $L_{i+1}$ é o vetor composto pelas\n",
    "respostas dos neurônios da camada $L_{i}$. Assim, um neurônio na camada $L_{i+1}$ recebe, como entrada, um vetor\n",
    "de dimensão $tam\\;L_i$, $i=0,1, \\ldots, k-2$.\n",
    "\n",
    "Cada conexão entre neurônios pode ser vista como associada ao peso que o neurônio\n",
    "atribui ao valor por ela provido. Essa visão possibilita a utilização de uma\n",
    "notação muito útil para se trabalhar com pesos: denota-se por $w_{ij}^l$ o peso \n",
    "da conexão entre o neurônio $j$ da camada $l-1$ com o neurônio $i$ da camada $l$,\n",
    "$l = 1, \\ldots, k-1$. Mais ainda, é possível representar todas as conexões entre\n",
    "duas camadas por uma matriz de pesos $\\mathbf{W}^l = (w_{ij}^l)$ de dimensão $tam\\;L_{l} \\times tam\\;L_{l-1}$.\n",
    "Para facilitar a identificação dos *biases* de cada neurônio em uma camada, denota-se por \n",
    "$\\mathbf{b}^l = \\langle b^l_1, b^l_2, \\ldots, b^l_{tam\\;L_l} \\rangle$ os *biases* \n",
    "dos neurônios da camada $L_l$, sendo $b^l_i$ o *bias* do neurônio $i$ na camada $L_l$.\n",
    "\n",
    "Sabendo disso, já é possível iniciar a implementação em Python da RNM!\n",
    "\n",
    "### Implementando a classe `MultilayerNeuralNetwork`\n",
    "\n",
    "Como o objetivo é\n",
    "produzir um módulo reutilizável, os recursos de orientação a objetos da linguagem Python\n",
    "serão aplicados. Além disso, o pacote `numpy` será utilizado para a manipulação de matrizes. \n",
    "\n",
    "A programação tem início com a importação do `numpy` e a declaração da classe `MultilayerNeuralNetwork`, a qual encapsulará todos os dados e métodos necessários ao\n",
    "funcionamento e à utilização da rede, como a matriz de pesos, a arquitetura e as rotinas de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class MultilayerNeuralNetwork:\n",
    "    '''A Multilayer Neural Network implementation.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa é apenas uma classe, e nada tem a ver com redes neurais além do nome simplesmente. O próximo passo é criar um **construtor**, método que sempre será chamado quando alguém\n",
    "criar (ou, mais tecnicamente, instanciar) a rede neural. Nele, é importante garantir\n",
    "a criação e inicialização da arquitetura, o que implica na configuração das camadas (quantas e quantos neurônios devem possuir), das matrizes de pesos e dos vetores de *bias*.  Estas serão inicializadas com valores aleatórios, seguindo\n",
    "a distribuição normal, e, em seguida, normalizadas pela raiz quadrada da quantidade de neurônios da camada correspondente. A inicialização dos pesos é um tópico importante, para o qual existem diversas propostas, mas que será abstraído neste momento por economia de tempo.\n",
    "Além disso, um parâmetro `alpha` será acrescentado, \n",
    "cuja existência será justificada mais adiante. Portanto, o construtor receberá do\n",
    "usuário a **arquitetura da rede**, no formato $[tam\\;L_0, tam\\;L_1, \\ldots, tam\\;L_{k-1}]$, e o tal `alpha`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def __init__(self, arch = [1,2,1], alpha = 0.1):\n",
    "        '''Initialize the network.'''\n",
    "        \n",
    "        self.W = {}\n",
    "        self.B = {}\n",
    "        self.alpha = alpha\n",
    "        self.arch = arch\n",
    "        \n",
    "        # Initialize the weight matrix and biases with normalized random values\n",
    "        for i in np.arange(1,len(self.arch)):\n",
    "            # Weights\n",
    "            w = np.random.randn(self.arch[i], self.arch[i-1])\n",
    "            self.W[i] = w/np.sqrt(self.arch[i])\n",
    "            # Biases\n",
    "            b = np.random.randn(self.arch[i],1)\n",
    "            self.B[i] = b/np.sqrt(self.arch[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como será utilizada a função sigmoide para a ativação dos neurônios, é importante\n",
    "que ela e sua derivada estejam implementadas na classe:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def sigmoid(self, x):\n",
    "        '''Sigmoid function.'''\n",
    "        return 1.0/(1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_deriv(self, x):\n",
    "        '''Derivative of the sigmoid function, considering that x is the result\n",
    "        of applying the sigmoid function to the net.\n",
    "        '''\n",
    "        return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso, é possível instanciar uma rede, mesmo que não se possa fazer nada com ela ainda, apenas para checar a sua estrutura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: array([[ 0.44721243, -1.13009679],\n",
      "       [ 0.48586539, -0.16278593],\n",
      "       [-0.40227884, -0.94164842]]), 2: array([[-1.62313517, -1.68975618, -0.82730578]])}\n",
      "{1: array([[-0.05423398],\n",
      "       [-1.28780246],\n",
      "       [-0.722561  ]]), 2: array([[ 0.25346544]])}\n"
     ]
    }
   ],
   "source": [
    "# Criando uma arquitetura com 2 neurônios na camada de entrada, uma camada escondida com 3 neurônios e 1 neurônio\n",
    "# na camada de saída.\n",
    "neuralnet = MultilayerNeuralNetwork(arch = [2,3,1], alpha = 0.5)\n",
    "print(neuralnet.W)\n",
    "print(neuralnet.B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respostas das camadas\n",
    "\n",
    "Note que cada camada produz um vetor de resposta composto pelas respostas individuais\n",
    "de cada um de seus neurônios. Denote por $\\mathbf{a^l} = \\langle a^l_1, a^l_2, \\ldots, a^l_{tam\\; L_l} \\rangle$ esse vetor para a camada $L_l$. Como visto, ele deverá ser a entrada para a camada seguinte, mas os seus valores serão combinados de forma particular\n",
    "pela matriz de pesos $\\mathbf{W}^{l+1}$. A expressão\n",
    "\n",
    "$$\\mathbf{z}^l = \\mathbf{W}^{l+1}\\cdot\\mathbf{a^l} + \\mathbf{b}^l$$\n",
    "\n",
    "produz um vetor cujos componentes são os valores $net$ de cada neurônio da camada\n",
    "$L_{l+1}$. Sabendo disso, qual seria a expressão para o vetor de saída da camada $L_{l+1}$, ou seja, para $\\mathbf{a}^{l+1}$?\n",
    "Basta aplicar a função de ativação a cada componente do vetor obtido! Considerando-se a aplicação da função ponto a ponto, ou seja, $f(\\langle x_1, x_2, \\ldots, x_n \\rangle) = \\langle f(x_1), f(x_2), \\ldots, f(x_n) \\rangle$, tem-se que:\n",
    "\n",
    "$$\\mathbf{a}^{l+1} = f(\\mathbf{W}^{l+1}\\cdot\\mathbf{a^l} + \\mathbf{b}^l) = f(\\mathbf{z}^l).$$\n",
    "\n",
    "O conhecimento adquirido até este ponto é suficiente para se entender a arquitetura de uma rede neural e como computar as saídas das suas camadas de neurônios a partir \n",
    "de um vetor de entrada. Já podemos implementar uma função muito importante da nossa rede neural: dado um vetor de entrada, qual é o vetor de resposta da rede, ou seja, qual o vetor na camada de saída? Essa função é comumente chamada de `predict`, pois é utilizada para gerar a previsão (resposta) da rede a partir de uma entrada. Note como é simples e advém diretamente da definição:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def predict(self, X, outputs=False):\n",
    "        ''' Given an input column vector X, compute the output of the network. '''\n",
    "        # Set the result of the input layer\n",
    "        p = np.atleast_2d(X).T\n",
    "        # Store outputs\n",
    "        if outputs: \n",
    "            A = {}\n",
    "            A[0] = p\n",
    "        # Compute the output of each layer, following the definition\n",
    "        for layer in np.arange(1, len(self.arch)):\n",
    "            p = self.sigmoid(np.dot(self.W[layer], p) + self.B[layer])\n",
    "            if outputs: A[layer] = p\n",
    "        # Return accordingly to outputs option\n",
    "        return A if outputs else p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ocorre que apenas saber como obter a saída da rede não resolve problema algum: as matrizes de pesos aleatórios \n",
    "tornam o modelo inútil. Ele necessita se ajustar (lê-se \"aprender\")\n",
    "para solucionar os problemas com os quais é confrontado, e é disso que \n",
    "trataremos a partir de agora!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado em uma RNM\n",
    "\n",
    "O modelo de aprendizagem de redes neurais é geralmente o **supervisionado**, pois faz\n",
    "uso de exemplos representantes da verdade (*ground truth*) para fazer\n",
    "com que a rede gere as respostas desejadas após uma **etapa de treinamento**. É como se um professor\n",
    "apresentasse a entrada, a rede respondesse e ele informasse qual foi o \n",
    "erro cometido. A rede, com base nisso, modifica sua estrutura (suas matrizes\n",
    " de pesos) para que o erro, da próxima vez que o professor mostrar\n",
    " o exemplo, seja garantidamente menor. Esse processo termina quando algum critério de parada é atingido. Os mais comuns são o erro máximo e o número de ciclos ou *epochs*.\n",
    " \n",
    " O erro cometido pela rede, ao responder para dado exemplo $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$, é modelado matematicamente por uma **função de custo**\n",
    " $C_x:\\mathbb{R}^{k+1} \\to \\mathbb{R}$, onde $k$ é a quantidade de pesos. As funções que podem desempenhar esse papel devem cumprir alguns requisitos\n",
    " básicos e dependem dos valores dos pesos e dos *biases*. Uma das mais comuns é a do **erro quadrático**, de equação:\n",
    " \n",
    " $$C_x = \\frac 1 2 ||\\mathbf{y}- \\mathbf{a}^{k-1}(\\mathbf{x})||^2 = \\frac 1 2 \\sum_i (y_i - a^{k-1}_i(\\mathbf{x}))^2,$$\n",
    "  $a^{k-1}(\\mathbf{x})$ é a saída da última camada da rede para o exemplo $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$ (que nossa rede já sabe computar). Note que\n",
    " quanto maior a diferença entre a verdade e a saída da rede, maior o valor dessa\n",
    " função, o que mostra sua adequação para representar o erro.\n",
    " \n",
    " A fim de representar o erro com respeito a todos os exemplos, define-se também a função\n",
    " de custo $\\bar{C}:\\mathbb{R}^{k+1} \\to \\mathbb{R}$, dada pela média dos erros para cada exemplo, ou seja:\n",
    " \n",
    " $$\n",
    "     \\bar{C} = \\frac 1 n \\sum_{\\langle \\mathbf{x}, \\mathbf{y} \\rangle \\in \\mathcal{D}} C_x = \\frac{1}{2n} \\sum_{\\langle \\mathbf{x}, \\mathbf{y} \\rangle \\in \\mathcal{D}} ||\\mathbf{y}- \\mathbf{a}^{k-1}(\\mathbf{x})||^2.\n",
    " $$ \n",
    " \n",
    " Precisaremos, em nossa implementação, do cálculo da função de custo quadrática. Façamos, pois:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def quadratic_loss(self, X, targets):\n",
    "        ''' Compute the total quadratic loss, given a matrix of data. '''\n",
    "        targets = np.atleast_2d(targets).T\n",
    "        predictions = self.predict(X)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "        return loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Assim, o objetivo do treinamento é fazer com que o valor de $C_x$ (e $\\bar{C}$, consequentemente), para cada exemplo $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$,\n",
    " seja menor a cada ciclo. Como $C_x$ é função dos pesos e dos *biases*, nada melhor que buscar ajustá-los a fim de alcançar esse objetivo. Esse ajuste ocorre por meio de algum\n",
    " **método de otimização**, cujo objetivo é encontrar um conjunto de valores para pesos e *biases* tal que o valor de $\\bar{C}$ seja tão menor quanto se queira. Um método \n",
    " muito utilizado para tanto chama-se **gradiente descendente**, do qual a próxima seção tratará.\n",
    " \n",
    " ## Gradiente descendente\n",
    " \n",
    " Fujamos do escopo das redes neurais, e tratemos do problema geral de minimizar uma função\n",
    " de múltiplas variáveis $C:\\mathbb{R}^n \\to \\mathbb{R}$. Queremos encontrar o vetor $\\mathbf{v} = \\langle v_1, v_2, \\ldots, v_n \\rangle$ tal que $C(\\mathbf{v}) = C(v_1, v_2, \\ldots, v_n)$ seja um mínimo global de $C$.\n",
    " \n",
    " Para conseguir a intuição sobre o método, vale reduzir a dimensão de entrada de $C$ para\n",
    " duas variáveis, $v_1$ e $v_2$, e imaginar que seu gráfico assume a forma de um vale. Por exemplo:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fcfb1aba198>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcfd80afb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def quad(x,y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "x = np.linspace(-10,10,30)\n",
    "y = np.linspace(-10,10,30)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "Z = quad(X,Y)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X,Y,Z,rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "ax.set_title('Quadratic plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função acima é bem simples, devemos concordar, e, apenas observando seu gráfico, é possível encontrar o mínimo. Também, utilizando técnicas primárias de cálculo, é possível encontrar o mínimo global sem muito esforço nesse caso. Porém, quando lidamos com aprendizado, geralmente estamos interessados em minimizar funções de dezenas, centenas, milhares de variáveis! É por isso que as técnicas tradicionais de cálculo se tornam inviáveis, e outros métodos entram em cena.\n",
    "\n",
    "O gradiente descendente, sendo um deles, não busca encontrar um mínimo global, mas sim **algum mínimo para a função**. Isso pode ser um problema em alguns casos, mas não se mostra um empecilho grave para o seu uso em muitas aplicações de redes neurais. Imagine uma bola colocada em algum ponto arbitrário do vale mostrado no gráfico acima. Nossa experiência diz que ela começará a rolar até o ponto mais baixo do vale, e esse fato nos dá uma heurística para encontrar um mínimo da função: encontremos uma maneira de mover a bola nas direções\n",
    "$v_1$ e $v_2$ de forma que ela sempre esteja descendo o vale! \n",
    "\n",
    "Já que queremos mover a bola, vamos representar o deslocamento por um vetor $\\Delta v = \\langle \\Delta v_1, \\Delta v_2 \\rangle$. Assumindo a bola inicialmente na posição $v = \\langle v_1, v_2 \\rangle$, qual será a mudança no valor de $C$ causada por esse deslocamento? Ou seja, como podemos estimar o valor de $\\Delta C = C(v+\\Delta v)-C(v)$?\n",
    "Do polinômio de Taylor para o caso Multivariável, temos que\n",
    "\n",
    "$$\\Delta C \\approx \\nabla C(v) \\cdot \\Delta v = \\left \\langle \\frac{\\partial C}{\\partial v_1}, \\frac{\\partial C}{\\partial v_2} \\right \\rangle \\cdot \\langle \\Delta v_1, \\Delta v_2 \\rangle ^T = \\frac{\\partial C}{\\partial v_1}\\Delta v_1 + \\frac{\\partial C}{\\partial v_2} \\Delta v_2.$$\n",
    "\n",
    "Queremos reduzir o valor de $C$, correto? Para isso, devemos escolher um valor de $\\Delta v$ apropriado, tal que $\\Delta C < 0$. Pela equação acima, isso se torna muito fácil! Vamos tomar $\\Delta v$ como\n",
    "\n",
    "$$\\Delta v = -\\alpha \\nabla C^T,$$\n",
    "onde $\\alpha \\in \\mathbb{R}_+$. Ainda não convencido de que o $\\Delta C$ causado por essa variação $\\Delta v$ seja negativa? Vamos substituir:\n",
    "\n",
    "$$\\Delta C \\approx \\nabla C \\cdot \\Delta v = \\nabla C \\cdot (-\\alpha \\nabla C^T) = -\\alpha (\\nabla C \\cdot \\nabla C^T ) = -\\alpha ||\\nabla C||^2 \\leq 0,$$\n",
    "já que $||\\nabla C||^2 \\geq 0$ e $\\alpha > 0$. Isso nos garante que, sempre que escolhermos o $\\Delta v$ daquela forma, o valor do $C$ (o valor do erro!) sempre vai diminuir (claro, dentro dos limites da aproximação).\n",
    "\n",
    "Observe que utilizamos, aqui, uma função de duas variáveis, como forma de simplificação. Porém, note que, nas equações vetoriais acima, esse fato não se mostrou nada relevante! De fato, não importa a dimensão: a regra do gradiente descendente funcionará, e é por isso que podemos utilizá-la em nossa rede neural! Assim, para reduzirmos o valor da função de custo $C$, alteraremos os pesos $\\mathbf w$ e os biases $\\mathbf b$ (as variáveis de $C$) pela regra abaixo, considerando a aplicação em cada variável:\n",
    "\n",
    "$$w_k \\gets w_k + (-\\alpha \\frac{\\partial C}{\\partial w_k})$$\n",
    "$$b_l \\gets b_l + (-\\alpha \\frac{\\partial C}{\\partial b_l})$$\n",
    "\n",
    "No contexto das redes, o parâmetro $\\alpha$ se chama **taxa de aprendizado** (*learning rate*) e, geralmente assumindo um valor pequeno (0.05, por exemplo), almeja diminuir o risco de provocarmos um $\\Delta C$ muito elevado, o que poderia representar uma fuga de um ponto mínimo, ou um muito pequeno, o que atrasaria o processo de otimização.\n",
    "\n",
    "### O gradiente descendente estocástico\n",
    "\n",
    "Suponha que nosso conjunto de treino $\\mathcal{D}$ possua muitas instâncias. Como \n",
    "$C = \\sum_{x \\in \\mathcal{D}} C_x,$\n",
    "computar $\\nabla C$ seria muito custoso, pois envolveria computar todos os valores\n",
    "$\\nabla C_x$ e depois fazer $\\nabla C = \\frac{1}{n} \\sum_{x \\in \\mathcal{D}} \\nabla C_x$.\n",
    "Isso poderia atrasar muito o treinamento! \n",
    "\n",
    "O que comumente se faz é adaptar o método do gradiente descendente para sua versão\n",
    "estocástica. Isso significa particionar aleatoriamente o conjunto $\\mathcal D$ em uma família de conjuntos\n",
    "$\\{\\mathcal{D_i}\\}_i$ de mesma cardinalidade (tamanho) $m$. Seja $\\mathcal{D_j} = \\{x^j_1, x^j_2, \\ldots, x^j_m\\}$ algum desses conjuntos e suponha $m$ suficientemente grande.\n",
    "Consideramos que a média entre os $\\nabla C_{x^j_k}$ se aproxime da média dos $\\nabla C_{x}$, ou seja:\n",
    "\n",
    "$$\\frac{\\sum_{k = 1}^m \\nabla C_{x^j_k}}{m} \\approx \\frac{\\sum_{x \\in \\mathcal{D}} \\nabla C_x}{n} = \\nabla C.$$\n",
    "\n",
    "Fantástico! Agora não precisamos pegar sempre o conjunto inteiro e gigantesco $\\mathcal{D}$ para treinar a rede! Basta pegarmos porções menores (chamadas *mini-batches*), estimar $\\Delta C$ utilizando a aproximação acima, e atualizar os pesos e *biases*! Na prática, tomamos um conjunto com $m$  pontos arbitrários, $\\mathcal{D_j} = \\{x^j_1, x^j_2, \\ldots, x^j_m\\}$, e executamos as atualizações:\n",
    "\n",
    "$$w_k \\gets w_k + (-\\frac{\\alpha}{m} \\sum_{p = 1}^m \\frac{\\partial C_{x^j_p}}{\\partial w_k})$$\n",
    "$$b_l \\gets b_l + (-\\frac{\\alpha}{m} \\sum_{p = 1}^m \\frac{\\partial C_{x^j_p}}{\\partial b_l})$$\n",
    "\n",
    "Caso o treinamento ainda não seja o bastante, tomamos outro conjunto de tamanho $m$,\n",
    "e executamos as atualizações. Isso se repete até que o conjunto $\\mathcal{D}$\n",
    "se exaura, marcando o fim de uma *epoch* do treino. As próximas *epochs* repetirão esse processo.\n",
    "\n",
    "Que tal implementarmos nosso *stochastic gradient descent* em nossa rede neural?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def stochastic_gradient_descent(self, D, batch_size):\n",
    "        ''' Update the weights and bias through stochastic gradient \n",
    "        descent optimization. '''\n",
    "        # Get the amount of data points\n",
    "        train_size = len(D)\n",
    "        # Shuffle the data\n",
    "        random.shuffle(D)\n",
    "        # Obtain mini-batches\n",
    "        mini_batches = [D[k : k + batch_size] \n",
    "            for k in range(0, train_size, batch_size)]\n",
    "        # Update weights and biases\n",
    "        for mini_batch in mini_batches:\n",
    "            self.optimize_mini_batch(mini_batch)\n",
    "    \n",
    "    def optimize_mini_batch(self, mini_batch):\n",
    "        ''' Given a minibatch {<x1,y1>,<x2,y2>,...,<xm,ym>}, optimize the cost function. '''\n",
    "        # Get the mini-batch size\n",
    "        m = len(mini_batch)\n",
    "        # Gradient of C\n",
    "        GCw = {}\n",
    "        GCb = {}\n",
    "        # Initialize with zeroes\n",
    "        for i in np.arange(1,len(self.arch)):\n",
    "            GCw[i] = np.zeros(self.W[i].shape)\n",
    "            GCb[i] = np.zeros(self.B[i].shape)\n",
    "        # Use the SGD update rules for teaching the neural network for each batch point\n",
    "        for x, y in mini_batch:\n",
    "            # Compute gradients: see next section to understand how!\n",
    "            Gw, Gb = self.compute_gradient(x, y)\n",
    "            for i in np.arange(1,len(self.arch)): \n",
    "                GCw[i] += Gw[i]\n",
    "                GCb[i] += Gb[i]\n",
    "        # Apply rules for updating weights and biases\n",
    "        for l in np.arange(1,len(self.arch)):\n",
    "            self.W[l] = self.W[l] - (self.alpha/m)*(GCw[l])\n",
    "            self.B[l] = self.B[l] - (self.alpha/m)*(GCb[l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta seção nos deu um método muito poderoso para nossa rede neural aprender\n",
    "com a experiência! Existe, porém, um ponto ainda obscuro... **Como calcular o gradiente?** Ou melhor, como calcular de forma eficiente? O algoritmo da próxima seção tratará exatamente disso! Vamos aproveitar a estrutura em camadas da nossa rede para computar rapidamente gradientes de forma brilhante!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " ## O algoritmo Backpropagation\n",
    " \n",
    " Por questões de simplicidade, consideraremos um exemplo fixo $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$, e chamaremos $C_x$ de $C$.\n",
    " \n",
    "O ajuste dos pesos, pelo método do gradiente descendente, como visto, demanda o conhecimento sobre como variações nas matrizes\n",
    " de pesos - de camadas diferentes da última, inclusive - e dos *biases* afetam o resultado da função de custo. Em outras palavras, queremos conhecer as derivadas\n",
    " \n",
    " $$\\frac{\\partial C}{\\partial w_{ij}^l} \\qquad \\frac{\\partial C}{\\partial b_{i}^l},$$\n",
    " para todos os $w_{ij}^l$ e $b_{i}^l$, o que nos permitirá utilizar as regras de atualização do gradiente descendente e fazer a rede neural aprender.\n",
    " \n",
    " Computá-los de forma explícita seria dispendioso. Existe, porém, um caminho implícito. Imagine que o neurônio $j$ da camada $L_l$\n",
    " deva produzir o valor $net$ denotado por $z_j^l$. Porém, suponha que esse valor\n",
    " seja alterado por uma quantidade pequena $\\Delta z_j^l$, de tal forma que a resposta dele\n",
    " seja $f(z_j^l + \\Delta z_j^l)$. Como a função de custo é afetada por essa mudança?\n",
    " Do polinômio de Taylor para o Cálculo Multivariável, chega-se que \n",
    " \n",
    " $$\\Delta C = \\frac{\\partial C}{\\partial z_{j}^l} \\Delta z_{j}^l.$$\n",
    " \n",
    " Note que, se $\\frac{\\partial C}{\\partial z_{j}^l}$ \n",
    " possuir um valor alto, é necessário um $\\Delta z_{j}^l$ de sinal oposto\n",
    " para reduzir o custo. Se possuir um valor próximo de zero, \n",
    " como $\\Delta z_j^l$ é pequeno, não se pode fazer muito para reduzir o custo, e o neurônio é dito estar\n",
    " em estado próximo do ótimo. Assim, faz sentido definir o erro nesse neurônio, \n",
    " denotado por $\\delta_j^l$, como\n",
    " \n",
    " \\begin{equation}\n",
    " \\delta_j^l = \\frac{\\partial C}{\\partial z_{j}^l}.\n",
    " \\end{equation}\n",
    " \n",
    " Além disso, define-se o vetor de erros dos neurônios da camada $L_l$ por\n",
    " $\\delta^l = \\langle \\delta^l_1, \\delta^l_2, \\ldots, \\delta^l_{tam\\;L_l} \\rangle$. O algoritmo Backpropagation está fundamentado em quatro equações, das quais se pode obter\n",
    " o gradiente da função de custo, de interesse para as regras de atualização do gradiente descendente. Pode-se dizer que este é o grande viabilizador das redes neurais modernas, incluindo\n",
    "as utilizadas para *Deep Learning*. Para simplificar a notação, considere a camada $L_{k-1}$ (de saída) denotada pelo índice $L$.\n",
    "\n",
    "Resta-nos compreender cada equação.\n",
    "\n",
    "### Uma equação para o $\\delta^L$\n",
    "\n",
    "Esta equação fornece meios para se calcular o erro na camada de saída. A função de custo pode ser vista da forma \n",
    "\n",
    "$$C=C(X_1, X_2, \\ldots, X_{tam\\,L}),$$\n",
    "\n",
    "tal que $X_1 = a_1^L(z^L_1, z^L_2 \\ldots, z^L_{tam\\,L}), X_2 = a_2^L(z^L_1, z^L_2 \\ldots, z^L_{tam\\,L}), \\ldots, X_{tam\\,L} = a_{tam\\, L}^L(z^L_1, z^L_2 \\ldots, z^L_{tam\\,L})$. Pela definição anterior, sabemos que:\n",
    "\n",
    "$$\\delta_j^L = \\frac{\\partial C}{\\partial z_{j}^L}.$$\n",
    "\n",
    "Utilizando a **regra da cadeia** do Cálculo Multivariável, tem-se que\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z_{j}^L} = \\sum_k \\frac{\\partial C}{\\partial a^L_j}\\frac{\\partial a^L_k}{\\partial z^L_j}.$$\n",
    "\n",
    "Como apenas $a^L_j$ depende de $z^L_j$, todas as $\\frac{\\partial a^L_k}{\\partial z^L_j}$, com $k \\neq j$, serão anuladas, restando que:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z_{j}^L} =\\frac{\\partial C}{\\partial a^L_j}\\frac{\\partial a^L_j}{\\partial z^L_j}.$$\n",
    "\n",
    "Como $a^L_j = f(z^L_j)$, \n",
    "\n",
    "$$\\frac{\\partial a^L_j}{\\partial z^L_j} = f'(z^L_j).$$\n",
    "\n",
    "Finalmente, chega-se à equação\n",
    "\n",
    "$$\\delta^L_j = \\frac{\\partial C}{\\partial a^L_j}f'(z_j^L).$$\n",
    "\n",
    "Na forma vetorial, temos:\n",
    "\n",
    "$$\\delta^L = \\nabla C_a \\odot f'(\\mathbf{z}^L).$$\n",
    "\n",
    "### Uma equação para o $\\delta^l$ em função de $\\delta^{l+1}$\n",
    "\n",
    "Na equação passada, conseguimos uma forma de calcular os erros da última camada, entretanto, queremos também um modo\n",
    "de se calcularem os erros nas demais camadas. A estratégia é tentar escrever $\\delta^{l}_j = \\frac{\\partial C}{\\partial z^l_j}$ em termos de $\\delta^{l+1}_{j} = \\frac{\\partial C}{\\partial z^{l+1}_j}$. Como? Primeiro, é válido notar que $z_j^{l+1}$, por definição,\n",
    "depende $z_j^l$:\n",
    "\n",
    "$$z_j^{l+1} = \\mathbf{W}^{l+1}_ja^l = \\mathbf{W}^{l+1}_jf(z^l) = \\sum_i w^{l+1}_{ij}f(z^l_i).$$\n",
    "\n",
    "Além disso, $C$ depende de $z_j^{l+1}$, devido à definição recursiva de $a^n$. Com isso, basta aplicar a regra da cadeia novamente, e temos que:\n",
    "\n",
    "$$\\delta_j^l = \\sum_k \\frac{\\partial C}{\\partial z^{l+1}_k}\\frac{\\partial z^{l+1}_k}{\\partial z^l_j} = \\sum_k \\frac{\\partial z^{l+1}_k}{\\partial z^l_k}\\delta^{l+1}_k.$$\n",
    "\n",
    "Diferenciando a expressão para $z_j^{l+1}$, temos que:\n",
    "$$\\frac{\\partial z_k^{l+1}}{\\partial z_j^l} = w^{l+1}_{jk}f'(z^l_j).$$\n",
    "\n",
    "Assim, substituindo, chegamos a:\n",
    "$$\\delta_j^l = \\sum_k w^{l+1}_{jk}\\delta^{l+1}_kf'(z^l_j).$$\n",
    "\n",
    "Podemos obter a versão vetorial da seguinte forma:\n",
    "\n",
    "\\begin{align}\n",
    "\\delta_j^l = & (\\sum_k w^{l+1}_{kj}\\delta^{l+1}_k)f'(z^l_j) \\\\\n",
    "           = & ((w^{l+1}_{j})^T \\cdot \\delta^{l+1})f'(z^l_j)\n",
    "\\end{align}\n",
    "\n",
    "Essa é a forma componente a componente; para obtermos o vetor $\\delta_j$, basta\n",
    "utilizarmos o produto de Hadamard:\n",
    "\n",
    "$$\\delta^l = ((w^{l+1})^T \\cdot \\delta^{l+1}) \\odot f'(z^l)$$\n",
    "\n",
    "### Uma equação para $\\frac{\\partial C}{\\partial b^l_{j}}$\n",
    "\n",
    "Chegou a hora de obtermos as nossas derivadas de interesse! Começaremos por aquelas\n",
    "com respeito aos *biases*. Da regra da cadeira para o cálculo de muitas variáveis, temos que:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^l_{j}}=\\sum_m \\frac{\\partial C}{\\partial z_m^l} \\frac{\\partial z_m^l}{\\partial b_{j}^l}$$\n",
    "\n",
    "Note agora que $\\frac{\\partial z_m^l}{\\partial b_{j}^l}$, quando $m \\neq j$,\n",
    "é zero, pois $z_m^l$ não depende de $b_{j}^l$ nessas condições. Assim, a equação acima\n",
    "se reduz a:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^l_{j}}=\\frac{\\partial C}{\\partial z_j^l} \\frac{\\partial z_j^l}{\\partial b_{j}^l}$$\n",
    "\n",
    "Vamos lembrar a forma de $z_j^l$:\n",
    "\n",
    "$$z_j^l = \\sum_k a^{l-1}_k w^l_{kj} + b^l_j$$\n",
    "\n",
    "Daí, derivando $z_j^l$ com respeito a $b^l_j$, temos:\n",
    "\n",
    "$$\\frac{\\partial z_j^l}{\\partial b_{j}^l} = 1.$$\n",
    "\n",
    "Substituindo na equação mais acima, chegamos que, simplesmente:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^l_{j}}=\\frac{\\partial C}{\\partial z_j^l} = \\delta^l_j.$$\n",
    "\n",
    "### Uma equação para $\\frac{\\partial C}{\\partial w^l_{jk}}$\n",
    "\n",
    "Sabemos que $C$ está em função de $z^l$ e que $z^l$ está em função dos pesos,\n",
    "o que inclui $w_{jk}^l$. Por essa razão, podemos novamente aplicar a regra\n",
    "da cadeia, da seguinte forma:\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}}=\\sum_m \\frac{\\partial C}{\\partial z_m^l} \\frac{\\partial z_m^l}{\\partial w_{jk}^l}$$\n",
    "\n",
    "Note que $w^l_{jk}$ apenas influencia no cálculo de $z^l_j$, logo\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}}=\\frac{\\partial C}{\\partial z_j^l} \\frac{\\partial z_j^l}{\\partial w_{jk}^l}=\\delta^l_j\\frac{\\partial z_j^l}{\\partial w_{jk}^l}.$$\n",
    "\n",
    "Lembremo-nos de que:\n",
    "$$z_j^{l} = W^{l}_ja^{l-1} = \\sum_i w^{l}_{ij}a^{l-1}_i.$$\n",
    "\n",
    "Com isso, diferenciando, temos:\n",
    "$$\\frac{\\partial z_j^{l}}{\\partial w_{jk}^l} =  a^{l-1}_k.$$\n",
    "\n",
    "Finalmente, substituindo:\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}}= a^{l-1}_k \\delta^l_j.$$\n",
    "\n",
    "\n",
    "Temos, assim, todas as equações fundamentais que constituem o algoritmo\n",
    "Backpropagation, e elas nos dão todas as derivadas parciais de que precisamos para treinar nossa rede com o gradiente descendente! Note que o algoritmo Backpropagation é um método geral de computar derivadas parciais nesse contexto, e pode ser utilizado em outros métodos de otimização!\n",
    "\n",
    "Agora, resta implementarmos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def compute_gradient(self, x, y):\n",
    "        ''' Use backpropagation equations to compute the gradient of the cost function. '''\n",
    "        # The index of the output layer, L\n",
    "        L = len(self.arch)-1\n",
    "        # Dicts for gradients\n",
    "        Gw = {}\n",
    "        Gb = {}\n",
    "        # Compute the network output for x input sample\n",
    "        A = self.predict(x, outputs=True)\n",
    "        # Declare what will be set of delta vectors \n",
    "        D = {}\n",
    "        # Compute the error for the last neurons, using Equation 1\n",
    "        y = np.atleast_2d(y).T\n",
    "        deltaL = (A[L] - y) * self.sigmoid_deriv(A[L])\n",
    "        # Include in the D matrix\n",
    "        D[L] = deltaL\n",
    "        # Compute each delta{l} = g(delta{l+1}), using Equation 2\n",
    "        for l in np.arange(L-1, 0, -1):\n",
    "            D[l] = (self.W[l+1].T.dot(D[l+1])) * self.sigmoid_deriv(A[l])\n",
    "        for l in np.arange(L, 0, -1):\n",
    "            # Compute derivatives with respect to biases, using Equation 3\n",
    "            Gb[l] = D[l]\n",
    "            # Compute derivatives with respect to biases, using Equation 4\n",
    "            Gw[l] = D[l].dot(A[l-1].T)\n",
    "        # Return all derivatives\n",
    "        return (Gw, Gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós temos agora todos os componentes para construir uma função de treino, que chamaremos de `fit`. Ela, basicamente, executará todos os métodos criado acima, informando ao usuário dados importantes sobre o processo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "    %%add_to MultilayerNeuralNetwork\n",
    "    def fit(self, X, Y, batch_size = 20, epochs = 1000, max_loss = 0.001, displayUpdate = 100):\n",
    "        ''' Train the neural network '''\n",
    "        # Assemble D dataset\n",
    "        D = list(zip(X,Y))\n",
    "        # Store losses for returning\n",
    "        losses = [self.quadratic_loss(X,Y)]\n",
    "        # Loop over epochs\n",
    "        current_epoch = 0\n",
    "        while current_epoch < epochs and losses[current_epoch] > max_loss:\n",
    "            # Optimize using gradient descent\n",
    "            self.stochastic_gradient_descent(D, batch_size)\n",
    "            # Compute and store loss\n",
    "            losses.append(self.quadratic_loss(X,Y))\n",
    "            # Print information\n",
    "            if current_epoch % displayUpdate == 0:\n",
    "                print(\"Epoch: {}, Loss: {:.7f}\".format(current_epoch + 1, losses[current_epoch]))\n",
    "            current_epoch += 1\n",
    "        # Print about stop reason\n",
    "        if (losses[current_epoch] < max_loss):\n",
    "            print(\"Stopped by max loss criterion, loss: {:.7f}.\", losses[current_epoch])\n",
    "        elif (current_epoch == epochs):\n",
    "            print(\"Stopped by epochs criterion.\")\n",
    "        # Return training information\n",
    "        return (losses, current_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando a rede neural\n",
    "\n",
    "Agora que temos nossa rede neural implementada, podemos resolver alguns problemas bastante interessantes. Aqui serão mostrados dois exemplos: o XOR e o reconhecimento de caracteres utilizando o *dataset* MNIST.\n",
    "\n",
    "## XOR\n",
    "\n",
    "XOR é uma operação bem conhecida da lógica proposicional que possui a seguinte tabela verdade:\n",
    "\n",
    "|$x_1$|$x_2$|$y$\n",
    "|---|---|---|\n",
    "|0|0|0|\n",
    "|1|0|1|\n",
    "|0|1|1|\n",
    "|1|1|0|\n",
    "\n",
    "A partir disso, tem-se o *dataset* XOR, composto pelos vetores:\n",
    "$\\langle 0,0 \\rangle, \\langle 1,0 \\rangle, \\langle 0,1 \\rangle, \\langle 1,1 \\rangle$,\n",
    "com os respectivos *labels* $0, 1 , 1, 0$, correspondendo a cada linha da tabela acima. O que acontece se plotarmos \n",
    "$x_1$ no eixo $x$, $x_2$ no eixo $y$ e atribuirmos uma cor para cada um dos dois valores possíveis para $y$? Note:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf9d48e160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x1 = np.array([[0],[1],[0],[1]])\n",
    "x2 = np.array([[0],[0],[1],[1]])\n",
    "\n",
    "plt.scatter(x1,x2,c=['green','red','red','green'])\n",
    "plt.title('The XOR dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que há de importante nesse *dataset*? **Não é possível separá-lo utilizando uma reta**! Ou seja, não corresponde a um problema de classificação com classes linearmente separáveis. Um neurônio somente não conseguiria resolvê-lo, mas nossa rede pode! Vamos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([1, 1]), array([0])), (array([1, 0]), array([1])), (array([0, 1]), array([1])), (array([0, 0]), array([0]))]\n",
      "[[1]\n",
      " [0]]\n",
      "Epoch: 1, Loss: 0.7843540\n",
      "Epoch: 101, Loss: 0.4885617\n",
      "Epoch: 201, Loss: 0.4667297\n",
      "Epoch: 301, Loss: 0.4283654\n",
      "Epoch: 401, Loss: 0.3916145\n",
      "Epoch: 501, Loss: 0.3688027\n",
      "Epoch: 601, Loss: 0.3526663\n",
      "Epoch: 701, Loss: 0.3353332\n",
      "Epoch: 801, Loss: 0.3037728\n",
      "Epoch: 901, Loss: 0.2351350\n",
      "Epoch: 1001, Loss: 0.1450077\n",
      "Epoch: 1101, Loss: 0.0863978\n",
      "Epoch: 1201, Loss: 0.0567800\n",
      "Epoch: 1301, Loss: 0.0408259\n",
      "Epoch: 1401, Loss: 0.0313072\n",
      "Epoch: 1501, Loss: 0.0251293\n",
      "Epoch: 1601, Loss: 0.0208512\n",
      "Epoch: 1701, Loss: 0.0177406\n",
      "Epoch: 1801, Loss: 0.0153905\n",
      "Epoch: 1901, Loss: 0.0135595\n",
      "Epoch: 2001, Loss: 0.0120971\n",
      "Epoch: 2101, Loss: 0.0109050\n",
      "Epoch: 2201, Loss: 0.0099163\n",
      "Epoch: 2301, Loss: 0.0090841\n",
      "Epoch: 2401, Loss: 0.0083750\n",
      "Epoch: 2501, Loss: 0.0077640\n",
      "Epoch: 2601, Loss: 0.0072325\n",
      "Epoch: 2701, Loss: 0.0067662\n",
      "Epoch: 2801, Loss: 0.0063542\n",
      "Epoch: 2901, Loss: 0.0059875\n",
      "Epoch: 3001, Loss: 0.0056593\n",
      "Epoch: 3101, Loss: 0.0053640\n",
      "Epoch: 3201, Loss: 0.0050968\n",
      "Epoch: 3301, Loss: 0.0048541\n",
      "Epoch: 3401, Loss: 0.0046326\n",
      "Epoch: 3501, Loss: 0.0044298\n",
      "Epoch: 3601, Loss: 0.0042435\n",
      "Epoch: 3701, Loss: 0.0040716\n",
      "Epoch: 3801, Loss: 0.0039127\n",
      "Epoch: 3901, Loss: 0.0037654\n",
      "Epoch: 4001, Loss: 0.0036284\n",
      "Epoch: 4101, Loss: 0.0035007\n",
      "Epoch: 4201, Loss: 0.0033814\n",
      "Epoch: 4301, Loss: 0.0032698\n",
      "Epoch: 4401, Loss: 0.0031651\n",
      "Epoch: 4501, Loss: 0.0030666\n",
      "Epoch: 4601, Loss: 0.0029740\n",
      "Epoch: 4701, Loss: 0.0028866\n",
      "Epoch: 4801, Loss: 0.0028041\n",
      "Epoch: 4901, Loss: 0.0027260\n",
      "Epoch: 5001, Loss: 0.0026520\n",
      "Epoch: 5101, Loss: 0.0025818\n",
      "Epoch: 5201, Loss: 0.0025152\n",
      "Epoch: 5301, Loss: 0.0024518\n",
      "Epoch: 5401, Loss: 0.0023915\n",
      "Epoch: 5501, Loss: 0.0023339\n",
      "Epoch: 5601, Loss: 0.0022790\n",
      "Epoch: 5701, Loss: 0.0022266\n",
      "Epoch: 5801, Loss: 0.0021764\n",
      "Epoch: 5901, Loss: 0.0021285\n",
      "Epoch: 6001, Loss: 0.0020825\n",
      "Epoch: 6101, Loss: 0.0020384\n",
      "Epoch: 6201, Loss: 0.0019961\n",
      "Epoch: 6301, Loss: 0.0019555\n",
      "Epoch: 6401, Loss: 0.0019165\n",
      "Epoch: 6501, Loss: 0.0018789\n",
      "Epoch: 6601, Loss: 0.0018428\n",
      "Epoch: 6701, Loss: 0.0018080\n",
      "Epoch: 6801, Loss: 0.0017744\n",
      "Epoch: 6901, Loss: 0.0017421\n",
      "Epoch: 7001, Loss: 0.0017109\n",
      "Epoch: 7101, Loss: 0.0016807\n",
      "Epoch: 7201, Loss: 0.0016516\n",
      "Epoch: 7301, Loss: 0.0016234\n",
      "Epoch: 7401, Loss: 0.0015962\n",
      "Epoch: 7501, Loss: 0.0015699\n",
      "Epoch: 7601, Loss: 0.0015443\n",
      "Epoch: 7701, Loss: 0.0015196\n",
      "Epoch: 7801, Loss: 0.0014957\n",
      "Epoch: 7901, Loss: 0.0014724\n",
      "Epoch: 8001, Loss: 0.0014499\n",
      "Epoch: 8101, Loss: 0.0014280\n",
      "Epoch: 8201, Loss: 0.0014068\n",
      "Epoch: 8301, Loss: 0.0013862\n",
      "Epoch: 8401, Loss: 0.0013661\n",
      "Epoch: 8501, Loss: 0.0013467\n",
      "Epoch: 8601, Loss: 0.0013277\n",
      "Epoch: 8701, Loss: 0.0013093\n",
      "Epoch: 8801, Loss: 0.0012914\n",
      "Epoch: 8901, Loss: 0.0012739\n",
      "Epoch: 9001, Loss: 0.0012569\n",
      "Epoch: 9101, Loss: 0.0012403\n",
      "Epoch: 9201, Loss: 0.0012242\n",
      "Epoch: 9301, Loss: 0.0012084\n",
      "Epoch: 9401, Loss: 0.0011931\n",
      "Epoch: 9501, Loss: 0.0011781\n",
      "Epoch: 9601, Loss: 0.0011635\n",
      "Epoch: 9701, Loss: 0.0011493\n",
      "Epoch: 9801, Loss: 0.0011354\n",
      "Epoch: 9901, Loss: 0.0011218\n",
      "Epoch: 10001, Loss: 0.0011085\n",
      "Epoch: 10101, Loss: 0.0010955\n",
      "Epoch: 10201, Loss: 0.0010829\n",
      "Epoch: 10301, Loss: 0.0010705\n",
      "Epoch: 10401, Loss: 0.0010584\n",
      "Epoch: 10501, Loss: 0.0010465\n",
      "Epoch: 10601, Loss: 0.0010349\n",
      "Epoch: 10701, Loss: 0.0010236\n",
      "Epoch: 10801, Loss: 0.0010125\n",
      "Epoch: 10901, Loss: 0.0010016\n",
      "Epoch: 11001, Loss: 0.0009910\n",
      "Epoch: 11101, Loss: 0.0009805\n",
      "Epoch: 11201, Loss: 0.0009703\n",
      "Epoch: 11301, Loss: 0.0009603\n",
      "Epoch: 11401, Loss: 0.0009505\n",
      "Epoch: 11501, Loss: 0.0009409\n",
      "Epoch: 11601, Loss: 0.0009315\n",
      "Epoch: 11701, Loss: 0.0009223\n",
      "Epoch: 11801, Loss: 0.0009132\n",
      "Epoch: 11901, Loss: 0.0009043\n",
      "Epoch: 12001, Loss: 0.0008956\n",
      "Epoch: 12101, Loss: 0.0008871\n",
      "Epoch: 12201, Loss: 0.0008787\n",
      "Epoch: 12301, Loss: 0.0008704\n",
      "Epoch: 12401, Loss: 0.0008624\n",
      "Epoch: 12501, Loss: 0.0008544\n",
      "Epoch: 12601, Loss: 0.0008466\n",
      "Epoch: 12701, Loss: 0.0008390\n",
      "Epoch: 12801, Loss: 0.0008314\n",
      "Epoch: 12901, Loss: 0.0008240\n",
      "Epoch: 13001, Loss: 0.0008168\n",
      "Epoch: 13101, Loss: 0.0008096\n",
      "Epoch: 13201, Loss: 0.0008026\n",
      "Epoch: 13301, Loss: 0.0007957\n",
      "Epoch: 13401, Loss: 0.0007889\n",
      "Epoch: 13501, Loss: 0.0007823\n",
      "Epoch: 13601, Loss: 0.0007757\n",
      "Epoch: 13701, Loss: 0.0007692\n",
      "Epoch: 13801, Loss: 0.0007629\n",
      "Epoch: 13901, Loss: 0.0007567\n",
      "Epoch: 14001, Loss: 0.0007505\n",
      "Epoch: 14101, Loss: 0.0007445\n",
      "Epoch: 14201, Loss: 0.0007385\n",
      "Epoch: 14301, Loss: 0.0007326\n",
      "Epoch: 14401, Loss: 0.0007269\n",
      "Epoch: 14501, Loss: 0.0007212\n",
      "Epoch: 14601, Loss: 0.0007156\n",
      "Epoch: 14701, Loss: 0.0007101\n",
      "Epoch: 14801, Loss: 0.0007047\n",
      "Epoch: 14901, Loss: 0.0006993\n",
      "Epoch: 15001, Loss: 0.0006940\n",
      "Epoch: 15101, Loss: 0.0006889\n",
      "Epoch: 15201, Loss: 0.0006837\n",
      "Epoch: 15301, Loss: 0.0006787\n",
      "Epoch: 15401, Loss: 0.0006737\n",
      "Epoch: 15501, Loss: 0.0006688\n",
      "Epoch: 15601, Loss: 0.0006640\n",
      "Epoch: 15701, Loss: 0.0006593\n",
      "Epoch: 15801, Loss: 0.0006546\n",
      "Epoch: 15901, Loss: 0.0006499\n",
      "Epoch: 16001, Loss: 0.0006454\n",
      "Epoch: 16101, Loss: 0.0006409\n",
      "Epoch: 16201, Loss: 0.0006364\n",
      "Epoch: 16301, Loss: 0.0006321\n",
      "Epoch: 16401, Loss: 0.0006277\n",
      "Epoch: 16501, Loss: 0.0006235\n",
      "Epoch: 16601, Loss: 0.0006193\n",
      "Epoch: 16701, Loss: 0.0006151\n",
      "Epoch: 16801, Loss: 0.0006110\n",
      "Epoch: 16901, Loss: 0.0006070\n",
      "Epoch: 17001, Loss: 0.0006030\n",
      "Epoch: 17101, Loss: 0.0005991\n",
      "Epoch: 17201, Loss: 0.0005952\n",
      "Epoch: 17301, Loss: 0.0005913\n",
      "Epoch: 17401, Loss: 0.0005875\n",
      "Epoch: 17501, Loss: 0.0005838\n",
      "Epoch: 17601, Loss: 0.0005801\n",
      "Epoch: 17701, Loss: 0.0005765\n",
      "Epoch: 17801, Loss: 0.0005729\n",
      "Epoch: 17901, Loss: 0.0005693\n",
      "Epoch: 18001, Loss: 0.0005658\n",
      "Epoch: 18101, Loss: 0.0005623\n",
      "Epoch: 18201, Loss: 0.0005589\n",
      "Epoch: 18301, Loss: 0.0005555\n",
      "Epoch: 18401, Loss: 0.0005521\n",
      "Epoch: 18501, Loss: 0.0005488\n",
      "Epoch: 18601, Loss: 0.0005455\n",
      "Epoch: 18701, Loss: 0.0005423\n",
      "Epoch: 18801, Loss: 0.0005391\n",
      "Epoch: 18901, Loss: 0.0005360\n",
      "Epoch: 19001, Loss: 0.0005328\n",
      "Epoch: 19101, Loss: 0.0005297\n",
      "Epoch: 19201, Loss: 0.0005267\n",
      "Epoch: 19301, Loss: 0.0005237\n",
      "Epoch: 19401, Loss: 0.0005207\n",
      "Epoch: 19501, Loss: 0.0005177\n",
      "Epoch: 19601, Loss: 0.0005148\n",
      "Epoch: 19701, Loss: 0.0005119\n",
      "Epoch: 19801, Loss: 0.0005091\n",
      "Epoch: 19901, Loss: 0.0005063\n",
      "Stopped by epochs criterion.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.78435400551833634,\n",
       "  0.76224179999820751,\n",
       "  0.73687336203341591,\n",
       "  0.70885714150588286,\n",
       "  0.67927408859206695,\n",
       "  0.64989123721373343,\n",
       "  0.61912143967834721,\n",
       "  0.59205817680388084,\n",
       "  0.56846821847801776,\n",
       "  0.54822826128367952,\n",
       "  0.53225746893322201,\n",
       "  0.52167436303791981,\n",
       "  0.5141521387114566,\n",
       "  0.50786684017033168,\n",
       "  0.50555503096804033,\n",
       "  0.50394080138839281,\n",
       "  0.50173746214324511,\n",
       "  0.50059654207653503,\n",
       "  0.49866792175306779,\n",
       "  0.49809692977271025,\n",
       "  0.49729488465373023,\n",
       "  0.49736546401075465,\n",
       "  0.49689203625797407,\n",
       "  0.49676675936615189,\n",
       "  0.49684560940975747,\n",
       "  0.49694475232190377,\n",
       "  0.49687578310246244,\n",
       "  0.49664043473624464,\n",
       "  0.49647171364040088,\n",
       "  0.49625014364546072,\n",
       "  0.49610891508004384,\n",
       "  0.49610684179900705,\n",
       "  0.49595381644628683,\n",
       "  0.4958627902757296,\n",
       "  0.49578132614129522,\n",
       "  0.49570241947308546,\n",
       "  0.49562786694259447,\n",
       "  0.49564448979864895,\n",
       "  0.49549840007109247,\n",
       "  0.49537682002540062,\n",
       "  0.49527339762256806,\n",
       "  0.49516597176283572,\n",
       "  0.49510287621317661,\n",
       "  0.49501120908583168,\n",
       "  0.49495397226970728,\n",
       "  0.49489859756075222,\n",
       "  0.49473091057026153,\n",
       "  0.49461922008085057,\n",
       "  0.49453392837288501,\n",
       "  0.49444288645439638,\n",
       "  0.49436723448003439,\n",
       "  0.49426056639399463,\n",
       "  0.4942032313223309,\n",
       "  0.49415797392412708,\n",
       "  0.49402058790373993,\n",
       "  0.49389968607225299,\n",
       "  0.49385167339591363,\n",
       "  0.49366310457388429,\n",
       "  0.49361931830378758,\n",
       "  0.49348470018837498,\n",
       "  0.49335922995471893,\n",
       "  0.49338781802938425,\n",
       "  0.49316646947541198,\n",
       "  0.49302964981941988,\n",
       "  0.49294476772633466,\n",
       "  0.49283590029832791,\n",
       "  0.49272902299956806,\n",
       "  0.4926220791317647,\n",
       "  0.49250660446480254,\n",
       "  0.49239065091141937,\n",
       "  0.49226873202440902,\n",
       "  0.49216016955489617,\n",
       "  0.49205906558170698,\n",
       "  0.49204958503673674,\n",
       "  0.4919893704249424,\n",
       "  0.49168720027126955,\n",
       "  0.49156868795911757,\n",
       "  0.49144369426547585,\n",
       "  0.49131818666447635,\n",
       "  0.49120454330263252,\n",
       "  0.49108539836092341,\n",
       "  0.49105051117336174,\n",
       "  0.49088794134976438,\n",
       "  0.49074648737964499,\n",
       "  0.49076440096572888,\n",
       "  0.49057057118907882,\n",
       "  0.49039610672808709,\n",
       "  0.49021517718510416,\n",
       "  0.49008037140468041,\n",
       "  0.48994371929407332,\n",
       "  0.48980597437095863,\n",
       "  0.48968327308700305,\n",
       "  0.48963874616305703,\n",
       "  0.48946381452382559,\n",
       "  0.48937694207869964,\n",
       "  0.48928966437798349,\n",
       "  0.4890951904449245,\n",
       "  0.48912274501691516,\n",
       "  0.48912924005469743,\n",
       "  0.48918287294818719,\n",
       "  0.48856171113545444,\n",
       "  0.48836688054589511,\n",
       "  0.48835546004372449,\n",
       "  0.48801121965416888,\n",
       "  0.48797118974204134,\n",
       "  0.48796102026266702,\n",
       "  0.48770130178453547,\n",
       "  0.48750995044271078,\n",
       "  0.48724402263070221,\n",
       "  0.48709833283172937,\n",
       "  0.48697039803524483,\n",
       "  0.48684966154107573,\n",
       "  0.48682357339018312,\n",
       "  0.48652463233362919,\n",
       "  0.48630589723481016,\n",
       "  0.48617331748890419,\n",
       "  0.48599732055049394,\n",
       "  0.48588468897554427,\n",
       "  0.48585145051445577,\n",
       "  0.48590084684298745,\n",
       "  0.48548801360940097,\n",
       "  0.48525098341896356,\n",
       "  0.48500193826877452,\n",
       "  0.48481895021785382,\n",
       "  0.48462399078330898,\n",
       "  0.48446616087483746,\n",
       "  0.48432149761534538,\n",
       "  0.48412832659637745,\n",
       "  0.48398354785125108,\n",
       "  0.48374242020590219,\n",
       "  0.48356275965169077,\n",
       "  0.48338163272861462,\n",
       "  0.4832766552149092,\n",
       "  0.48307340176794228,\n",
       "  0.48285618559212728,\n",
       "  0.48262117532429999,\n",
       "  0.48250755729835326,\n",
       "  0.48224511258296249,\n",
       "  0.48204717295347604,\n",
       "  0.48184369978889496,\n",
       "  0.48168105394080546,\n",
       "  0.48162180280446576,\n",
       "  0.4813293198292829,\n",
       "  0.48129449673918245,\n",
       "  0.48098306080444253,\n",
       "  0.48070417091502454,\n",
       "  0.48063293217013764,\n",
       "  0.48020080710669966,\n",
       "  0.47999623299874744,\n",
       "  0.47987543000316635,\n",
       "  0.4798258545107944,\n",
       "  0.47949450113858944,\n",
       "  0.4791327828513281,\n",
       "  0.47892541557341667,\n",
       "  0.47868849290693971,\n",
       "  0.47845211353665212,\n",
       "  0.47822001739014131,\n",
       "  0.47801021062890875,\n",
       "  0.47778592801934117,\n",
       "  0.47754869696171121,\n",
       "  0.47739512891970948,\n",
       "  0.47720036735091342,\n",
       "  0.47692173888893019,\n",
       "  0.47665871935009518,\n",
       "  0.47635724608373964,\n",
       "  0.47612379126428633,\n",
       "  0.47595311480254943,\n",
       "  0.47563268966205841,\n",
       "  0.47539539965506461,\n",
       "  0.47516045970744891,\n",
       "  0.47488103269522219,\n",
       "  0.47472284950798177,\n",
       "  0.47451961560192946,\n",
       "  0.47411201931317182,\n",
       "  0.47384068354549502,\n",
       "  0.47359298668146599,\n",
       "  0.47335580617450368,\n",
       "  0.47306800833424673,\n",
       "  0.47286398618647829,\n",
       "  0.47255150279292496,\n",
       "  0.47230201981388875,\n",
       "  0.47203985298495393,\n",
       "  0.47175347462934714,\n",
       "  0.47146641343318829,\n",
       "  0.47120735620682758,\n",
       "  0.47096698547364585,\n",
       "  0.47070859660216141,\n",
       "  0.47047497394458365,\n",
       "  0.4701460178945811,\n",
       "  0.46987977337692904,\n",
       "  0.46950148488200849,\n",
       "  0.46926771276525503,\n",
       "  0.46917485154090682,\n",
       "  0.46880555216942998,\n",
       "  0.46874634145938476,\n",
       "  0.46822638742999945,\n",
       "  0.46791538353037171,\n",
       "  0.46775709282593997,\n",
       "  0.46735811565141594,\n",
       "  0.46712853526614923,\n",
       "  0.46672967156709577,\n",
       "  0.46649558012620435,\n",
       "  0.46643327173243637,\n",
       "  0.46637334675828912,\n",
       "  0.46604565726931158,\n",
       "  0.46514137892017998,\n",
       "  0.46476781635507047,\n",
       "  0.46458530976360712,\n",
       "  0.46409565163801403,\n",
       "  0.46366496494181803,\n",
       "  0.46343218562535426,\n",
       "  0.46295855423517873,\n",
       "  0.46267324154917805,\n",
       "  0.46247714661181299,\n",
       "  0.46195205605155709,\n",
       "  0.46162740296190197,\n",
       "  0.46139358246617834,\n",
       "  0.46117158151559778,\n",
       "  0.46072395024584439,\n",
       "  0.46034644869293445,\n",
       "  0.46004459179280249,\n",
       "  0.45958128949385674,\n",
       "  0.45935119699461741,\n",
       "  0.45887148208918072,\n",
       "  0.45848302107494904,\n",
       "  0.45813541680229153,\n",
       "  0.45788331555394679,\n",
       "  0.45736751606617931,\n",
       "  0.45703167986946747,\n",
       "  0.45668017396088478,\n",
       "  0.45626651300326104,\n",
       "  0.45588569474231011,\n",
       "  0.45553275738142163,\n",
       "  0.45519514616116363,\n",
       "  0.45479174505344933,\n",
       "  0.45440002141519492,\n",
       "  0.45403662209460116,\n",
       "  0.45367016739773958,\n",
       "  0.45325168679150102,\n",
       "  0.45294986561036449,\n",
       "  0.45251090909098596,\n",
       "  0.45222746366784028,\n",
       "  0.45195005808853472,\n",
       "  0.45133360796040256,\n",
       "  0.45102974065358653,\n",
       "  0.45055574249021452,\n",
       "  0.45023209819512366,\n",
       "  0.44994148341529855,\n",
       "  0.44969690474934348,\n",
       "  0.44915661068086843,\n",
       "  0.4486609094392936,\n",
       "  0.44819363358860265,\n",
       "  0.44779228908464902,\n",
       "  0.44751993985924865,\n",
       "  0.44691967204004318,\n",
       "  0.44651085313140365,\n",
       "  0.44612785962631057,\n",
       "  0.44570481130293749,\n",
       "  0.44532302725907191,\n",
       "  0.44495009828184762,\n",
       "  0.44448313894808983,\n",
       "  0.44409938516444114,\n",
       "  0.44366481692171472,\n",
       "  0.44332016357450377,\n",
       "  0.44294759355299351,\n",
       "  0.44252688870511631,\n",
       "  0.44203363479416735,\n",
       "  0.4416389701641012,\n",
       "  0.44120223406695036,\n",
       "  0.44078916011986879,\n",
       "  0.44038932661329055,\n",
       "  0.43995709117743059,\n",
       "  0.43959122256484529,\n",
       "  0.43910033993586889,\n",
       "  0.43870300602736728,\n",
       "  0.43828135141991464,\n",
       "  0.43791872675156213,\n",
       "  0.43749033936148207,\n",
       "  0.43706961554519103,\n",
       "  0.43663350654835253,\n",
       "  0.43624717671944835,\n",
       "  0.43583326910434278,\n",
       "  0.43542384555503189,\n",
       "  0.43506754066609044,\n",
       "  0.43463209063790353,\n",
       "  0.43415841911675451,\n",
       "  0.4336995930253405,\n",
       "  0.43336498969448378,\n",
       "  0.4328794285586276,\n",
       "  0.43256920924083053,\n",
       "  0.43215374823381025,\n",
       "  0.43197066191722827,\n",
       "  0.43138456303488704,\n",
       "  0.43095648302502887,\n",
       "  0.4304907316824701,\n",
       "  0.43030975013445449,\n",
       "  0.42979580691153613,\n",
       "  0.42945884140700813,\n",
       "  0.4289896394917897,\n",
       "  0.42853831040701112,\n",
       "  0.42836542299150138,\n",
       "  0.42751231868003836,\n",
       "  0.42693896687030136,\n",
       "  0.42653880410248041,\n",
       "  0.42612294433930714,\n",
       "  0.42567987711589367,\n",
       "  0.42530311654863207,\n",
       "  0.4249204482689839,\n",
       "  0.42450265924391756,\n",
       "  0.42408412196138062,\n",
       "  0.42378145373202825,\n",
       "  0.42337572213322827,\n",
       "  0.42289650501830678,\n",
       "  0.42247546504029876,\n",
       "  0.42191380682835916,\n",
       "  0.42151737083437124,\n",
       "  0.42110610858512121,\n",
       "  0.42071658419843938,\n",
       "  0.42035505359766506,\n",
       "  0.42000912457254941,\n",
       "  0.41966881491083163,\n",
       "  0.41916924625933849,\n",
       "  0.41875551524850158,\n",
       "  0.41841530889490736,\n",
       "  0.41810661850398129,\n",
       "  0.41765437611934025,\n",
       "  0.41720772083592306,\n",
       "  0.41674606256848679,\n",
       "  0.41632707674438763,\n",
       "  0.41593974210802376,\n",
       "  0.41553692066185383,\n",
       "  0.41515524423384115,\n",
       "  0.41474544236968425,\n",
       "  0.41438481749116718,\n",
       "  0.41396637606782866,\n",
       "  0.41359670942866522,\n",
       "  0.4132175395179209,\n",
       "  0.41281923620431227,\n",
       "  0.41245745563674108,\n",
       "  0.41208389728582367,\n",
       "  0.41176375149523392,\n",
       "  0.41146557240237175,\n",
       "  0.41098133734908354,\n",
       "  0.41055245485331332,\n",
       "  0.41018347932128646,\n",
       "  0.40986005248650137,\n",
       "  0.40946536106072273,\n",
       "  0.40907275419055067,\n",
       "  0.40872188661366293,\n",
       "  0.40832310100175206,\n",
       "  0.40793301332081983,\n",
       "  0.40756565315629084,\n",
       "  0.40724854822882067,\n",
       "  0.40686914612815284,\n",
       "  0.40652158367758739,\n",
       "  0.40632913384295144,\n",
       "  0.40589997611565026,\n",
       "  0.40573586022081098,\n",
       "  0.40559183865050952,\n",
       "  0.40518537318101161,\n",
       "  0.40469016951532105,\n",
       "  0.4042358784134012,\n",
       "  0.40388826358256358,\n",
       "  0.40341492145113345,\n",
       "  0.40316658826607815,\n",
       "  0.40283591793725132,\n",
       "  0.40237757394507095,\n",
       "  0.40206397693969487,\n",
       "  0.40161879993167959,\n",
       "  0.40133902689405526,\n",
       "  0.40103616534310249,\n",
       "  0.40073303977421881,\n",
       "  0.40042935162048099,\n",
       "  0.40006166266752829,\n",
       "  0.39976315454320993,\n",
       "  0.3994061010104536,\n",
       "  0.3990311455703589,\n",
       "  0.39873674429939537,\n",
       "  0.39838431637804766,\n",
       "  0.3981102112571257,\n",
       "  0.39781169342729328,\n",
       "  0.39746059930173816,\n",
       "  0.39708482741840029,\n",
       "  0.39677047656913222,\n",
       "  0.39643863268713181,\n",
       "  0.39613555474972462,\n",
       "  0.39582810699922344,\n",
       "  0.39552582259011748,\n",
       "  0.39521523398921649,\n",
       "  0.39494459244367869,\n",
       "  0.39465864538455919,\n",
       "  0.39441083903188334,\n",
       "  0.39396549676431375,\n",
       "  0.39368181987189299,\n",
       "  0.3933422562079274,\n",
       "  0.39305007603618797,\n",
       "  0.39277213988543558,\n",
       "  0.39247745760901315,\n",
       "  0.39218906132491027,\n",
       "  0.39190543762201863,\n",
       "  0.39161447660602444,\n",
       "  0.39133851953048632,\n",
       "  0.39101726854524749,\n",
       "  0.39075133555204439,\n",
       "  0.39046577553146883,\n",
       "  0.39024700608419449,\n",
       "  0.39005486791693944,\n",
       "  0.38967426696141905,\n",
       "  0.38932381051539844,\n",
       "  0.38903940073020593,\n",
       "  0.38875811550146927,\n",
       "  0.38850339314144616,\n",
       "  0.38822409038879474,\n",
       "  0.38798024842506779,\n",
       "  0.38768473913833323,\n",
       "  0.38743273456768446,\n",
       "  0.38719159270535736,\n",
       "  0.38694978189423224,\n",
       "  0.38660657372384954,\n",
       "  0.38636247668316204,\n",
       "  0.38617250151343674,\n",
       "  0.38589275913209187,\n",
       "  0.38561537830740511,\n",
       "  0.3852874732011356,\n",
       "  0.38502626105950721,\n",
       "  0.3847664399068107,\n",
       "  0.38453819443657511,\n",
       "  0.38428048053473701,\n",
       "  0.38402550084697307,\n",
       "  0.38377285773845904,\n",
       "  0.38359857475087744,\n",
       "  0.3833132771976504,\n",
       "  0.3830897009539676,\n",
       "  0.38293619793014261,\n",
       "  0.38270057001120417,\n",
       "  0.3823589822594517,\n",
       "  0.38206140824158125,\n",
       "  0.38193420502488501,\n",
       "  0.38178946252112156,\n",
       "  0.38155716366491399,\n",
       "  0.38148073956325279,\n",
       "  0.38115466167803774,\n",
       "  0.38082242854280901,\n",
       "  0.38068777510585222,\n",
       "  0.38038955031022148,\n",
       "  0.3801084725186033,\n",
       "  0.3797871695764406,\n",
       "  0.3795477096434885,\n",
       "  0.37929784222492452,\n",
       "  0.37909858307903249,\n",
       "  0.37889503089273008,\n",
       "  0.37875852602149634,\n",
       "  0.37854888014880611,\n",
       "  0.37822834280428075,\n",
       "  0.37795548584323579,\n",
       "  0.37770242024657374,\n",
       "  0.37749295431592822,\n",
       "  0.37727396620272685,\n",
       "  0.37705869580153512,\n",
       "  0.37688404168218154,\n",
       "  0.37666318807350202,\n",
       "  0.37648865461098946,\n",
       "  0.37631943830704667,\n",
       "  0.37615095091320311,\n",
       "  0.37583941605138738,\n",
       "  0.37559912183724575,\n",
       "  0.37541565352202472,\n",
       "  0.37514476404590041,\n",
       "  0.37492319402951946,\n",
       "  0.37471636690214244,\n",
       "  0.37452383020942809,\n",
       "  0.37432097566354755,\n",
       "  0.37411638590686142,\n",
       "  0.37392176881136663,\n",
       "  0.37370972780572942,\n",
       "  0.37351224269054523,\n",
       "  0.3733096388791155,\n",
       "  0.37310787749224633,\n",
       "  0.37290457439770358,\n",
       "  0.37272353959345134,\n",
       "  0.37254611321591313,\n",
       "  0.3723082065003086,\n",
       "  0.3721308694912685,\n",
       "  0.37199663223687102,\n",
       "  0.37192594556880509,\n",
       "  0.37182687382568236,\n",
       "  0.3717259654392065,\n",
       "  0.37150772255600056,\n",
       "  0.37134518749742357,\n",
       "  0.37112422689886432,\n",
       "  0.37081329966882881,\n",
       "  0.3704818380749082,\n",
       "  0.37026431200101734,\n",
       "  0.37010125462810778,\n",
       "  0.36993817786776745,\n",
       "  0.3698382931712223,\n",
       "  0.36974630761129668,\n",
       "  0.3695572027501392,\n",
       "  0.36921662325364385,\n",
       "  0.36904998160478519,\n",
       "  0.36880267772333658,\n",
       "  0.36860347937504578,\n",
       "  0.36840347407564078,\n",
       "  0.36827038331109513,\n",
       "  0.36809027074847095,\n",
       "  0.36790825702400498,\n",
       "  0.36774918074483809,\n",
       "  0.36765421811156906,\n",
       "  0.36745286634131469,\n",
       "  0.36720209970769563,\n",
       "  0.36697374465288174,\n",
       "  0.36679440451957868,\n",
       "  0.36662265415736983,\n",
       "  0.36646056698076906,\n",
       "  0.36629168484671243,\n",
       "  0.36615683498818091,\n",
       "  0.36594521443012618,\n",
       "  0.36576595965963582,\n",
       "  0.36560950309294338,\n",
       "  0.36550332725800322,\n",
       "  0.36541876568498916,\n",
       "  0.36525318055237216,\n",
       "  0.36508747255595131,\n",
       "  0.36482411272981885,\n",
       "  0.36467152376547401,\n",
       "  0.36444900725026991,\n",
       "  0.36432857907787564,\n",
       "  0.36424716882953267,\n",
       "  0.36408730466695921,\n",
       "  0.36382776839729414,\n",
       "  0.36362728602048044,\n",
       "  0.36346072677591562,\n",
       "  0.36337184002276612,\n",
       "  0.3631934285562613,\n",
       "  0.36301680871701314,\n",
       "  0.36284261141609914,\n",
       "  0.36269413646860388,\n",
       "  0.36245281283979358,\n",
       "  0.36225944549764189,\n",
       "  0.36210162096061949,\n",
       "  0.36193241957590871,\n",
       "  0.36177695569122187,\n",
       "  0.36161313369043613,\n",
       "  0.36147866806246165,\n",
       "  0.36133563502208255,\n",
       "  0.3611758676776301,\n",
       "  0.36098883970124118,\n",
       "  0.36081741524661542,\n",
       "  0.3606822406690034,\n",
       "  0.36050231048931808,\n",
       "  0.36037558218486809,\n",
       "  0.36018907188651977,\n",
       "  0.36002111779160195,\n",
       "  0.35988571612537584,\n",
       "  0.35979410101942844,\n",
       "  0.35958640957168786,\n",
       "  0.35939398701960185,\n",
       "  0.35928380849767722,\n",
       "  0.35912674624521357,\n",
       "  0.35907127793931531,\n",
       "  0.35883899186754536,\n",
       "  0.35869887492654084,\n",
       "  0.35849154315576204,\n",
       "  0.35840671235123167,\n",
       "  0.35817536958421631,\n",
       "  0.35808982918602178,\n",
       "  0.35802237491278238,\n",
       "  0.35784069959087361,\n",
       "  0.35758114840623412,\n",
       "  0.35744288282213582,\n",
       "  0.35737051261514213,\n",
       "  0.3572745119379731,\n",
       "  0.35708716133753143,\n",
       "  0.35690801857674348,\n",
       "  0.35681274003961799,\n",
       "  0.35675163185136394,\n",
       "  0.3564805016753238,\n",
       "  0.35620937533665636,\n",
       "  0.35605256048751299,\n",
       "  0.35585375244204054,\n",
       "  0.35578598015103019,\n",
       "  0.35571960354711907,\n",
       "  0.35556429532745848,\n",
       "  0.35531343156166351,\n",
       "  0.35509170796030332,\n",
       "  0.35491216178947482,\n",
       "  0.35480074990052279,\n",
       "  0.35466414512878186,\n",
       "  0.35458959993934319,\n",
       "  0.35436539580368065,\n",
       "  0.35422783283222148,\n",
       "  0.35401589474879497,\n",
       "  0.35386751139163558,\n",
       "  0.35368950537182431,\n",
       "  0.3535476840571915,\n",
       "  0.35343387415964339,\n",
       "  0.35328192363100858,\n",
       "  0.35312962338984077,\n",
       "  0.35297767260033619,\n",
       "  0.35288023244892558,\n",
       "  0.35266632648865071,\n",
       "  0.35251511654607381,\n",
       "  0.35233018110313474,\n",
       "  0.35218242775126224,\n",
       "  0.35209265295105485,\n",
       "  0.35204488767932834,\n",
       "  0.35201034108117873,\n",
       "  0.35183220217650163,\n",
       "  0.35154809055843578,\n",
       "  0.35137631810458891,\n",
       "  0.3512837543851971,\n",
       "  0.35110190092895271,\n",
       "  0.35094925969950691,\n",
       "  0.35070850448699364,\n",
       "  0.35061325766174822,\n",
       "  0.3504422254426558,\n",
       "  0.35037702701590523,\n",
       "  0.35013176788775402,\n",
       "  0.35003820588989087,\n",
       "  0.34980242299202186,\n",
       "  0.34965655234824899,\n",
       "  0.34956226169082188,\n",
       "  0.34952302232187715,\n",
       "  0.34920297443334591,\n",
       "  0.34897427912432361,\n",
       "  0.34889481435406594,\n",
       "  0.34874177597021039,\n",
       "  0.34850809246912046,\n",
       "  0.34842901789464203,\n",
       "  0.34838455610628782,\n",
       "  0.34828280034328379,\n",
       "  0.34799774595918104,\n",
       "  0.34775721962062217,\n",
       "  0.34758831651487276,\n",
       "  0.34751306149948236,\n",
       "  0.34725660590755658,\n",
       "  0.34717906324765058,\n",
       "  0.34713386075597785,\n",
       "  0.34706040549037459,\n",
       "  0.34682745369066709,\n",
       "  0.34652275949414857,\n",
       "  0.34627515460753755,\n",
       "  0.34621301654387254,\n",
       "  0.34593959038556826,\n",
       "  0.34577335252926966,\n",
       "  0.34569015214471993,\n",
       "  0.34544442799368646,\n",
       "  0.34535848243460548,\n",
       "  0.34509587414760357,\n",
       "  0.34489389284493266,\n",
       "  0.34473086098517836,\n",
       "  0.34457662315393167,\n",
       "  0.34441091067776808,\n",
       "  0.34421896347443198,\n",
       "  0.34405376504871915,\n",
       "  0.34395957016393086,\n",
       "  0.34387664133013257,\n",
       "  0.34379930808000692,\n",
       "  0.34357863831342184,\n",
       "  0.34328612081666149,\n",
       "  0.34322234602349044,\n",
       "  0.3431099131715562,\n",
       "  0.3427933575719212,\n",
       "  0.34253696156224872,\n",
       "  0.34244297545259184,\n",
       "  0.34218245575769407,\n",
       "  0.34196111599353796,\n",
       "  0.34176493406456598,\n",
       "  0.34159347518109145,\n",
       "  0.34141798088463388,\n",
       "  0.34131147951525176,\n",
       "  0.3412359091963344,\n",
       "  0.34096243061111192,\n",
       "  0.34076933943873,\n",
       "  0.34052322012481201,\n",
       "  0.34030998301317616,\n",
       "  0.34011649081147405,\n",
       "  0.33992980941864659,\n",
       "  0.3397434861694536,\n",
       "  0.3395664354441934,\n",
       "  0.33938925048808588,\n",
       "  0.33927061757274929,\n",
       "  0.33901437546283175,\n",
       "  0.33879221375180335,\n",
       "  0.33860252488940573,\n",
       "  0.33841206002899649,\n",
       "  0.33822103461192465,\n",
       "  0.33803721195363512,\n",
       "  0.33792841059695994,\n",
       "  0.33773149804077057,\n",
       "  0.33744166065733355,\n",
       "  0.33720932388700409,\n",
       "  0.33701721829506598,\n",
       "  0.33679486348301652,\n",
       "  0.33659453670086792,\n",
       "  0.33638282439902772,\n",
       "  0.33618495820671856,\n",
       "  0.3359919076296321,\n",
       "  0.33579099152989483,\n",
       "  0.33555831610155978,\n",
       "  0.33533322452149705,\n",
       "  0.33512933514108428,\n",
       "  0.3349304738536843,\n",
       "  0.33468970025116901,\n",
       "  0.33447400987389081,\n",
       "  0.334267858679686,\n",
       "  0.33405092266624753,\n",
       "  0.33391042624676337,\n",
       "  0.33361007136450488,\n",
       "  0.33345885598480496,\n",
       "  0.3332212699719207,\n",
       "  0.33293338409188689,\n",
       "  0.33270784709838336,\n",
       "  0.33247896994226955,\n",
       "  0.33224998339097189,\n",
       "  0.3320760279950376,\n",
       "  0.33182928904269671,\n",
       "  0.33168218396346189,\n",
       "  0.33156557769441397,\n",
       "  0.33114502539856655,\n",
       "  0.33099752480845474,\n",
       "  0.33071699231631885,\n",
       "  0.33036555067640389,\n",
       "  0.33006676370538346,\n",
       "  0.32982283213783703,\n",
       "  0.32957416566587577,\n",
       "  0.32929745755851547,\n",
       "  0.32904679981243867,\n",
       "  0.32883352269548577,\n",
       "  0.32857611521174795,\n",
       "  0.32827268097288786,\n",
       "  0.32801568980911622,\n",
       "  0.32774151502149718,\n",
       "  0.32747813453500713,\n",
       "  0.32721683932206153,\n",
       "  0.32701378966352279,\n",
       "  0.32669055053004237,\n",
       "  0.32650629393934538,\n",
       "  0.32633416368584295,\n",
       "  0.32618387988104253,\n",
       "  0.32585114847049368,\n",
       "  0.32553071813349016,\n",
       "  0.32531299224827048,\n",
       "  0.32483982702750103,\n",
       "  0.32465837460073732,\n",
       "  0.32431562072242937,\n",
       "  0.32388159803710292,\n",
       "  0.323534606411643,\n",
       "  0.32322436040457414,\n",
       "  0.32293216456690471,\n",
       "  0.32262852611086501,\n",
       "  0.32231369700838153,\n",
       "  0.32201970343712288,\n",
       "  0.3217331383373781,\n",
       "  0.32138022625825252,\n",
       "  0.3210704039415857,\n",
       "  0.32074232595334529,\n",
       "  0.32043292836674198,\n",
       "  0.32013309025018605,\n",
       "  0.31976622862161264,\n",
       "  0.31948440921983451,\n",
       "  0.31924157764242228,\n",
       "  0.31887763641121636,\n",
       "  0.31853984676158176,\n",
       "  0.31831122082761876,\n",
       "  0.31782030743500417,\n",
       "  0.31740766649914187,\n",
       "  0.31704148812788197,\n",
       "  0.31669125090781758,\n",
       "  0.31634056682239103,\n",
       "  0.31598655412153154,\n",
       "  0.31563525834507189,\n",
       "  0.31536044132928026,\n",
       "  0.31497493242259811,\n",
       "  0.3145274764824007,\n",
       "  0.31415279159550785,\n",
       "  0.31387059900402009,\n",
       "  0.31339881182871238,\n",
       "  0.31298542067524943,\n",
       "  0.31265552002802416,\n",
       "  0.3123737601983187,\n",
       "  0.31184736698908311,\n",
       "  0.31144578844157877,\n",
       "  0.31105108402920717,\n",
       "  0.31060597016122687,\n",
       "  0.31020458834897963,\n",
       "  0.30978085064728283,\n",
       "  0.30936465244201394,\n",
       "  0.30894945345423652,\n",
       "  0.30854107737414849,\n",
       "  0.30814885367105216,\n",
       "  0.30767395177607282,\n",
       "  0.30724583871533395,\n",
       "  0.30689112783197264,\n",
       "  0.30637141639629045,\n",
       "  0.30598094434469569,\n",
       "  0.30563689074172257,\n",
       "  0.30533517638365454,\n",
       "  0.30503302955334488,\n",
       "  0.30427671213172236,\n",
       "  0.30377275307298585,\n",
       "  0.30327751212584536,\n",
       "  0.30277754577985772,\n",
       "  0.30244027930654382,\n",
       "  0.30178432327957133,\n",
       "  0.30126164582695647,\n",
       "  0.30077370788821023,\n",
       "  0.30036605505228747,\n",
       "  0.29979611309550241,\n",
       "  0.29939584062415192,\n",
       "  0.29888630077535722,\n",
       "  0.29852975870688236,\n",
       "  0.29814157191890117,\n",
       "  0.29773730703471468,\n",
       "  0.2970592870646635,\n",
       "  0.29628778408297785,\n",
       "  0.29573806682335352,\n",
       "  0.29520548370558619,\n",
       "  0.2947880659346897,\n",
       "  0.2943755640126352,\n",
       "  0.29397496550659441,\n",
       "  0.29352748436047438,\n",
       "  0.29305934451678622,\n",
       "  0.29231952325809285,\n",
       "  0.29191275362019486,\n",
       "  0.29090942366616068,\n",
       "  0.29042262731752921,\n",
       "  0.28999795222831398,\n",
       "  0.28924140226656953,\n",
       "  0.28873357023004065,\n",
       "  0.28802139049767572,\n",
       "  0.28758603153626472,\n",
       "  0.28715727196407453,\n",
       "  0.28667547463720128,\n",
       "  0.28613277889930594,\n",
       "  0.28503976670809333,\n",
       "  0.28458298685852818,\n",
       "  0.28379826474583131,\n",
       "  0.28306948550020589,\n",
       "  0.28237323601712405,\n",
       "  0.28160286125653622,\n",
       "  0.28095786568725423,\n",
       "  0.28039396902557912,\n",
       "  0.27967265748280074,\n",
       "  0.2790885098599083,\n",
       "  0.27855480147229972,\n",
       "  0.27776476683941675,\n",
       "  0.27699049984004731,\n",
       "  0.27633900002986045,\n",
       "  0.27564089706202727,\n",
       "  0.27499271153990817,\n",
       "  0.27425159060497428,\n",
       "  0.27355041647675515,\n",
       "  0.27294309201088063,\n",
       "  0.27213617019514069,\n",
       "  0.27148366485929221,\n",
       "  0.27093200082308039,\n",
       "  0.27002302593194488,\n",
       "  0.26928735904801387,\n",
       "  0.26866798315147961,\n",
       "  0.26784908229075288,\n",
       "  0.26716481148271021,\n",
       "  0.26633388276687187,\n",
       "  0.26571271315972611,\n",
       "  0.26516528035915388,\n",
       "  0.26411552991604448,\n",
       "  0.26334393055182348,\n",
       "  0.2625684932210065,\n",
       "  0.26193216486681387,\n",
       "  0.26100321017540945,\n",
       "  0.26022637107039565,\n",
       "  0.25957647098319614,\n",
       "  0.25896149529713552,\n",
       "  0.25833754965780059,\n",
       "  0.25710085667275168,\n",
       "  0.25629106049205042,\n",
       "  0.25546529320310085,\n",
       "  0.25468655511655375,\n",
       "  0.25397176780922626,\n",
       "  0.25331855629507322,\n",
       "  0.25224849377915926,\n",
       "  0.25141380596829582,\n",
       "  0.25057974127640814,\n",
       "  0.24972825325012282,\n",
       "  0.24895812497690845,\n",
       "  0.24827710251780669,\n",
       "  0.24766129154674937,\n",
       "  0.24665868944227742,\n",
       "  0.24603529229610577,\n",
       "  0.24520088865569828,\n",
       "  0.24408667533010073,\n",
       "  0.24312918149552143,\n",
       "  0.24217522538981062,\n",
       "  0.2412500702939698,\n",
       "  0.24059703037877048,\n",
       "  0.23994022995947911,\n",
       "  0.2392480538846295,\n",
       "  0.23773505317056429,\n",
       "  0.23676070285205003,\n",
       "  0.23587395388571414,\n",
       "  0.23513503643144162,\n",
       "  0.2340821945955108,\n",
       "  0.23326038898883997,\n",
       "  0.23229476878649835,\n",
       "  0.23150274650550856,\n",
       "  0.23052264505784684,\n",
       "  0.22960574597704025,\n",
       "  0.22869546939416896,\n",
       "  0.22796778558171779,\n",
       "  0.22691049038472677,\n",
       "  0.2259438761057021,\n",
       "  0.22504264895254739,\n",
       "  0.22409227013539568,\n",
       "  0.22316888039776508,\n",
       "  0.2222609282930593,\n",
       "  0.22131487401846461,\n",
       "  0.2204121908858846,\n",
       "  0.21962892736122169,\n",
       "  0.21886491707934913,\n",
       "  0.21771950015469785,\n",
       "  0.21674961278452295,\n",
       "  0.21591923192988829,\n",
       "  0.21477829507062751,\n",
       "  0.21396060611589005,\n",
       "  0.21291591226337137,\n",
       "  0.21209239161639018,\n",
       "  0.21137907969023242,\n",
       "  0.21027559237151749,\n",
       "  0.20914456174994464,\n",
       "  0.20819433046104524,\n",
       "  0.20724544170579118,\n",
       "  0.20629862717795042,\n",
       "  0.20550393418421539,\n",
       "  0.2047703999974386,\n",
       "  0.20402340868336941,\n",
       "  0.20256084814176156,\n",
       "  0.20171567062022772,\n",
       "  0.20098370130750007,\n",
       "  0.1997141729409895,\n",
       "  0.19876060616120877,\n",
       "  0.19791070187918733,\n",
       "  0.19712603608324614,\n",
       "  0.19640926420207028,\n",
       "  0.19565648510559436,\n",
       "  0.19484225517990225,\n",
       "  0.19350063665117065,\n",
       "  0.19215352242982908,\n",
       "  0.19129473804963942,\n",
       "  0.19029229811568166,\n",
       "  0.18939168438421466,\n",
       "  0.18855942618090316,\n",
       "  0.18746610545655862,\n",
       "  0.18666300277381079,\n",
       "  0.18562009556851641,\n",
       "  0.18477460438692031,\n",
       "  0.18375555789351511,\n",
       "  0.18295318233271818,\n",
       "  0.18210933677685737,\n",
       "  0.18139483362591635,\n",
       "  0.1800800680457989,\n",
       "  0.17931923532331692,\n",
       "  0.17860242337646581,\n",
       "  0.17736014453552421,\n",
       "  0.17645188588030372,\n",
       "  0.17552535326051766,\n",
       "  0.17472463649437422,\n",
       "  0.17375810641481904,\n",
       "  0.17304338309019604,\n",
       "  0.17205341887959022,\n",
       "  0.17101282574495091,\n",
       "  0.17011867162741892,\n",
       "  0.16934148691744552,\n",
       "  0.16857717825979046,\n",
       "  0.16756653513771697,\n",
       "  0.16659218271625345,\n",
       "  0.16586394639492641,\n",
       "  0.16507199274851256,\n",
       "  0.16396101370646504,\n",
       "  0.16316771196988,\n",
       "  0.16247719196765956,\n",
       "  0.16164153222260527,\n",
       "  0.16094387983486233,\n",
       "  0.15970816543773708,\n",
       "  0.15884713086276994,\n",
       "  0.15797586916830034,\n",
       "  0.15716937302137049,\n",
       "  0.15631844522251037,\n",
       "  0.15544351509401519,\n",
       "  0.15461281586824177,\n",
       "  0.15380650694568038,\n",
       "  0.15295997261097827,\n",
       "  0.15213911873083272,\n",
       "  0.15144495524413543,\n",
       "  0.15049518428232045,\n",
       "  0.14974706168207275,\n",
       "  0.14889577146565824,\n",
       "  0.14816125941646696,\n",
       "  0.14752193212000861,\n",
       "  0.14658459127445667,\n",
       "  0.14586430774724346,\n",
       "  ...],\n",
       " 20000)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos compor nosso dataset X justapondo horizontalmente x1 e x2 declarados anteriormente\n",
    "X = np.array([[1,1],[1,0],[0,1],[0,0]])\n",
    "# Temos também de declarar os targets\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "l = list(zip(X,y))\n",
    "\n",
    "print(l)\n",
    "\n",
    "a = np.atleast_2d(l[1][0]).T\n",
    "print(a)\n",
    "\n",
    "# Agora, instanciamos e treinamos nossa rede neural!\n",
    "xorNetwork = MultilayerNeuralNetwork(arch=[2,2,1], alpha=0.5)\n",
    "xorNetwork.fit(X, y, batch_size=1, epochs = 20000, displayUpdate=100, max_loss=0.0000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "O MNIST é um *dataset* massivamente utilizado pelos estudiosos de Machine Learning composto\n",
    "por imagens de dígitos de 0 a 9. Ele possui 60000 exemplos de treino e 10000 de testes. \n",
    "Os vetores de características são 784-dimensionais ($28 \\times 28$ pixels por imagem),\n",
    "com componentes assumindo valores em $[0,255]$. O propósito é corretamente classificar\n",
    "os dígitos desse *dataset*!\n",
    "\n",
    "A biblioteca `sklearn` oferece o uma amostra do MNIST por meio de comandos simples. \n",
    "Vamos utilizá-los para obter esse *dataset* e, com ele,\n",
    "treinar uma rede neural especializada em classificar\n",
    "seus dígitos de 0 a 9!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST dataset\n",
      "[INFO] samples: 1797, dim: 64\n",
      "[INFO] training network...\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "27\n",
      "Epoch: 1, Loss: 1751.6072410\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "27\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "27\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "27\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "27\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "27\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "27\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "27\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-4c751c4560f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] training network...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmnistNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultilayerNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmnistNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplayUpdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, batch_size, epochs, max_loss, displayUpdate)\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mstochastic_gradient_descent\u001b[0;34m(self, D, batch_size)\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36moptimize_mini_batch\u001b[0;34m(self, mini_batch)\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mcompute_gradient\u001b[0;34m(self, x, y)\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, outputs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import sklearn tools\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load a sample of MNIST through simple sklearn commands\n",
    "print(\"[INFO] loading MNIST dataset\")\n",
    "digits = datasets.load_digits()\n",
    "data = digits.data.astype(\"float\")\n",
    "data = (data - data.min()) / (data.max() - data.min())    # normalize data\n",
    "print(\"[INFO] samples: {}, dim: {}\".format(data.shape[0], data.shape[1]))\n",
    "\n",
    "# Split the dataset into train and test\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, digits.target, test_size=0.25)\n",
    "\n",
    "# Binarize labels\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)\n",
    "\n",
    "# Train the neural network\n",
    "print(\"[INFO] training network...\")\n",
    "mnistNN = MultilayerNeuralNetwork(arch=[trainX.shape[1], 32, 16, 10], alpha=0.1)\n",
    "mnistNN.fit(trainX, trainY, epochs=1000, batch_size=30, max_loss=0.00001, displayUpdate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez treinada a rede, podemos checar seu desempenho no conjunto de testes, por meio da função `classification_report`, que fornece índices de desempenho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "(450, 64)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        50\n",
      "          1       0.96      1.00      0.98        48\n",
      "          2       1.00      0.98      0.99        42\n",
      "          3       0.98      1.00      0.99        50\n",
      "          4       0.98      0.98      0.98        41\n",
      "          5       0.98      0.98      0.98        49\n",
      "          6       0.98      0.98      0.98        41\n",
      "          7       0.98      1.00      0.99        47\n",
      "          8       0.97      0.92      0.94        37\n",
      "          9       0.98      1.00      0.99        45\n",
      "\n",
      "avg / total       0.98      0.98      0.98       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] evaluating network...\")\n",
    "# Test the model\n",
    "print(testX.shape)\n",
    "predictions = mnistNN.predict(testX)\n",
    "predictions = predictions.T.argmax(axis=1)\n",
    "print(classification_report(testY.argmax(axis=1), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais com Keras\n",
    "\n",
    "O que fizemos até então geralmente não é feito na prática. Existem bibliotecas mais robustas e otimizadas para se trabalhar com redes neurais e outros modelos. O Keras é uma biblioteca para Python muito utilizada para Machine Learning e permite o treino e avaliação de redes neurais com poucas linhas de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training...\n",
      "Train on 52500 samples, validate on 17500 samples\n",
      "Epoch 1/100\n",
      "52500/52500 [==============================] - 6s 117us/step - loss: 1.7456 - acc: 0.5229 - val_loss: 1.0489 - val_acc: 0.7161\n",
      "Epoch 2/100\n",
      "52500/52500 [==============================] - 4s 83us/step - loss: 0.7404 - acc: 0.8094 - val_loss: 0.5832 - val_acc: 0.8473\n",
      "Epoch 3/100\n",
      "52500/52500 [==============================] - 5s 87us/step - loss: 0.4951 - acc: 0.8656 - val_loss: 0.4578 - val_acc: 0.8633\n",
      "Epoch 4/100\n",
      "52500/52500 [==============================] - 5s 89us/step - loss: 0.4038 - acc: 0.8886 - val_loss: 0.3985 - val_acc: 0.8883\n",
      "Epoch 5/100\n",
      "52500/52500 [==============================] - 4s 81us/step - loss: 0.3598 - acc: 0.8988 - val_loss: 0.3597 - val_acc: 0.8967\n",
      "Epoch 6/100\n",
      "52500/52500 [==============================] - 4s 80us/step - loss: 0.3340 - acc: 0.9052 - val_loss: 0.3424 - val_acc: 0.9020\n",
      "Epoch 7/100\n",
      "52500/52500 [==============================] - 4s 80us/step - loss: 0.3163 - acc: 0.9091 - val_loss: 0.3225 - val_acc: 0.9038\n",
      "Epoch 8/100\n",
      "52500/52500 [==============================] - 4s 81us/step - loss: 0.3027 - acc: 0.9134 - val_loss: 0.3211 - val_acc: 0.9058\n",
      "Epoch 9/100\n",
      "52500/52500 [==============================] - 5s 93us/step - loss: 0.2908 - acc: 0.9164 - val_loss: 0.3024 - val_acc: 0.9098\n",
      "Epoch 10/100\n",
      "52500/52500 [==============================] - 5s 88us/step - loss: 0.2813 - acc: 0.9189 - val_loss: 0.3180 - val_acc: 0.9076\n",
      "Epoch 11/100\n",
      "52500/52500 [==============================] - 5s 88us/step - loss: 0.2725 - acc: 0.9211 - val_loss: 0.2886 - val_acc: 0.9136\n",
      "Epoch 12/100\n",
      "52500/52500 [==============================] - 5s 89us/step - loss: 0.2645 - acc: 0.9230 - val_loss: 0.2866 - val_acc: 0.9159\n",
      "Epoch 13/100\n",
      "51712/52500 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.9252"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-2c57a8634af0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0msgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import sklearn and keras tools\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "\n",
    "# Download the full MNIST dataset\n",
    "dataset = datasets.fetch_mldata(\"MNIST Original\")\n",
    "\n",
    "# Normalize data\n",
    "data = dataset.data.astype(\"float\")/255.0\n",
    "\n",
    "# Split data into sets\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, dataset.target, test_size=0.25)\n",
    "\n",
    "# Binarize labels\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)\n",
    "\n",
    "# Prepare the feedforward neural network with keras\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(784,), activation=\"sigmoid\"))\n",
    "model.add(Dense(128, activation=\"sigmoid\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Train the neural network\n",
    "print(\"[INFO] training...\")\n",
    "sgd = SGD(0.1)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=100, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
